{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4a4de81",
   "metadata": {},
   "source": [
    "Copyright (c) 2023 Graphcore Ltd. All rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37526ca7",
   "metadata": {},
   "source": [
    "# PyTorch Geometric on IPUs at a glance\n",
    "\n",
    "IPUs can significantly accelerate both training and inference on GNNs. To use an existing PyTorch Geometric (PyG) model on IPUs some minor changes are needed. Some of these changes are required so that the model can run on IPUs, and other changes are optional for improving performance.\n",
    "\n",
    "In this tutorial you will learn how to:\n",
    "\n",
    "- Run an existing PyTorch Geometric model on the IPU,\n",
    "- Accelerate your dataloader performance using the PopTorch (IPU-specific set of extensions for PyTorch) dataloader, while satisfying the static graph requirements of the IPU by using fixed sized inputs,\n",
    "- Make the necessary changes in some PyTorch Geometric layers and operations to meet the static graph requirements of the IPU.\n",
    "\n",
    "While this tutorial will cover enough of the basics of GNNs, PyTorch Geometric and PopTorch\n",
    "for you to start developing and porting your GNN applications to the IPU;\n",
    "the following resources can be used to complement your understanding of:\n",
    "\n",
    "- PopTorch : [Introduction to PopTorch - running a simple model](https://github.com/graphcore/tutorials/tree/master/tutorials/pytorch/basics);\n",
    "- GNNs : [A Gentle Introduction to Graph Neural Networks](https://distill.pub/2021/gnn-intro/)\n",
    "- PyTorch Geometric (PyG): [Official notebooks examples and tutorials](https://pytorch-geometric.readthedocs.io/en/latest/notes/colabs.html)\n",
    "\n",
    "## Running on Paperspace\n",
    "\n",
    "The Paperspace environment lets you run this notebook with no set up. To improve your experience we preload datasets and pre-install packages, this can take a few minutes, if you experience errors immediately after starting a session please try restarting the kernel before contacting support. If a problem persists or you want to give us feedback on the content of this notebook, please reach out to through our community of developers using our [slack channel](https://www.graphcore.ai/join-community) or raise a [GitHub issue](https://github.com/graphcore/examples).\n",
    "\n",
    "Requirements:\n",
    "\n",
    "* Python packages installed with `pip install -r ../requirements.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87650b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6180ea",
   "metadata": {},
   "source": [
    "And for compatibility with the Paperspace environment variables we will do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd81b3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dataset_directory = os.getenv(\"DATASET_DIR\", \"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a07c5fc",
   "metadata": {},
   "source": [
    "Now we are ready to start!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b71cb2d",
   "metadata": {},
   "source": [
    "## Porting to the IPU basics\n",
    "\n",
    "To run your model using PyTorch Geometric on the IPU, the model will need to target PopTorch. PopTorch is a set of IPU-specific extensions which allows you to run PyTorch native models on the IPU.\n",
    "It is designed to require as few changes as possible from native PyTorch, but there are some differences. This means a few changes are required:\n",
    "\n",
    "* Move the loss function inside the `forward` method of your model.\n",
    "* Wrap the model in `poptorch.trainingModel` or `poptorch.inferenceModel`.\n",
    "* Remove the manual call to the backward pass and optimizer steps - both are handled by PopTorch automatically.\n",
    "\n",
    "Additional useful changes to make:\n",
    "* Use a PopTorch optimizer, specifically designed for the IPU.\n",
    "\n",
    "Let's see what these changes mean by taking a look at a small example. \n",
    "First let's load a dataset: the Cora dataset is a citation network where a node represents a document and an edge exists if there is a citation between the two documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4799756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "dataset = Planetoid(dataset_directory, \"Cora\", transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06125f6",
   "metadata": {},
   "source": [
    "Let's look at a typical training example. We will use a GCN layer, one of the most commonly used GNN operators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79df06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1234)\n",
    "        self.conv = GCNConv(in_channels, out_channels, add_self_loops=False)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv(x, edge_index, edge_weight).relu()\n",
    "        return x\n",
    "\n",
    "\n",
    "model = GCN(dataset.num_features, dataset.num_classes)\n",
    "model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Training on CPU.\")\n",
    "\n",
    "for epoch in range(1, 6):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index, data.edge_attr)\n",
    "    loss = F.cross_entropy(out, data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch}, Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddd4ea1",
   "metadata": {},
   "source": [
    "Now let's make the changes mentioned above to make this example run on the IPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1c1c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import poptorch\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1234)\n",
    "        self.conv = GCNConv(in_channels, out_channels, add_self_loops=False)\n",
    "\n",
    "    def forward(self, x, edge_index, y, edge_weight=None):\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv(x, edge_index, edge_weight).relu()\n",
    "\n",
    "        if self.training:\n",
    "            loss = F.cross_entropy(x, y)\n",
    "            return x, loss\n",
    "        return x\n",
    "\n",
    "\n",
    "model = GCN(dataset.num_features, dataset.num_classes)\n",
    "model.train()\n",
    "optimizer = poptorch.optim.Adam(model.parameters(), lr=0.001)\n",
    "poptorch_model = poptorch.trainingModel(model, optimizer=optimizer)\n",
    "\n",
    "print(\"Training on IPU.\")\n",
    "for epoch in range(1, 6):\n",
    "    output, loss = poptorch_model(\n",
    "        data.x, data.edge_index, data.y, edge_weight=data.edge_attr\n",
    "    )\n",
    "    print(f\"Epoch: {epoch}, Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a786ba44",
   "metadata": {},
   "source": [
    "You have now successfully compiled and run the model on IPU!\n",
    "\n",
    "We have seen the changes required to get training your PyTorch Geometric model on IPU. For more comprehensive information please refer to the [PopTorch documentation](https://docs.graphcore.ai/projects/poptorch-user-guide/en/latest/index.html).\n",
    "\n",
    "Now let's take a look at some of the other changes that are useful to get more performance out of the IPU.\n",
    "\n",
    "## High performance dataloader and fixed size inputs\n",
    "\n",
    "PopTorch provides its own dataloader that behaves very similarly to the PyTorch dataloader you may be familiar with, `torch.utils.data.DataLoader`. The [PopTorch dataloader](https://docs.graphcore.ai/projects/poptorch-user-guide/en/latest/pytorch_to_poptorch.html#preparing-your-data) provides the following features:\n",
    "\n",
    "* It takes a `poptorch.Options` instance to use IPU-specific features for example [deviceIterations](https://docs.graphcore.ai/projects/poptorch-user-guide/en/latest/batching.html?highlight=deviceIterations#poptorch-options-deviceiterations);\n",
    "* It automatically computes the number of elements consumed by a single step;\n",
    "* It enables asynchronous data loading.\n",
    "\n",
    "[PopTorch Geometric](https://docs.graphcore.ai/projects/poptorch-geometric-user-guide/), the IPU-specific PyTorch Geometric library, provides a wrapper for the PopTorch dataloader, making it easy to get performant PyTorch Geometric models running on the IPU. Let's see how to get started with it.\n",
    "\n",
    "First we load a dataset. In this case we are loading the MUTAG dataset, which is a collection of many small graphs>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2835a22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "\n",
    "dataset = TUDataset(dataset_directory, name=\"MUTAG\")\n",
    "data = dataset[0]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a997534d",
   "metadata": {},
   "source": [
    "To create a dataloader in PyTorch Geometric we do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6b844e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d53f4a8",
   "metadata": {},
   "source": [
    "The IPU needs fixed sized inputs, which means that prior knowledge of the shape of the input tensors is required. \n",
    "There are different ways to achieve fixed sized inputs, and the method used will depend on the type of input graph dataset we're working with:\n",
    "* if we're dealing with a dataset of many small graphs, we can batch the input graphs via the dataloader and pad the resulting batch: you can check out our tutorial on [Small Graph Batching with Padding](../3_small_graph_batching_with_padding/3_small_graph_batching_with_padding.ipynb) for a detailed walkthrough. This approach may result in a very large amount of padding in specific use cases: we present a more efficient batching strategy called packing in a dedicated tutorial on [Small Graph Batching with Packing](../4_small_graph_batching_with_packing/4_small_graph_batching_with_packing.ipynb).\n",
    "* if we're dealing with a dataset of a single large graph, we can sample from it and then pad the samples to obtain static shapes. You can refer to the [Cluster CGN example](../../../../gnn/cluster_gcn/pytorch_geometric/node_classification_with_cluster_gcn.ipynb) for a large graph use case.\n",
    "\n",
    "We demonstrate the usage of `FixedSizeDataLoader`, a class to create a fixed batch sampler with `batch_size` graphs in each batch. \n",
    "The `num_nodes` and `num_edges` optional arguments allow you to set the total number of nodes and edges in a batch, respectively, to make the batch fixed size and therefore suitable for the IPU.\n",
    "We can inspect the dataset using the `Summary` helper functionality to collect some statistics on the number of nodes and edges in the dataset: this will help us decide which `num_nodes` and `num_edges` to use in the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383f70d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from poptorch_geometric import FixedSizeDataLoader\n",
    "from torch_geometric.data.summary import Summary\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "dataset_summary = Summary.from_dataset(dataset)\n",
    "dataset_summary\n",
    "max_number_of_nodes = int(dataset_summary.num_nodes.max)\n",
    "max_number_of_edges = int(dataset_summary.num_edges.max)\n",
    "print(f\"Max number of nodes in the dataset is: {max_number_of_nodes}\")\n",
    "print(f\"Max number of edges in the dataset is: {max_number_of_edges}\")\n",
    "\n",
    "ipu_dataloader = FixedSizeDataLoader(\n",
    "    dataset, num_nodes=300, num_edges=600, batch_size=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6a84a2",
   "metadata": {},
   "source": [
    "If we look at the what the dataloader has produced, you will see that `ipu_dataloader` produces `batch_size` mini-batches with the specified number of nodes and edge to work with fixed size inputs. \n",
    "The other dimensions match the PyTorch Geometric dataloader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638d6b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{next(iter(dataloader)) = }\")\n",
    "print(f\"{next(iter(ipu_dataloader)) = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae96ca0e",
   "metadata": {},
   "source": [
    "Let's define our GCN based model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eae312",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "\n",
    "class GcnForIpu(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, batch_size):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1234)\n",
    "        self.batch_size = batch_size\n",
    "        self.conv = GCNConv(in_channels, out_channels, add_self_loops=False)\n",
    "\n",
    "    def forward(self, x, edge_index, y, batch):\n",
    "        x = self.conv(x, edge_index).relu()\n",
    "\n",
    "        x = global_mean_pool(x, batch, size=self.batch_size)\n",
    "\n",
    "        if self.training:\n",
    "            loss = F.cross_entropy(x, y)\n",
    "            return x, loss\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9111a50",
   "metadata": {},
   "source": [
    "Now we can use the dataloader with our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e02eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GcnForIpu(dataset.num_features, dataset.num_classes, batch_size=10)\n",
    "\n",
    "optim = poptorch.optim.Adam(model.parameters(), lr=0.01)\n",
    "poptorch_model = poptorch.trainingModel(model, optimizer=optim)\n",
    "poptorch_model.train()\n",
    "\n",
    "in_data = next(iter(ipu_dataloader))\n",
    "poptorch_model(in_data.x, in_data.edge_index, in_data.y, in_data.batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5274db6c",
   "metadata": {},
   "source": [
    "We can extend this simple example to make use of some of the PopTorch features mentioned above, for example increasing the number of device iterations. This will mean running the training loop on the IPU over that `deviceIterations` number of iterations, preparing this number of mini-batches on the host so these iterations can be done faster. \n",
    "\n",
    "Using the standard PopTorch dataloader unlocks some more very useful features, see the PopTorch tutorial on [Efficient Data Loading](https://github.com/graphcore/tutorials/tree/master/tutorials/pytorch/efficient_data_loading)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bef5fa",
   "metadata": {},
   "source": [
    "## Operation and layer considerations\n",
    "\n",
    "There are particular operations and layers that have to be taken into consideration when porting your model to the IPU. Many of these are because of having to compile a static graph for the IPU. Each has a simple solution which we describe below.\n",
    "\n",
    "### Operations\n",
    "\n",
    "#### Boolean indexing\n",
    "\n",
    "Indexing a tensor with a tensor of booleans can result in a tensor that isn't a fixed size in every case. This invalidates the IPU requirement of having a static graph. These operations are used in many places, for example in the calculation of the loss when a mask is applied to the final activations. We can see this in the following operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8fc9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Planetoid(dataset_directory, \"Cora\", transform=T.NormalizeFeatures())\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e20139",
   "metadata": {},
   "source": [
    "Typically we would do the following to apply the mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ab5b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.x[data.train_mask]\n",
    "y = data.y[data.train_mask]\n",
    "loss = F.cross_entropy(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265acb69",
   "metadata": {},
   "source": [
    "Depending on the number of true values in `train_mask` then `x` will be a different size per sample and therefore does not fulfill the requirement of a static graph for IPU. To avoid this we can use `torch.where` which will produce a fixed size output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3148183",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.where(data.train_mask, data.y, -100)\n",
    "loss = F.cross_entropy(data.x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1364db4c",
   "metadata": {},
   "source": [
    "Here `y` is a fixed size independent of how many true values are in `train_mask`. Here we also use the fact that `-100` is ignored by default in the loss function, therefore we populate the masked `y` values with `-100` and can skip the masking operation on `x`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38ec75c",
   "metadata": {},
   "source": [
    "### PyTorch Geometric Layers\n",
    "\n",
    "A few common layers used in PyTorch Geometric have features that need to be considered when using them with IPUs. These are listed below with solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b896aa",
   "metadata": {},
   "source": [
    "#### Global pooling layers\n",
    "\n",
    "Global pooling layers are very common in PyTorch Geometric, for example `global_mean_pool`, `global_max_pool` and `global_add_pool`. These layers attempt to calculate the batch size if not provided which cannot be done automatically on the IPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a98ce3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "x = global_mean_pool(data.x, data.batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af28c2b",
   "metadata": {},
   "source": [
    "Instead can specify the batch size as an input of the pooling function to avoid this automatic calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52477f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "x = global_mean_pool(data.x, data.batch, size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f487459a",
   "metadata": {},
   "source": [
    "#### GCNConv layers\n",
    "\n",
    "The `GCNConv` layer adds self-loops to the input graph by default. Self-loops are only added to those nodes that don't already have an existing self-loop. This results in the output having an unpredictable size and therefore does not fulfill the requirement that the graph must be static for the IPU. To avoid this we can do the following.\n",
    "\n",
    "First let's look at the layer, with self-loops turned on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df31481",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = GCNConv(in_channels=10, out_channels=10)\n",
    "conv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94213af",
   "metadata": {},
   "source": [
    "We can force this layer to not add the self-loops and instead add them at the dataset loading stage. Let's turn off the self-loops in the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f955e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = GCNConv(in_channels=10, out_channels=10, add_self_loops=False)\n",
    "conv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c40792",
   "metadata": {},
   "source": [
    "Then we need to ensure these self-loops exist in the dataset samples. We can use a transform to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd059ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.transforms as T\n",
    "\n",
    "transform = T.AddSelfLoops()\n",
    "transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d3cb7f",
   "metadata": {},
   "source": [
    "And then apply this transformation to the dataset, for example as a pretransform, shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe2523e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TUDataset(\n",
    "    f\"{dataset_directory}/self_loops\", name=\"MUTAG\", pre_transform=transform\n",
    ")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e843431",
   "metadata": {},
   "source": [
    "Now the data itself contains self-loops and they aren't required to be added in the GCN conv layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc87bb45",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we have discussed the aspects that must be considered when using PyTorch Geometric on IPUs.\n",
    "\n",
    "You should now have a good understanding of:\n",
    "* How to port an existing PyTorch Geometric model to run on the IPU.\n",
    "* How to get the most out of dataloading when using the IPU while respecting the requirement of fixed size inputs.\n",
    "\n",
    "For the next steps you can explore some of our other [tutorials](..), which look more in depth at some of the topics discussed here.\n",
    "Or take a look at our GNN examples which dive into more specific applications using state of the art models: for instance, take a look at our [Schnet Notebook](../../../../gnn/schnet/pytorch_geometric/molecular_property_prediction_with_schnet.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.2.0+1262_poptorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "traceability": {
   "sdk_version": "3.2.0+1262",
   "source_file": "1_at_a_glance.py",
   "sst_version": "0.0.10",
   "timestamp": "2023-03-13T10:22"
  },
  "vscode": {
   "interpreter": {
    "hash": "4b7133ac454a965c7b25bf17fb22ff5a4b1e3a6d812b3e17a861b05f76606f09"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
