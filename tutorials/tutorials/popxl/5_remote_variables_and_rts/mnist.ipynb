{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "037d7304",
   "metadata": {},
   "source": [
    "Copyright (c) 2022 Graphcore Ltd. All rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcc11a5",
   "metadata": {},
   "source": [
    "# Streaming Memory and Replicated Tensor Sharding\n",
    "\n",
    "In this tutorial we are going to show how to effectively use the Streaming Memory to address memory issues and how the use of Streaming Memory can be combined with Replicated Tensor Sharding.\n",
    "\n",
    "You must have followed all previous tutorials and you should first go through the `replica_grouping.ipynb` notebook that you can find in this folder.\n",
    "\n",
    "## Streaming Memory and IPU Hardware\n",
    "\n",
    "Before entering into the topic, we suggest you read the [IPU hardware overview](https://docs.graphcore.ai/projects/ipu-programmers-guide/en/3.3.0/about_ipu.html). Here we just give a very short overiew.\n",
    "\n",
    "The building blocks for IPU pods are IPU-machines, which are systems of 4 connected IPUs.\n",
    "\n",
    "Each IPU has 10 **IPU-links** to connect to other IPUs. Multiple links can connect two IPUs, in which case the data transfer is faster between these IPU with respect to others. \n",
    "In the M2000 machine, for example, the basic IPU machine has the following structure\n",
    "\n",
    "<img src=\"images/ipu_links.png\" alt=\"Fig1: IPU links\" style=\"width:400px;\"/>\n",
    "\n",
    "**Figure 1:** *Ipu links*\n",
    "\n",
    "When you use replication you don't select the IPUs to be used for replicas. Given the required number of IPUs, the best option is chosen automatically. \n",
    "\n",
    "The memory in an IPU-based machine is made up of **In-Processor-Memory** (SRAM) and **Streaming Memory**.\n",
    "\n",
    "<img src=\"images/mem_arch.png\" alt=\"Fig2: IPU memory architecture\" style=\"width:600px;\"/>\n",
    "\n",
    "**Figure 2:** *IPU memory architecture*\n",
    "\n",
    "The In-Processor-Memory is the IPU’s local **SRAM** and is split between tiles. Each tile only has direct access to code and data living in its local memory.\n",
    "\n",
    "The Streaming Memory is made up of several **DDR memory chips** and is not directly accessible to the tiles. Reading/Storing data to the Streaming Memory happens through PCle links and Gateway links and is slower than chip-to-chip communication via IPU-links.\n",
    "\n",
    "Knowing that difference is important to understand how to efficiently apply the following techniques. \n",
    "\n",
    "\n",
    "## Replicated tensor sharding \n",
    "\n",
    "When we are using replication, some tensor may be the same across replicas. \n",
    "In order to save memory, we can choose to slice these tensors so that each replica only has ` num_elements/num_replicas` elements of the tensor, with extra padding if needed (read also [popART User Guide](https://docs.graphcore.ai/projects/popart-user-guide/en/3.3.0/tensor_locations.html?highlight=replicated%20tensor%20sharding#replicated-tensor-sharding-rts)).\n",
    "\n",
    "This technique is called [Replicated tensor sharding](https://docs.graphcore.ai/projects/ipu-programmers-guide/en/3.3.0/algorithmic_techniques.html#replicated-tensor-sharding).\n",
    "\n",
    "When the full tensors are required by the program you need to make use of `gather` collectives, hence sharding comes with extra communication cost. This communication will happen through IPU-links. \n",
    "\n",
    "However, the full tensor is no longer in memory for the entire lifetime of the program, so the memory gain can be significant.\n",
    "\n",
    "When we implement data parallelism with replication, a lot of variables are duplicated: each replica is using exactly the same weights, and also the optimizer state is the same: these are typically good candidates for Replicated Tensor Sharding.\n",
    "\n",
    "As we will see in the next section, Replicated Tensor Sharding is commonly used together with remote variables because it can amortise its cost.\n",
    "\n",
    "<img src=\"images/rts.png\" alt=\"Fig3: Replicated Tensor Sharding\" style=\"width:600px;\"/>\n",
    "\n",
    "**Figure 3:** *Replicated Tensor Sharding*\n",
    "\n",
    "### RTS in popxl.addons\n",
    "Sharded tensors can be identified since they have a `meta_shape`, representing the shape of the original tensor. Hence, you can know if a tensor is sharded by checking if `t.meta_shape` is not `None`.\n",
    "The `shape` of a sharded tensor is always flattened, `(num_elements/num_replicas, )`. This is also a reminder: only element-wise operations are safe to do on shards, because you can't rely on a specific layout. The layout pertains to the full tensor.\n",
    "\n",
    "When you create a layer with the `addons.Module` class, you can specify sharded inputs with the\n",
    "`module.add_replica_sharded_variable_input` method. This works just like `module.add_variable_input` but expects sharded variables. Even if the input is sharded, the full tensor will be initialised.\n",
    "\n",
    "An analogue function `add_replica_sharded_variable_input`  is available in `addons.variable_factory` if you are using factories outside the module context.\n",
    "\n",
    "To shard a tensor either `ops.collectives.replica_sharded_slice` or `ops.collectives.replicated_reduce_scatter(..., configure_output_for_replicated_tensor_sharding=True)` can be used. Both require a ReplicaGrouping `group` parameter to specify which is the set of IPUs you want to shard the tensor in. Each shard will have `(num_elements/group_size, )` elements.\n",
    "\n",
    "Another useful function when dealing with sharded tensors is ```addons.rts.replica_sharded_spec(t: popxl.Tensor, shard_over: popxl.ReplicaGrouping)```. It returns the `TensorSpec` obtained by sharding the tensor over the  `shard_over.group_size` IPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1b7e28",
   "metadata": {},
   "source": [
    "## Remote Variables\n",
    "When the IPU memory is insufficient the Streaming Memory can be used to store data remotely.\n",
    "The ideal use case for the streaming memory is data which does not require frequent access, so that the communication cost can be amortised. \n",
    "Moreover, we can **shard** remote variables to make the transfer faster: in a way sharding a remote variable is equivalent to perform part of the transfer using the IPU-Links instead of remote transfer, increasing the effective bandwidth. \n",
    "\n",
    "To store data in the Streaming Memory we make use of **remote buffers** and **remote variables** (see also [remote variables in popxl](https://docs.graphcore.ai/projects/popxl/en/3.3.0/api/remote-buffers.html?highlight=remote%20buffers)).\n",
    "\n",
    "A remote buffer represents a data array in the Streaming Memory, and a remote variable is a tensor whose data is located at a certain position in a remote buffer (hence, in the Streaming Memory). \n",
    "\n",
    "A remote variable is always linked to a specific **entry** in a buffer, which tells where the variable starts. It provides an handle to a specific entry in the buffer.\n",
    "\n",
    "The remote buffer can be thought as an array of the given `tensor_dtype`  with `tensor_total_elements * entries` slots.\n",
    "\n",
    "The image below shows two different buffers, both with 2 entries but created with different tensor shapes, the first with a flat count of 4 elements and the second with 6. \n",
    "\n",
    "<img src=\"images/remote_buffers.jpg\" alt=\"Fig4: Remote buffers, entries and remote variables\" style=\"width:600px;\"/>\n",
    "\n",
    "**Figure 4:** *Remote buffers, entries and remote variables*\n",
    "\n",
    "When you want to actually use a remote variable in your program, you need to load the data from the buffer to the IPU.\n",
    "\n",
    "In `popxl`, remote buffers are created via `popxl.remote_buffer` and `popxl.replica_sharded_buffer`. They are handled via the corresponding `popxl.remote_variable` and `popxl.remote_replica_sharded_variable`.\n",
    "\n",
    "As we've said, each entry is handled separately and corresponds to a different variable. This is reflected by the need to specify the entry number when you create a remote variable: `popxl.remote_variable(data, buffer, entry_number, replica_grouping)`.\n",
    "\n",
    "Likewise, when you load or store a variable from a buffer you have to tell the entry number, because in this way we know where data for the variable is located\n",
    "```python\n",
    "loaded_x = ops.remote_load(buffer, entry_number)\n",
    "ops.remote_store(buffer, entry_number, loaded_x)\n",
    "```\n",
    "\n",
    "The ReplicaGrouping concept applies to remote variables in the same way it applies to standard variables.\n",
    "\n",
    "### Remote variables in popxl.addons\n",
    "\n",
    "#### Creating buffers\n",
    "In `popxl.addons` you can create `NamedRemoteBuffers` starting from `NamedVariableFactories` using \n",
    "\n",
    "```python\n",
    "addons.named_variable_buffers(named_factories, entries, shard_over_dict)\n",
    "```\n",
    "This functions create a buffer for each VariableFactory with the specified entries.\n",
    "The `shard_over_dict` parameter is optional. If it's not specified, a `replica_sharded_buffer` will be created using the replica groupings from the factories as shard groups. The default behaviour is thus: *shard over all the IPUs where the variable is equal*. If you don't want to use Replicated Tensor Sharding at all, you can set `shard_over_dict=False` (disable for all variables). If you want to customise the amount of Replicated Tensor Sharding, you can use a dictionary that associates each factory's name with an integer ranging from 1 to `group_size`, where a  `1` denotes *no sharding* and `group_size` is the maximum amount of sharding.\n",
    "\n",
    "`entries > 1` can be useful if you have multiple instances of the same layer. You can create buffers for all the layer variables specifying `entries = layers_copies` and access variables for the different copies by changing the entry number. This usually result in better code and faster compilation times with respect to having different buffers for each layer.\n",
    "\n",
    "Once you have buffers for your factories, you can create remote variables using remote initialisation:\n",
    "```python\n",
    "buffers = addons.named_variable_buffers(\n",
    "    facts, ...\n",
    ")\n",
    "# init the buffer and create remote variables\n",
    "remote_vars = facts.init_remote(buffers)\n",
    "\n",
    "# load \n",
    "# act with / modify variables\n",
    "# store back\n",
    "```\n",
    "\n",
    "#### Loading/Storing from/to buffers\n",
    "\n",
    "Once you have the `NamedRemoteBuffers` you can create a graph to load variables with  \n",
    "```python\n",
    "load_graph, names = load_remote_graph(buffers: NamedRemoteBuffers, entries: int = 1)\n",
    "```\n",
    "This function returns a `GraphWithNamedArgs` and a list of names.\n",
    "The resulting graph has no named inputs and it requires the `entry_index` as input: `load_graph.call(0)` returns the remote variable stored at index 0 in the remote buffer.\n",
    "The `entries` argument can be provided to resize each buffer as needed, in case you want to enlarge your buffer.\n",
    "\n",
    "Likewise, you can create a graph to store variables into buffer after you have updated them with \n",
    "```python\n",
    "store_graph = store_remote_graph(buffers: NamedRemoteBuffers, entries: int = 1)\n",
    "```\n",
    "The graph has a named input for each buffer provided, with the tensor shape of the buffer.  It needs to be bound before calling it.\n",
    "```python\n",
    "store_graph.bind(w0).call(0)\n",
    "```\n",
    "\n",
    "In the example below we illustrate these concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd66988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "707ca5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from functools import partial\n",
    "from typing import Mapping, Optional\n",
    "import torch\n",
    "import numpy as np\n",
    "from time import time\n",
    "from dataclasses import dataclass\n",
    "import popxl\n",
    "from popxl import ReplicaGrouping\n",
    "import popxl_addons as addons\n",
    "import popxl.ops as ops\n",
    "from typing import Union, Dict\n",
    "from popxl_addons.graph import GraphWithNamedArgs\n",
    "from popxl_addons.named_replica_grouping import NamedReplicaGrouping\n",
    "from popxl_addons.named_tensors import NamedTensors\n",
    "from popxl_addons.variable_factory import NamedVariableFactories\n",
    "from popxl_addons.rts import replica_sharded_spec, reduce_replica_sharded_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6933ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph : load_remote_subgraph(1)\n",
      "  (%1) -> (%3) {\n",
      "    Init.101 () -> (%2 [(5,) float32])\n",
      "    RemoteLoad.102 (%2 [(5,) float32], %1 [() int32]) -> (%3 [(5,) float32])\n",
      "  }\n",
      "Graph : store_remote_subgraph(2)\n",
      "  (%1, weight=%2) -> () {\n",
      "    RemoteStore.103 (%2 [(5,) float32], %1 [() int32]) -> ()\n",
      "  }\n",
      "initial values\n",
      "\t w_0:  [nan nan nan nan nan]\n",
      "\t output0:  [nan nan nan nan nan]\n",
      "\t w_1:  [nan nan nan nan nan]\n",
      "\t output1:  [nan nan nan nan nan]\n",
      "updated values\n",
      "\t w_0:  [nan nan nan nan nan]\n",
      "\t output0:  [nan nan nan nan nan]\n",
      "\t w_1:  [nan nan nan nan nan]\n",
      "\t output1:  [nan nan nan nan nan]\n"
     ]
    }
   ],
   "source": [
    "class Add(addons.Module):\n",
    "    def build(self, x: popxl.Tensor):\n",
    "        w = self.add_variable_input(\"weight\", partial(np.ones, x.shape), x.dtype)\n",
    "        x = popxl.ops.add(w, x)\n",
    "        return x\n",
    "\n",
    "\n",
    "ir = popxl.Ir()\n",
    "with ir.main_graph, popxl.in_sequence(True):\n",
    "    # ----- Create a single add graph -----\n",
    "    data = np.ones((5,), dtype=np.float32)\n",
    "    x = popxl.constant(data)\n",
    "    facts, graph = Add().create_graph(x.spec)\n",
    "\n",
    "    # ----- Create NamedRemoteBuffers -----\n",
    "    # We use two entries since we want to have two copies of the layer,\n",
    "    # referencing different remote variables\n",
    "    entries = 2\n",
    "    buffers = addons.named_variable_buffers(\n",
    "        facts, entries=entries, shard_over_dict=False\n",
    "    )  # no shards\n",
    "\n",
    "    # ----- Create create load and store graphs -----\n",
    "    load_graph, names = addons.load_remote_graph(buffers, entries=entries)\n",
    "    store_graph = addons.store_remote_graph(buffers, entries=entries)\n",
    "\n",
    "    print(load_graph.print_schedule())\n",
    "    print(store_graph.print_schedule())\n",
    "\n",
    "    # ----- Initialise remote variables, both entries -----\n",
    "    remote_vars_0 = facts.init_remote(buffers, 0)\n",
    "    remote_vars_1 = facts.init_remote(buffers, 1)\n",
    "\n",
    "    # ----- Load remote variables  -----\n",
    "    (w_loaded_0,) = load_graph.call(0)\n",
    "    (w_loaded_1,) = load_graph.call(1)\n",
    "\n",
    "    # out streams to get the value back\n",
    "    w0_initial_d2h = addons.host_store(\n",
    "        w_loaded_0\n",
    "    )  # expected output (all elements equal): 1\n",
    "    w1_initial_d2h = addons.host_store(\n",
    "        w_loaded_1\n",
    "    )  # expected output (all elements equal): 1\n",
    "\n",
    "    # ----- Bind and call  -----\n",
    "    w_named_tensors_0 = NamedTensors.pack(names, (w_loaded_0,))\n",
    "    w_named_tensors_1 = NamedTensors.pack(names, (w_loaded_1,))\n",
    "\n",
    "    bound_add_0 = graph.bind(w_named_tensors_0)  # bind to first set of variables\n",
    "    bound_add_1 = graph.bind(w_named_tensors_1)  # bind to second set of variables\n",
    "\n",
    "    (x0,) = bound_add_0.call(x)\n",
    "    (x1,) = bound_add_1.call(x)\n",
    "\n",
    "    # out stream to get the value back\n",
    "    x0_d2h = addons.host_store(x0)  # expected output (all elements equal): 1+1 = 2\n",
    "    x1_d2h = addons.host_store(x1)  # expected output (all elements equal): 1+1 = 2\n",
    "\n",
    "    # ----- Modify weights  -----\n",
    "    update_val = np.full((5,), 1.0, dtype=np.float32)\n",
    "\n",
    "    updater0 = popxl.constant(update_val)  # now first layer has w = 2\n",
    "    updater1 = popxl.constant(2 * update_val)  # now second layer has w = 3\n",
    "\n",
    "    w_loaded_0 += updater0\n",
    "    w_loaded_1 += updater1\n",
    "\n",
    "    # store the new weights into buffer\n",
    "    store_graph.bind(w_named_tensors_0).call(0)\n",
    "    store_graph.bind(w_named_tensors_1).call(1)\n",
    "\n",
    "    # load again and check that the buffer contains the updated value\n",
    "    (w_loaded_0,) = load_graph.call(0)\n",
    "    (x0_new,) = bound_add_0.call(x)\n",
    "    (w_loaded_1,) = load_graph.call(1)\n",
    "    (x1_new,) = bound_add_1.call(x)\n",
    "\n",
    "    # out stream to get the value back\n",
    "    w0_after_d2h = addons.host_store(\n",
    "        w_loaded_0\n",
    "    )  # expected output (all elements equal): 2\n",
    "    x0_new_d2h = addons.host_store(\n",
    "        x0_new\n",
    "    )  # expected output (all elements equal): 1+2 = 3\n",
    "    w1_after_d2h = addons.host_store(\n",
    "        w_loaded_1\n",
    "    )  # expected output (all elements equal): 3\n",
    "    x1_new_d2h = addons.host_store(\n",
    "        x1_new\n",
    "    )  # expected output (all elements equal): 1+3 = 4\n",
    "\n",
    "with popxl.Session(ir, \"ipu_hw\") as session:\n",
    "    outputs = session.run()\n",
    "\n",
    "print(\"initial values\")\n",
    "print(\"\\t w_0: \", outputs[w0_initial_d2h])\n",
    "print(\"\\t output0: \", outputs[x0_d2h])\n",
    "print(\"\\t w_1: \", outputs[w1_initial_d2h])\n",
    "print(\"\\t output1: \", outputs[x1_d2h])\n",
    "\n",
    "print(\"updated values\")\n",
    "print(\"\\t w_0: \", outputs[w0_after_d2h])\n",
    "print(\"\\t output0: \", outputs[x0_new_d2h])\n",
    "print(\"\\t w_1: \", outputs[w1_after_d2h])\n",
    "print(\"\\t output1: \", outputs[x1_new_d2h])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f704ce",
   "metadata": {},
   "source": [
    "## Mnist with off-chip optimizer state\n",
    "\n",
    "The starting point for this tutorial will be the [Data parallelism tutorial](../3_data_parallelism).\n",
    "\n",
    "We are going to modify the optimizer so that its state is stored remotely in the Streaming Memory:\n",
    "- The Adam optimizer now uses `add_replica_sharded_variable_input` when the variable is sharded.\n",
    "- We introduce a `remote_step` function that creates remote buffers and all the graphs (load/store + optimizer graph), load the state, call the optimizer and store the new state to the buffer. \n",
    "\n",
    "Since the optimizer state is sharded, also its inputs must be sharded: ```def build(self, var: popxl.TensorByRef, grad: popxl.Tensor, ...)``` `var` and `grad` must be sharded too.\n",
    "This implies few things:\n",
    "\n",
    "- When the optimizer is created, we need to specify sharded specs: \n",
    "```python\n",
    "optimizer.create_graph(\n",
    "    replica_sharded_spec(var, shard_over=shard_group),\n",
    "    replica_sharded_spec(grad, shard_over=shard_group),\n",
    "    ...\n",
    "    )\n",
    "```\n",
    "- When the optimizer is called, `var` and `grad` must be sharded tensors. To achieve this, we need to:\n",
    "    - Reduce gradients across replicas with `reduce_replica_sharded_graph(..., shard_groups : NamedReplicaGrouping, replica_group: ReplicaGrouping)`. This will create the appropriate collectives to `reduce` gradients across the `replica_group` provided (data parallel replicas) and shard them in the provided `shard_groups`. If the groups are the same, it becomes a `replicated_reduce_scatter(..., configure_output_for_replicated_tensor_sharding=True)`. Otherwise, it is just a `reduce` in the `replica_group` followed by a `replica_sharded_slice` in the `shard_group`.\n",
    "    - Shard the variables using `ops.collectives.replica_sharded_slice(..., group=shard_group)`. This just means splitting the tensor. \n",
    "    ``` python\n",
    "    for name, v in variables.named_tensors.items():\n",
    "        ir = popxl.gcg().ir\n",
    "        shard = ops.collectives.replica_sharded_slice(v, group=shard_groups[name])\n",
    "     ```\n",
    "- The optimizer is called with the shards as inputs: `opt.call(sharded_var, sharded_grad)`. After the call, `sharded_var` is updated with the new value since is a `TensorByRef` input. However, we need to collect all the updated shards with `ops.collectives.replicated_all_gather` and copy the new value in the original full tensor with `ops.var_updates.copy_var_update_`.\n",
    "```python\n",
    "for name, v in enumerate(sharded_vars.tensors):\n",
    "    if v.meta_shape:\n",
    "        # we need to gather the updated shards\n",
    "        v_full = ops.collectives.replicated_all_gather(v, group=shard_groups[name])\n",
    "        # and copy the updated value in the original full tensor\n",
    "        ops.var_updates.copy_var_update_(variables.tensors[name], v_full)\n",
    "```\n",
    "\n",
    "That step wouldn't be necessary if the variable were sharded too since in that case we could act on shards directly. \n",
    "\n",
    "The image below summarises the structure of the program.\n",
    "\n",
    "<img src=\"images/program_structure.png\" alt=\"Fig5: Structure of the program\" style=\"width:600px;\"/>\n",
    "\n",
    "**Figure 5:** *Structure of the program. Variables and accumulators are the same among all replicas, and are not sharded. Instead, the optimiser state is sharded so that each replica only holds a slice of the tensor. They are also remote variables, so they live in Streaming Memory and are only loaded when needed. The optimiser updates variable shards using gradients shards. Updated shards are then recombined to a full updated variable.*\n",
    "\n",
    "### Memory and performance\n",
    "You can play around with the `sharded_threshold` option to see how the throughput, execution time and memory usage change.\n",
    "You can also compare the results with the on-chip optimizer (you can run the [Data parallelism tutorial](../3_data_parallelism) with the same parameters).\n",
    "\n",
    "To analyze the memory usage you can use [Popvision Graph Analyser](https://docs.graphcore.ai/projects/graph-analyser-userguide/en/3.11.2/user-guide.html?highlight=Rx%2FTx#execution-view-options).\n",
    "\n",
    "To generate your own profiles, comment out the code related to validation in the `mnist.py` script and run it with\n",
    "```\n",
    "POPLAR_ENGINE_OPTIONS='{\"autoReport.all\":\"true\"}' python3 mnist.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4a8b5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81578e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75c8d185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist_data(test_batch_size: int, batch_size: int):\n",
    "    training_data = torch.utils.data.DataLoader(\n",
    "        torchvision.datasets.MNIST(\n",
    "            \"~/.torch/datasets\",\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=torchvision.transforms.Compose(\n",
    "                [\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                    torchvision.transforms.Normalize(\n",
    "                        (0.1307,), (0.3081,)\n",
    "                    ),  # mean and std computed on the training set.\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    validation_data = torch.utils.data.DataLoader(\n",
    "        torchvision.datasets.MNIST(\n",
    "            \"~/.torch/datasets\",\n",
    "            train=False,\n",
    "            download=True,\n",
    "            transform=torchvision.transforms.Compose(\n",
    "                [\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                    torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "        batch_size=test_batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    return training_data, validation_data\n",
    "\n",
    "\n",
    "def accuracy(predictions: np.ndarray, labels: np.ndarray):\n",
    "    ind = np.argmax(predictions, axis=-1).flatten()\n",
    "    labels = labels.detach().numpy().flatten()\n",
    "    return np.mean(ind == labels) * 100.0\n",
    "\n",
    "\n",
    "class Linear(addons.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        out_features: int,\n",
    "        bias: bool = True,\n",
    "        replica_grouping: Optional[ReplicaGrouping] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.out_features = out_features\n",
    "        self.bias = bias\n",
    "        self.replica_grouping = replica_grouping\n",
    "\n",
    "    def build(self, x: popxl.Tensor) -> popxl.Tensor:\n",
    "        w = self.add_variable_input(\n",
    "            \"weight\",\n",
    "            partial(np.random.normal, 0, 0.02, (x.shape[-1], self.out_features)),\n",
    "            x.dtype,\n",
    "            replica_grouping=self.replica_grouping,\n",
    "        )\n",
    "        y = x @ w\n",
    "        if self.bias:\n",
    "            b = self.add_variable_input(\n",
    "                \"bias\",\n",
    "                partial(np.zeros, y.shape[-1]),\n",
    "                x.dtype,\n",
    "                replica_grouping=self.replica_grouping,\n",
    "            )\n",
    "            y = y + b\n",
    "        return y\n",
    "\n",
    "\n",
    "class Net(addons.Module):\n",
    "    def __init__(self, rg: ReplicaGrouping, cache: Optional[addons.GraphCache] = None):\n",
    "        super().__init__(cache=cache)\n",
    "        self.fc1 = Linear(512, rg)\n",
    "        self.fc2 = Linear(512, rg)\n",
    "        self.fc3 = Linear(512, rg)\n",
    "        self.fc4 = Linear(10, rg)\n",
    "\n",
    "    def build(self, x: popxl.Tensor):\n",
    "        x = x.reshape((-1, 28 * 28))\n",
    "        x = ops.gelu(self.fc1(x))\n",
    "        x = ops.gelu(self.fc2(x))\n",
    "        x = ops.gelu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def evaluate_throughput(session, samples_per_step, epochs: int = 5):\n",
    "    inputs = {\n",
    "        stream: np.ones(\n",
    "            session._full_input_shape(stream.shape), stream.dtype.as_numpy()\n",
    "        )\n",
    "        for stream in session.expected_inputs()\n",
    "    }\n",
    "\n",
    "    durations = []\n",
    "    with session:\n",
    "        for i in range(epochs):\n",
    "            start = time()\n",
    "            session.run(inputs)\n",
    "            dur = time() - start\n",
    "            durations.append(dur)\n",
    "\n",
    "    duration = np.mean(durations)\n",
    "\n",
    "    result_str = (\n",
    "        f\"Mean duration: {duration} s \"\n",
    "        f\"Throughput: {samples_per_step/duration:6.1f} samples/s \"\n",
    "    )\n",
    "    print(result_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5d5efa",
   "metadata": {},
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b401ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Train:\n",
    "    micro_batch_size: int = 8\n",
    "    lr: Union[float, popxl.Tensor] = 1e-3\n",
    "    epochs: int = 1\n",
    "    gradient_accumulation: int = 1\n",
    "    data_parallel: int = 1\n",
    "    device = \"ipu_hw\"\n",
    "    sharded_threshold: int = 512\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Test:\n",
    "    micro_batch_size: int = 80\n",
    "    device = \"ipu_hw\"\n",
    "\n",
    "\n",
    "class Options:\n",
    "    train = Train()\n",
    "    test = Test()\n",
    "\n",
    "\n",
    "opts = Options()\n",
    "opts.train.micro_batch_size = 8\n",
    "opts.train.lr = 1e-3\n",
    "opts.train.epochs = 1\n",
    "opts.train.gradient_accumulation = 4\n",
    "opts.train.data_parallel = 4\n",
    "opts.train.sharded_threshold = 512\n",
    "\n",
    "opts.test.micro_batch_size = 80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e72737",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aeeb2439",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adam optimizer.\n",
    "Defines adam update step for a single variable\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Adam(addons.Module):\n",
    "    # we need to specify in_sequence because a lot of operations are in place and their order\n",
    "    # shouldn't be rearranged\n",
    "    @popxl.in_sequence()\n",
    "    def build(\n",
    "        self,\n",
    "        var: popxl.TensorByRef,\n",
    "        grad: popxl.Tensor,\n",
    "        replica_grouping: Optional[popxl.ReplicaGrouping] = None,\n",
    "        *,\n",
    "        lr: Union[float, popxl.Tensor],\n",
    "        beta1: Union[float, popxl.Tensor] = 0.9,\n",
    "        beta2: Union[float, popxl.Tensor] = 0.999,\n",
    "        eps: Union[float, popxl.Tensor] = 1e-5,\n",
    "        weight_decay: Union[float, popxl.Tensor] = 0.0,\n",
    "        first_order_dtype: popxl.dtype = popxl.float16,\n",
    "        bias_correction: bool = True,\n",
    "    ):\n",
    "\n",
    "        # gradient estimators for the variable var - same shape as the variable\n",
    "\n",
    "        # Sharded inputs must be added with add_replica_sharded_variable_input\n",
    "        if var.meta_shape:\n",
    "            # shard over factor can be automatically computed from the variable\n",
    "            shard_over = np.prod(var.meta_shape) // np.prod(var.shape)\n",
    "            first_order = self.add_replica_sharded_variable_input(\n",
    "                \"first_order\",\n",
    "                partial(np.zeros, var.meta_shape),\n",
    "                first_order_dtype,\n",
    "                replica_grouping=replica_grouping,\n",
    "                shard_over=shard_over,\n",
    "                by_ref=True,\n",
    "            )\n",
    "            second_order = self.add_replica_sharded_variable_input(\n",
    "                \"second_order\",\n",
    "                partial(np.zeros, var.meta_shape),\n",
    "                popxl.float32,\n",
    "                replica_grouping=replica_grouping,\n",
    "                shard_over=shard_over,\n",
    "                by_ref=True,\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            first_order = self.add_variable_input(\n",
    "                \"first_order\",\n",
    "                partial(np.zeros, var.shape),\n",
    "                first_order_dtype,\n",
    "                by_ref=True,\n",
    "                replica_grouping=replica_grouping,\n",
    "            )\n",
    "            second_order = self.add_variable_input(\n",
    "                \"second_order\",\n",
    "                partial(np.zeros, var.shape),\n",
    "                popxl.float32,\n",
    "                by_ref=True,\n",
    "                replica_grouping=replica_grouping,\n",
    "            )\n",
    "\n",
    "        ops.var_updates.accumulate_moving_average_(first_order, grad, f=beta1)\n",
    "        ops.var_updates.accumulate_moving_average_square_(second_order, grad, f=beta2)\n",
    "\n",
    "        # adam is a biased estimator: provide the step to correct bias\n",
    "        step = None\n",
    "        if bias_correction:\n",
    "            step = self.add_variable_input(\n",
    "                \"step\", partial(np.zeros, ()), popxl.float32, by_ref=True\n",
    "            )\n",
    "\n",
    "        # calculate the weight increment with adam heuristic\n",
    "        updater = ops.var_updates.adam_updater(\n",
    "            first_order,\n",
    "            second_order,\n",
    "            weight=var,\n",
    "            weight_decay=weight_decay,\n",
    "            time_step=step,\n",
    "            beta1=beta1,\n",
    "            beta2=beta2,\n",
    "            epsilon=eps,\n",
    "        )\n",
    "\n",
    "        # in place weight update: w += (-lr)*dw\n",
    "        ops.scaled_add_(var, updater, b=-lr)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Optimizer step  with off-chip state. Needs to be called in the main context.\n",
    "A step consists in:\n",
    "    - load state from buffer\n",
    "    - call optimizer\n",
    "    - store new state into buffer\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def remote_step(\n",
    "    var: popxl.Tensor,\n",
    "    grad: popxl.Tensor,\n",
    "    optimizer: addons.Module,\n",
    "    opts,\n",
    "    shard_group: ReplicaGrouping,\n",
    "):\n",
    "    facts, opt_graph = optimizer.create_graph(\n",
    "        replica_sharded_spec(var, shard_over=shard_group),\n",
    "        replica_sharded_spec(grad, shard_over=shard_group),\n",
    "        lr=opts.train.lr,\n",
    "        replica_grouping=popxl.gcg().ir.replica_grouping(\n",
    "            group_size=opts.train.data_parallel\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # keep the state of the optimizer in remote buffers\n",
    "    shard_over = {n: rg.group_size for n, rg in get_shard_groups(opts, facts).items()}\n",
    "    buffers = addons.named_variable_buffers(\n",
    "        facts, entries=1, shard_over_dict=shard_over\n",
    "    )\n",
    "    # create graph for loading the state\n",
    "    opt_load, names = addons.load_remote_graph(buffers, entries=1)\n",
    "    # create graph for storing the state after it is updated\n",
    "    opt_store = addons.store_remote_graph(buffers, entries=1)\n",
    "\n",
    "    # init the buffer\n",
    "    state = facts.init_remote(buffers)\n",
    "    # load remote variables: remote buffer -> device memory\n",
    "    loaded_state = opt_load.call(0)\n",
    "    state = NamedTensors.from_dict(dict(zip(names, loaded_state)))\n",
    "\n",
    "    # bind optimizer to loaded vars and call optimizer\n",
    "    opt_graph.bind(state).call(var, grad)\n",
    "\n",
    "    # bind store graph to loaded vars and store remote variables: device memory -> remote buffer\n",
    "    opt_store.bind(state).call(0)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Update all variables creating per-variable optimizers. \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def optimizer_step(\n",
    "    variables: NamedTensors,\n",
    "    grads: Dict[popxl.Tensor, popxl.Tensor],\n",
    "    optimizer: addons.Module,\n",
    "    accum_counter: popxl.Tensor,\n",
    "    opts,\n",
    "    shard_groups: Dict,\n",
    "):\n",
    "\n",
    "    for name, var in variables.named_tensors.items():\n",
    "        remote_step(var, grads[var], optimizer, opts, shard_groups[name])\n",
    "\n",
    "    if accum_counter is not None:\n",
    "        # Reset accumulators.\n",
    "        ops.var_updates.accumulator_scale_(accum_counter, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9d74c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Build the replica groupings to be used for replicated tensor sharding.\n",
    "If the tensor has less elements than the threshold, the group_size will be 1\n",
    "so that no sharding happens. Otherwise, tensors will be sharded across the data\n",
    "parallel replicas.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_shard_groups(opts, facts: NamedVariableFactories) -> NamedReplicaGrouping:\n",
    "    ir = popxl.gcg().ir\n",
    "\n",
    "    rts_groups = {}\n",
    "    for k, f in facts.to_dict().items():\n",
    "        size = np.prod(f.shape)\n",
    "        rg = f.replica_grouping\n",
    "        if size >= opts.train.sharded_threshold and size % rg.group_size == 0:\n",
    "            rts_groups[k] = rg\n",
    "        else:\n",
    "            rts_groups[k] = ir.replica_grouping(group_size=1)\n",
    "    # it is important to sort the tensor names.\n",
    "    return dict(sorted(rts_groups.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b4c39ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_program(opts):\n",
    "    ir = popxl.Ir(replication=opts.train.data_parallel)\n",
    "\n",
    "    with ir.main_graph:\n",
    "        # ----- Streams  -----\n",
    "\n",
    "        img_spec = popxl.TensorSpec(\n",
    "            (opts.train.micro_batch_size, 28, 28), popxl.float32\n",
    "        )\n",
    "\n",
    "        img_stream = popxl.h2d_stream(img_spec.shape, popxl.float32, \"image\")\n",
    "        label_stream = popxl.h2d_stream(\n",
    "            (opts.train.micro_batch_size,), popxl.int32, \"labels\"\n",
    "        )\n",
    "        loss_stream = popxl.d2h_stream((), popxl.float32, \"loss\")\n",
    "\n",
    "        # ----- Create graphs  -----\n",
    "        rg = ir.replica_grouping(group_size=opts.train.data_parallel)\n",
    "        facts, fwd_graph = Net(rg).create_graph(img_spec)\n",
    "        variables = facts.init()\n",
    "        bound_fwd = fwd_graph.bind(variables)\n",
    "        shard_groups = get_shard_groups(opts, facts)\n",
    "\n",
    "        counter = None\n",
    "        required_grads = fwd_graph.args.tensors\n",
    "\n",
    "        if opts.train.gradient_accumulation > 1:\n",
    "            bwd_facts, bwd_graph = addons.autodiff_with_accumulation(\n",
    "                fwd_graph, required_grads, replica_groupings=facts.replica_groupings\n",
    "            )\n",
    "            accumulated_grads = bwd_facts.init()\n",
    "            counter = accumulated_grads.mean_accum_counter\n",
    "            bound_bwd = bwd_graph.bind(accumulated_grads)\n",
    "        else:\n",
    "            bwd_graph = addons.autodiff(fwd_graph, grads_required=required_grads)\n",
    "\n",
    "        # ----- Gradient accumulation loop  -----\n",
    "        with popxl.in_sequence(True):\n",
    "            for ga_step in range(opts.train.gradient_accumulation):\n",
    "                # ----- Load data  -----\n",
    "\n",
    "                img_t = ops.host_load(img_stream)\n",
    "                labels = ops.host_load(label_stream, \"labels\")\n",
    "\n",
    "                # ----- Fwd  -----\n",
    "\n",
    "                # full weights are used, we are not sharding the network weights\n",
    "                fwd_info = bound_fwd.call_with_info(img_t)\n",
    "                x = fwd_info.outputs[0]\n",
    "\n",
    "                # ----- Loss  -----\n",
    "\n",
    "                loss, dx = addons.ops.cross_entropy_with_grad(x, labels)\n",
    "                ops.host_store(loss_stream, loss)\n",
    "\n",
    "                # ----- Bwd  -----\n",
    "\n",
    "                activations = bwd_graph.grad_graph_info.inputs_dict(fwd_info)\n",
    "                if opts.train.gradient_accumulation > 1:\n",
    "                    # full weights, we are not sharding the backward accumulators\n",
    "                    bound_bwd.call(dx, args=activations)\n",
    "                    grads = accumulated_grads.tensors[:-1]  # exclude the counter\n",
    "\n",
    "                else:\n",
    "                    grads = bwd_graph.call(dx, args=activations)\n",
    "\n",
    "            if opts.train.data_parallel > 1:\n",
    "                # ----- Reduce and shard gradients  -----\n",
    "                keys = [\n",
    "                    n\n",
    "                    for n, g in accumulated_grads.named_tensors.items()\n",
    "                    if n != \"mean_accum_counter\"\n",
    "                ]\n",
    "                grads = NamedTensors.pack(keys, grads)\n",
    "                # tensors whose elements exceed threshold will be reduce_scattered -> sharded\n",
    "                reduce_group = rg\n",
    "                grad_reduce, names = reduce_replica_sharded_graph(\n",
    "                    grads,\n",
    "                    \"mean\",\n",
    "                    shard_groups=NamedReplicaGrouping.from_dict(\n",
    "                        get_shard_groups(opts, bwd_facts)\n",
    "                    ),\n",
    "                    replica_group=reduce_group,\n",
    "                )\n",
    "                grads = grad_reduce.bind(grads).call()\n",
    "\n",
    "                # ----- Shard forward variables  -----\n",
    "                sharded_vars = []\n",
    "                names = []\n",
    "                for name, v in variables.named_tensors.items():\n",
    "                    ir = popxl.gcg().ir\n",
    "                    if shard_groups[name].group_size > 1:\n",
    "                        shard = ops.collectives.replica_sharded_slice(\n",
    "                            v, group=shard_groups[name]\n",
    "                        )\n",
    "                    else:\n",
    "                        shard = v\n",
    "\n",
    "                    sharded_vars.append(shard)\n",
    "                    names.append(name)\n",
    "\n",
    "                sharded_vars = NamedTensors.pack(names, sharded_vars)\n",
    "            else:\n",
    "                sharded_vars = variables\n",
    "\n",
    "            # ----- Optimizer  -----\n",
    "\n",
    "            grads_dict = dict(zip(sharded_vars.tensors, grads))\n",
    "            optimizer = Adam(cache=False)\n",
    "            # the optimizer step will update the shards in place (sharded vars are TensorByRef inputs)\n",
    "            optimizer_step(\n",
    "                sharded_vars, grads_dict, optimizer, counter, opts, shard_groups\n",
    "            )\n",
    "\n",
    "            # gather shards and copy into full tensor\n",
    "            if opts.train.data_parallel > 1:\n",
    "                for name, v in sharded_vars.named_tensors.items():\n",
    "                    if v.meta_shape:\n",
    "                        # we need to gather the updated shards\n",
    "                        v_full = ops.collectives.replicated_all_gather(\n",
    "                            v, group=shard_groups[name]\n",
    "                        )\n",
    "                        # and copy the updated value in the original full tensor\n",
    "                        ops.var_updates.copy_var_update_(\n",
    "                            variables.named_tensors[name], v_full\n",
    "                        )\n",
    "\n",
    "    ir.num_host_transfers = opts.train.gradient_accumulation\n",
    "    return (\n",
    "        popxl.Session(ir, \"ipu_hw\"),\n",
    "        [img_stream, label_stream],\n",
    "        variables,\n",
    "        loss_stream,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "117fd42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: The compile time engine option debug.branchRecordTile is set to \"5887\" when creating the Engine. (At compile-tile it was set to 1471)\n"
     ]
    }
   ],
   "source": [
    "global_batch_size = (\n",
    "    opts.train.micro_batch_size\n",
    "    * opts.train.gradient_accumulation\n",
    "    * opts.train.data_parallel\n",
    ")\n",
    "training_data, test_data = get_mnist_data(opts.test.micro_batch_size, global_batch_size)\n",
    "train_session, train_input_streams, train_variables, loss_stream = train_program(opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9e07526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss:0.2702: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:09<00:00, 47.17it/s]\n"
     ]
    }
   ],
   "source": [
    "with train_session:\n",
    "    nr_batches = len(training_data)\n",
    "    for epoch in range(1, opts.train.epochs + 1):\n",
    "        nr_batches = len(training_data)\n",
    "        for epoch in range(1, opts.train.epochs + 1):\n",
    "            print(\"Epoch {0}/{1}\".format(opts.train.epochs, opts.train.epochs))\n",
    "            bar = tqdm(training_data, total=nr_batches)\n",
    "            for data, labels in bar:\n",
    "                # reshape data accounting for replication and num hosts transfers\n",
    "                data = data.reshape(\n",
    "                    train_session.ir.num_host_transfers,\n",
    "                    train_session.ir.replication_factor,\n",
    "                    opts.train.micro_batch_size,\n",
    "                    28,\n",
    "                    28,\n",
    "                ).squeeze()\n",
    "                labels = labels.reshape(\n",
    "                    train_session.ir.num_host_transfers,\n",
    "                    train_session.ir.replication_factor,\n",
    "                    opts.train.micro_batch_size,\n",
    "                ).squeeze()\n",
    "\n",
    "                inputs: Mapping[popxl.HostToDeviceStream, np.ndarray] = dict(\n",
    "                    zip(train_input_streams, [data.squeeze().float(), labels.int()])\n",
    "                )\n",
    "                loss = train_session.run(inputs)\n",
    "                losses_np = loss[\n",
    "                    loss_stream\n",
    "                ]  # shape(ir.num_host_transfers, ir.replication_factor, )\n",
    "                avg_loss = np.mean(losses_np)\n",
    "                bar.set_description(\"Loss:{:0.4f}\".format(avg_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5b4d109e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get weights data : dictionary { train_session variables : tensor data (numpy) }\n",
    "train_vars_to_data = train_session.get_tensors_data(train_variables.tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb5e2d4",
   "metadata": {},
   "source": [
    "### Throughput and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e1c49dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_program(test_batch_size, device):\n",
    "    ir = popxl.Ir(replication=1)\n",
    "    with ir.main_graph:\n",
    "        # Inputs\n",
    "        in_stream = popxl.h2d_stream((test_batch_size, 28, 28), popxl.float32, \"image\")\n",
    "        in_t = ops.host_load(in_stream)\n",
    "\n",
    "        # Create graphs\n",
    "        rg = ir.replica_grouping(group_size=1)\n",
    "        facts, graph = Net(rg).create_graph(in_t)\n",
    "\n",
    "        # Initialise variables\n",
    "        variables = facts.init()\n",
    "\n",
    "        # Forward\n",
    "        (outputs,) = graph.bind(variables).call(in_t)\n",
    "        out_stream = popxl.d2h_stream(outputs.shape, outputs.dtype, \"outputs\")\n",
    "        ops.host_store(out_stream, outputs)\n",
    "\n",
    "    ir.num_host_transfers = 1\n",
    "    return popxl.Session(ir, device), [in_stream], variables, out_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9c6a80df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train session\n",
      "Mean duration: 0.0033693790435791017 s Throughput: 37989.2 samples/s \n"
     ]
    }
   ],
   "source": [
    "# Create test program and test session\n",
    "test_session, test_input_streams, test_variables, out_stream = test_program(\n",
    "    opts.test.micro_batch_size, opts.test.device\n",
    ")\n",
    "\n",
    "# dictionary { train_session variables : test_session variables }\n",
    "train_vars_to_test_vars = train_variables.to_mapping(test_variables)\n",
    "\n",
    "# Create a dictionary { test_session variables : tensor data (numpy) }\n",
    "# We want to copy the values before evaluating throughput on synthetic data, otherwise weights are changed\n",
    "test_vars_to_data = {\n",
    "    test_var: train_vars_to_data[train_var].copy()\n",
    "    for train_var, test_var in train_vars_to_test_vars.items()\n",
    "}\n",
    "\n",
    "# Copy trained weights to the program, with a single host to device transfer at the end\n",
    "test_session.write_variables_data(test_vars_to_data)\n",
    "\n",
    "# evaluate the ratio samples per step / time for train session\n",
    "print(\"train session\")\n",
    "evaluate_throughput(train_session, global_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3db6da79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:01<00:00, 90.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 95.87%\n"
     ]
    }
   ],
   "source": [
    "nr_batches = len(test_data)\n",
    "sum_acc = 0.0\n",
    "with test_session:\n",
    "    for data, labels in tqdm(test_data, total=nr_batches):\n",
    "        inputs: Mapping[popxl.HostToDeviceStream, np.ndarray] = dict(\n",
    "            zip(test_input_streams, [data.squeeze().float(), labels.int()])\n",
    "        )\n",
    "        output = test_session.run(inputs)\n",
    "        sum_acc += accuracy(output[out_stream], labels)\n",
    "print(\"Accuracy on test set: {:0.2f}%\".format(sum_acc / len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "de31e5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test session\n",
      "Mean duration: 0.0008002281188964844 s Throughput: 99971.5 samples/s \n"
     ]
    }
   ],
   "source": [
    "samples_per_step = (\n",
    "    opts.test.micro_batch_size\n",
    ")  # no data parallelism or gradient accumulation for inference in this program\n",
    "# evaluate the ratio samples per step / time for train session\n",
    "print(\"test session\")\n",
    "evaluate_throughput(test_session, samples_per_step)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
