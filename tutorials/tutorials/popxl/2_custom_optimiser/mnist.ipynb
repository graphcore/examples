{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2022 Graphcore Ltd. All rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Notebook autogenerated from mnist.py on 21-Jul-2022*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PopXL Custom Optimiser\n",
    "\n",
    "## Introduction\n",
    "\n",
    "We saw in the [PopXL basics tutorial](../1_basic_concepts) that we can\n",
    "easily create graphs with an internal state using the `addons.Module` class and we\n",
    "used modules to implement dense layers of a simple neural network.\n",
    "\n",
    "In this tutorial, we will learn how to implement a custom optimiser: the [Adam\n",
    "optimiser](https://paperswithcode.com/method/adam). Many optimisers, like\n",
    "layers, must manage some persistent variables. To manage this internal state we\n",
    "will re-use the same programming pattern with `addons.Module` as a base class to\n",
    "the optimiser.\n",
    "\n",
    "Once you've finished this tutorial, you will:\n",
    "\n",
    "- be able to write your own custom optimiser for your models.\n",
    "- understand graph caching and how it helps subgraph reuse.\n",
    "- have used some of the built-in rules within the `popxl.ops.var_updates` module\n",
    "  which are useful for a variety of tasks including updating your optimiser's\n",
    "  internal state variables.\n",
    "\n",
    "If you are unfamiliar with PopXL, you may want to try out\n",
    "[the tutorial covering the basic concepts](../1_basic_concepts). You may also want\n",
    "to refer to the [PopXL user guide](https://docs.graphcore.ai/projects/popxl/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the Jupyter Notebook version of this tutorial:\n",
    "\n",
    "1. Install a Poplar SDK (version 2.6 or later) and source the `enable.sh`\n",
    "   scripts for both PopART and Poplar as described in the [Getting Started\n",
    "   guide](https://docs.graphcore.ai/en/latest/getting-started.html) for your IPU\n",
    "   system.\n",
    "2. Create a virtual environment.\n",
    "3. In the same virtual environment, install the Jupyter Notebook server: `python\n",
    "   -m pip install jupyter`.\n",
    "4. Launch a Jupyter Server on a specific port: `jupyter-notebook --no-browser\n",
    "   --port <port number>`. Be sure to be in the virtual environment.\n",
    "5. Connect via SSH to your remote machine, forwarding your chosen port: `ssh -NL\n",
    "   <port number>:localhost:<port number> <your username>@<remote machine>`.\n",
    "\n",
    "For more details about this process, or if you need troubleshooting, see our\n",
    "[guide on using IPUs from Jupyter\n",
    "Notebooks](../../standard_tools/using_jupyter/README.md).\n",
    "\n",
    "If using VS Code, Intellisense can help you understand the tutorial code. It\n",
    "will show function and class descriptions when hovering over their names and\n",
    "lets you easily jump to their definitions. Consult the [VS Code setup\n",
    "guide](../VSCodeSetup.md) to use Intellisense for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "sst_ignore_md",
     "sst_ignore_code_only",
     "sst_hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "%pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "We start by importing all the modules we will need for this\n",
    "tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Mapping, Optional, Union\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "\n",
    "import popxl\n",
    "import popxl_addons as addons\n",
    "import popxl.ops as ops\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Adam optimiser\n",
    "\n",
    "Below, we implement the Adam optimiser by deriving from the `addons.Module`\n",
    "class. The `Adam` class defines the update step for a single variable in its\n",
    "`build` method.\n",
    "First we will see how to correctly deal with in-place operations, then we will\n",
    "define the update step using PopXL's `var_updates` module,\n",
    "and, finally we will test and inspect our optimiser on a single variable.\n",
    "\n",
    "### Managing in-place ops\n",
    "\n",
    "The update is performed in-place on the `weight` argument which contains the\n",
    "model variable updated by the Adam optimiser. Because this operation is in-place, we pass the\n",
    "argument as a `TensorByRef` and use the `@popxl.in_sequence()` decorator to\n",
    "prevent the operations from being rearranged by the compiler. The rest of the\n",
    "definition follows the same pattern used to add weights to our layers in the\n",
    "[PopXL basics tutorial](../1_basic_concepts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "sst_hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "class Adam(addons.Module):\n",
    "    # We need to specify `in_sequence` because many operations are in-place\n",
    "    # and their order shouldn't be changed\n",
    "    @popxl.in_sequence()\n",
    "    def build(\n",
    "        self,\n",
    "        weight: popxl.TensorByRef,\n",
    "        grad: popxl.Tensor,\n",
    "        *,\n",
    "        lr: Union[float, popxl.Tensor],\n",
    "        beta1: Union[float, popxl.Tensor] = 0.9,\n",
    "        beta2: Union[float, popxl.Tensor] = 0.999,\n",
    "        eps: Union[float, popxl.Tensor] = 1e-5,\n",
    "        weight_decay: Union[float, popxl.Tensor] = 0.0,\n",
    "        first_order_dtype: popxl.dtype = popxl.float16,\n",
    "        bias_correction: bool = True,\n",
    "    ):\n",
    "        # Gradient estimator for the variable `weight` - same shape as the variable\n",
    "        first_order = self.add_variable_input(\n",
    "            \"first_order\",\n",
    "            partial(np.zeros, weight.shape),\n",
    "            first_order_dtype,\n",
    "            by_ref=True,\n",
    "        )\n",
    "        ops.var_updates.accumulate_moving_average_(first_order, grad, f=beta1)\n",
    "\n",
    "        # Variance estimator for the variable `weight` - same shape as the variable\n",
    "        second_order = self.add_variable_input(\n",
    "            \"second_order\", partial(np.zeros, weight.shape), popxl.float32, by_ref=True\n",
    "        )\n",
    "        ops.var_updates.accumulate_moving_average_square_(second_order, grad, f=beta2)\n",
    "\n",
    "        # Adam is a biased estimator: provide the step variable to correct bias\n",
    "        step = None\n",
    "        if bias_correction:\n",
    "            step = self.add_variable_input(\n",
    "                \"step\", partial(np.zeros, ()), popxl.float32, by_ref=True\n",
    "            )\n",
    "\n",
    "        # Calculate the weight increment with an Adam heuristic\n",
    "        # Here we use the built-in `adam_updater`, but you can write your own.\n",
    "        dw = ops.var_updates.adam_updater(\n",
    "            first_order,\n",
    "            second_order,\n",
    "            weight=weight,\n",
    "            weight_decay=weight_decay,\n",
    "            time_step=step,\n",
    "            beta1=beta1,\n",
    "            beta2=beta2,\n",
    "            epsilon=eps,\n",
    "        )\n",
    "\n",
    "        # in-place weight update: weight += (-lr)*dw\n",
    "        ops.scaled_add_(weight, dw, b=-lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Adam optimiser needs state to store the mean and uncentred variance (first and second\n",
    "moments) of the gradients. These need to be of type `Variable`, hence we add them with\n",
    "`Module.add_variable_input`, creating named inputs for them (`first_order` and\n",
    "`second_order`).\n",
    "\n",
    "We used `Module.add_variable_input` in the [PopXL introductory\n",
    "tutorial](../1_basic_concepts) to add weights to our layers. However, in the\n",
    "`Adam` implementation you should notice a few differences.\n",
    "\n",
    "- We now have a `@popxl.in_sequence()` decorator on top of the `build` method.\n",
    "  This forces all operations to be added in the exact order we define\n",
    "  them, enforcing topological constraints between them. This is necessary here\n",
    "  since most of the optimiser operations are **in-place**, hence their order of\n",
    "  execution must be strictly preserved. Remember this whenever you have in-place\n",
    "  operations.\n",
    "- The `weight` input is a `popxl.TensorByRef`: any change made to this variable\n",
    "  will be automatically copied to the parent graph. See\n",
    "  [TensorByRef](https://docs.graphcore.ai/projects/popxl/en/3.2.0/api.html#popxl.Ir.create_graph)\n",
    "  for more information.\n",
    "- Some parameters, such as learning rate or weight decay, are defined as\n",
    "  `Union[float, popxl.Tensor]`. If the parameter was provided as a simple\n",
    "  `float`, it would be \"baked\" into the graph, with no possibility of changing\n",
    "  it at run-time. Instead, if the parameter is a `Tensor` (or `TensorSpec`) it\n",
    "  will appear as an input to the graph, which needs to be provided when calling\n",
    "  the graph. If you plan to change a parameter (for example, because you have a\n",
    "  learning rate schedule), this is the way to go.\n",
    "\n",
    "The rest of the logic is straightforward:\n",
    "\n",
    "- We update `first_order`, the mean estimator of the `weight` gradient.\n",
    "- We update `second_order`, the uncentred variance estimator of `weight`\n",
    "  gradient.\n",
    "- We optionally correct the estimators, since they are biased.\n",
    "- We compute the increment delta-weight `dw`.\n",
    "- We update the variable `weight` with `scaled_add_` to implement the equation\n",
    "  `weight -= lr * dw`.\n",
    "\n",
    "### Using the `var_updates` module\n",
    "\n",
    "The `ops.var_updates` module contains several useful update rules (you\n",
    "can also create your own). In this example, we will use three of the built-in\n",
    "rules:\n",
    "\n",
    "- `ops.var_updates.accumulate_moving_average_(average, new_sample, coefficient)`\n",
    "  updates `average` in-place with an exponential moving average rule:\n",
    "\n",
    "  ```python\n",
    "  average = (coefficient * average) + ((1-coefficient) * new_sample)\n",
    "  ```\n",
    "\n",
    "- `accumulate_moving_average_square_(average, new_sample, coefficient)` updates\n",
    "  `average` in-place but uses the square of the sample.\n",
    "- `ops.var_updates.adam_updater(...)` returns the Adam increment `dw` required\n",
    "  for the weight update. This is computed using the Adam internal state which\n",
    "  comprises of the first and second moments.\n",
    "\n",
    "### Using our custom optimiser\n",
    "\n",
    "Let's inspect the optimiser graph and its use in a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ir = popxl.Ir(replication=1)\n",
    "\n",
    "with ir.main_graph:\n",
    "    var = popxl.variable(np.ones((2, 2)), popxl.float32)\n",
    "    grad = popxl.variable(np.full((2, 2), 0.1), popxl.float32)\n",
    "\n",
    "    # create graph and factories - float learning rate\n",
    "    adam_facts, adam = Adam(cache=True).create_graph(var, var.spec, lr=1e-3)\n",
    "\n",
    "    # create graph and factories - Tensor learning rate\n",
    "    adam_facts_lr, adam_lr = Adam().create_graph(\n",
    "        var, var.spec, lr=popxl.TensorSpec((), popxl.float32)\n",
    "    )\n",
    "\n",
    "    print(\"Adam with float learning rate\\n\")\n",
    "    print(adam.print_schedule())\n",
    "    print(\"\\n Adam with tensor learning rate\\n\")\n",
    "    print(adam_lr.print_schedule())\n",
    "\n",
    "    # instantiate optimiser variables\n",
    "    adam_state = adam_facts.init()\n",
    "    adam_state_lr = adam_facts_lr.init()\n",
    "\n",
    "    # optimization step for float lr: call the bound graph providing the\n",
    "    # variable to update and the gradient\n",
    "    adam.bind(adam_state).call(var, grad)\n",
    "\n",
    "    # optimization step for tensor lr: call the bound graph providing the\n",
    "    # variable to update, the gradient and the learning rate\n",
    "    adam_lr.bind(adam_state_lr).call(var, grad, popxl.constant(1e-3))\n",
    "\n",
    "ir.num_host_transfers = 1\n",
    "session = popxl.Session(ir, \"ipu_hw\")\n",
    "print(\"\\n Before Adam update\")\n",
    "var_data = session.get_tensor_data(var)\n",
    "state = session.get_tensors_data(adam_state.tensors)\n",
    "print(\"Variable:\\n\", var)\n",
    "print(\"Adam state:\")\n",
    "for name, data in state.items():\n",
    "    print(name, \"\\n\", state[name])\n",
    "\n",
    "with session:\n",
    "    session.run()\n",
    "\n",
    "print(\"\\n After Adam update\")\n",
    "var_data = session.get_tensor_data(var)\n",
    "state = session.get_tensors_data(adam_state.tensors)\n",
    "print(\"Variable:\\n\", var)\n",
    "print(\"Adam state:\")\n",
    "for name, data in state.items():\n",
    "    print(name, \"\\n\", state[name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST with Adam\n",
    "\n",
    "We can now refactor our MNIST example to incorporate the Adam optimiser. Note\n",
    "that we need an optimiser for each variable: we first define the\n",
    "`optimiser_step` function which creates the graph for each variable and performs\n",
    "a full weight update for all the variables. Since the `Adam` module uses\n",
    "`cache=True`, if two graphs happens to be the same, the same graph will be\n",
    "re-used.\n",
    "\n",
    "We will use a simple float learning rate (rather than `Tensor`), since we don't\n",
    "plan to change its value during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimiser_step(\n",
    "    variables,\n",
    "    grads: Dict[popxl.Tensor, popxl.Tensor],\n",
    "    optimiser: addons.Module,\n",
    "    learning_rate: popxl.float32 = 1e-3,\n",
    "):\n",
    "    \"\"\"\n",
    "    Update all variables creating per-variable optimisers.\n",
    "    \"\"\"\n",
    "    for name, var in variables.named_tensors.items():\n",
    "        # Create optimiser and state factories for the variable\n",
    "        opt_facts, opt_graph = optimiser.create_graph(\n",
    "            var, var.spec, lr=learning_rate, weight_decay=0.0, bias_correction=False\n",
    "        )\n",
    "        state = opt_facts.init()\n",
    "\n",
    "        # Bind the graph to its state and call it.\n",
    "        # Both the state and the variables are updated in-place and are passed\n",
    "        # by ref, hence after the graph is called they are updated.\n",
    "        opt_graph.bind(state).call(var, grads[var])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data, and define our network using exactly the same code as in the\n",
    "PopXL basics tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist_data(test_batch_size: int, batch_size: int):\n",
    "    training_data = torch.utils.data.DataLoader(\n",
    "        torchvision.datasets.MNIST(\n",
    "            \"~/.torch/datasets\",\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=torchvision.transforms.Compose(\n",
    "                [\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                    # mean and std computed on the training set.\n",
    "                    torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    validation_data = torch.utils.data.DataLoader(\n",
    "        torchvision.datasets.MNIST(\n",
    "            \"~/.torch/datasets\",\n",
    "            train=False,\n",
    "            download=True,\n",
    "            transform=torchvision.transforms.Compose(\n",
    "                [\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                    torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "        batch_size=test_batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    return training_data, validation_data\n",
    "\n",
    "\n",
    "class Linear(addons.Module):\n",
    "    def __init__(self, out_features: int, bias: bool = True):\n",
    "        super().__init__()\n",
    "        self.out_features = out_features\n",
    "        self.bias = bias\n",
    "\n",
    "    def build(self, x: popxl.Tensor) -> popxl.Tensor:\n",
    "        # add a state variable to the module\n",
    "        w = self.add_variable_input(\n",
    "            \"weight\",\n",
    "            partial(np.random.normal, 0, 0.02, (x.shape[-1], self.out_features)),\n",
    "            x.dtype,\n",
    "        )\n",
    "        y = x @ w\n",
    "        if self.bias:\n",
    "            # add a state variable to the module\n",
    "            b = self.add_variable_input(\"bias\", partial(np.zeros, y.shape[-1]), x.dtype)\n",
    "            y = y + b\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "class Net(addons.Module):\n",
    "    def __init__(self, cache: Optional[addons.GraphCache] = None):\n",
    "        super().__init__(cache=cache)\n",
    "        self.fc1 = Linear(512)\n",
    "        self.fc2 = Linear(512)\n",
    "        self.fc3 = Linear(512)\n",
    "        self.fc4 = Linear(10)\n",
    "\n",
    "    def build(self, x: popxl.Tensor):\n",
    "        x = x.reshape((-1, 28 * 28))\n",
    "        x = ops.gelu(self.fc1(x))\n",
    "        x = ops.gelu(self.fc2(x))\n",
    "        x = ops.gelu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training code is almost unchanged from that of the [PopXL basics\n",
    "tutorial](../1_basic_concepts), the only difference being that we now call our\n",
    "`Adam` class and `optimiser_step` function as the optimiser instead of the\n",
    "simple `scaled_add_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_program(batch_size, device, learning_rate):\n",
    "    ir = popxl.Ir(replication=1)\n",
    "\n",
    "    with ir.main_graph:\n",
    "        # Create input streams from host to device\n",
    "        img_stream = popxl.h2d_stream((batch_size, 28, 28), popxl.float32, \"image\")\n",
    "        img_t = ops.host_load(img_stream)  # load data\n",
    "        label_stream = popxl.h2d_stream((batch_size,), popxl.int32, \"labels\")\n",
    "        labels = ops.host_load(label_stream, \"labels\")\n",
    "\n",
    "        # Create forward graph\n",
    "        facts, fwd_graph = Net().create_graph(img_t)\n",
    "\n",
    "        # Create backward graph via autodiff transform\n",
    "        bwd_graph = addons.autodiff(fwd_graph)\n",
    "\n",
    "        # Initialise variables (weights)\n",
    "        variables = facts.init()\n",
    "\n",
    "        # Call the forward graph with call_with_info because we want to retrieve\n",
    "        # information from the call site\n",
    "        fwd_info = fwd_graph.bind(variables).call_with_info(img_t)\n",
    "        x = fwd_info.outputs[0]  # forward output\n",
    "\n",
    "        # Compute loss and starting gradient for backpropagation\n",
    "        loss, dx = addons.ops.cross_entropy_with_grad(x, labels)\n",
    "\n",
    "        # Setup a stream to retrieve loss values from the host\n",
    "        loss_stream = popxl.d2h_stream(loss.shape, loss.dtype, \"loss\")\n",
    "        ops.host_store(loss_stream, loss)\n",
    "\n",
    "        # Retrieve activations from the forward graph\n",
    "        activations = bwd_graph.grad_graph_info.inputs_dict(fwd_info)\n",
    "\n",
    "        # Call the backward graph providing the starting value for\n",
    "        # backpropagation and activations\n",
    "        bwd_info = bwd_graph.call_with_info(dx, args=activations)\n",
    "\n",
    "        # Adam optimiser, with cache\n",
    "        grads_dict = bwd_graph.grad_graph_info.fwd_parent_ins_to_grad_parent_outs(\n",
    "            fwd_info, bwd_info\n",
    "        )\n",
    "        optimiser = Adam(cache=True)\n",
    "        optimiser_step(variables, grads_dict, optimiser, learning_rate)\n",
    "\n",
    "    ir.num_host_transfers = 1\n",
    "    return popxl.Session(ir, device), [img_stream, label_stream], variables, loss_stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice above that we created the `Adam` module using `cache=True`. This\n",
    "will enable graph reuse, if possible, when calling `optimiser.create_graph`. For\n",
    "our optimiser this would be when there are multiple variables with the same\n",
    "shape and data type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run a training session.\n",
    "\n",
    "Since we are using the Adam optimiser, we need to use a smaller learning rate than before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 8\n",
    "test_batch_size = 80\n",
    "device = \"ipu_hw\"\n",
    "learning_rate = 1e-3\n",
    "epochs = 1\n",
    "\n",
    "training_data, test_data = get_mnist_data(test_batch_size, train_batch_size)\n",
    "train_session, train_input_streams, train_variables, loss_stream = train_program(\n",
    "    train_batch_size, device, learning_rate\n",
    ")\n",
    "\n",
    "num_batches = len(training_data)\n",
    "with train_session:\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"Epoch {epoch}/{epochs}\")\n",
    "        bar = tqdm(training_data, total=num_batches)\n",
    "        for data, labels in bar:\n",
    "            inputs: Mapping[popxl.HostToDeviceStream, np.ndarray] = dict(\n",
    "                zip(train_input_streams, [data.squeeze().float(), labels.int()])\n",
    "            )\n",
    "            loss = train_session.run(inputs)[loss_stream]\n",
    "            bar.set_description(f\"Loss:{loss:0.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the trained weights to use during inference\n",
    "train_vars_to_data = train_session.get_tensors_data(train_variables.tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "As we did previously, to test our model we need to create an inference-only\n",
    "program and run it on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_program(test_batch_size, device):\n",
    "    ir = popxl.Ir(replication=1)\n",
    "\n",
    "    with ir.main_graph:\n",
    "        # Inputs\n",
    "        in_stream = popxl.h2d_stream((test_batch_size, 28, 28), popxl.float32, \"image\")\n",
    "        in_t = ops.host_load(in_stream)\n",
    "\n",
    "        # Create graphs\n",
    "        facts, graph = Net().create_graph(in_t)\n",
    "\n",
    "        # Initialise variables\n",
    "        variables = facts.init()\n",
    "\n",
    "        # Forward\n",
    "        (outputs,) = graph.bind(variables).call(in_t)\n",
    "        out_stream = popxl.d2h_stream(outputs.shape, outputs.dtype, \"outputs\")\n",
    "        ops.host_store(out_stream, outputs)\n",
    "\n",
    "    ir.num_host_transfers = 1\n",
    "    return popxl.Session(ir, device), [in_stream], variables, out_stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the test program and copy the trained weights to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_session, test_input_streams, test_variables, out_stream = test_program(\n",
    "    test_batch_size, device\n",
    ")\n",
    "\n",
    "train_vars_to_test_vars = train_variables.to_mapping(test_variables)\n",
    "\n",
    "test_vars_to_data = {\n",
    "    test_var: train_vars_to_data[train_var].copy()\n",
    "    for train_var, test_var in train_vars_to_test_vars.items()\n",
    "}\n",
    "\n",
    "test_session.write_variables_data(test_vars_to_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's run the test session and measure the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions: np.ndarray, labels: np.ndarray):\n",
    "    ind = np.argmax(predictions, axis=-1).flatten()\n",
    "    labels = labels.detach().numpy().flatten()\n",
    "    return np.mean(ind == labels) * 100.0\n",
    "\n",
    "\n",
    "num_batches = len(test_data)\n",
    "sum_acc = 0.0\n",
    "with test_session:\n",
    "    for data, labels in tqdm(test_data, total=num_batches):\n",
    "        inputs: Mapping[popxl.HostToDeviceStream, np.ndarray] = dict(\n",
    "            zip(test_input_streams, [data.squeeze().float(), labels.int()])\n",
    "        )\n",
    "        output = test_session.run(inputs)\n",
    "        sum_acc += accuracy(output[out_stream], labels)\n",
    "\n",
    "test_set_accuracy = sum_acc / len(test_data)\n",
    "print(f\"Accuracy on test set: {test_set_accuracy:0.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial we wrote a custom optimiser using the popxl.addons API. We\n",
    "achieved the following:\n",
    "\n",
    "- built an Adam Optimiser (by subclassing `addons.Module`) and ran it with an\n",
    "  MNIST model.\n",
    "- used the `popxl.in_sequence()` function and learnt why it is needed, to\n",
    "  prevent operations from being rearranged by the compiler.\n",
    "- used `popxl.TensorByRef` to pass variable updates back to the parent graph.\n",
    "- explored some of the functions within the `popxl.ops.var_updates` module.\n",
    "- became familiar with using `Tensor` parameters versus simpler (built-in) types\n",
    "  in `addons.Module.build`, when declaring dynamic parameters.\n",
    "- exploited graph caching in `addons.Module`: `addons.Module(cache=True)` to\n",
    "  enable graph reuse.\n",
    "\n",
    "To try out more features in PopXL [look at our other\n",
    "tutorials](../../README.md).\n",
    "\n",
    "You can also read our [PopXL User\n",
    "Guide](https://docs.graphcore.ai/projects/popxl/en/3.2.0/) for more\n",
    "information.\n",
    "\n",
    "As the PopXL API is still experimental, we would love to hear your feedback on it\n",
    "([support@graphcore.ai](mailto:support@graphcore.ai?subject=PopXL%20Feedback)).\n",
    "Your input could help drive its future direction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 ('2.5.0+981_poptorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.9"
  },
  "traceability": {
   "sdk_version": "2.6.0+1074",
   "source_file": "mnist.py",
   "sst_version": "0.0.7",
   "timestamp": "2022-07-21T18:57"
  },
  "vscode": {
   "interpreter": {
    "hash": "2a03457e0af8d7c2befa16b89da6022356bf82144fa0aff9a3e7d522dc292234"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
