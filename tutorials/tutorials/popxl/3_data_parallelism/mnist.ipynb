{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c12c975",
   "metadata": {},
   "source": [
    "Copyright (c) 2022 Graphcore Ltd. All rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269c7104",
   "metadata": {},
   "source": [
    "# Data parallelism\n",
    "\n",
    "Data parallelism techniques are essential to achieve a better throughput during execution.\n",
    "There are mainly two kind of data parallelism:\n",
    "\n",
    "- **intra-device data parallelism**. This is controlled by the **micro batch size**. When we use a ```micro_batch_size``` greater than one, the IPU executes the same program in parallel on the different samples of the micro batch. \n",
    "- **across-devices data parallelism**. We can also choose to run the same program on different devices, feeding the copies with different data. \n",
    "An efficient way to implement this is to use [**replication**](https://docs.graphcore.ai/projects/ipu-programmers-guide/en/3.1.0/algorithmic_techniques.html?highlight=micro%20batch#replication). This executes the same program across multiple devices and provides collective operations for the replicas to communicate. The ```session.ir.replication_factor``` controls the total number of replicas, whatever they are used for (we will see other uses of replication). When using replicas only to implement data parallelism, this factor will be set equal to your ```data_parallel``` factor.\n",
    "\n",
    "Unless otherwise specified, with the term data parallelism we will always imply data parallelism across different replicas.\n",
    "\n",
    "## Replication\n",
    "\n",
    "Each program has an inner execution strategy, which can include multiple devices. \n",
    "For example, we can design a program that uses 2 IPUs for its execution.\n",
    "\n",
    "It's important to understand that replication is always orthogonal to the inner structure of the program: if we choose to run a 2-IPUs program with 4 replicas, we will use ``` 2 * 4 = 8 ``` IPUs, since each replica will need 2 IPUs for its execution.\n",
    "\n",
    "With data parallelism we can process more data simultaneously, but this comes with an additional **communication** cost.\n",
    "This communication is achieved by means of **replicated collective operations**. Replicated collective operations perform a tensor operation across replicas, while standard **collective operations** perform tensor operations across devices in a single replica for multi-IPUs programs.\n",
    "\n",
    "Available collective operations can be found in ```popxl.ops.collectives```. For example, a ```replicated_all_reduce``` op takes a tensor on each replica, calculates the sum (other reduction options are available) across the replicas and then creates a new tensor on each replica holding that sum. An ```all_reduce``` op will do the same, but across IPUs within a single replica.\n",
    "\n",
    "Without data parallelism, a training step has the form\n",
    "\n",
    "```shell\n",
    "repeat:\n",
    "  # fwd and bwd can happen on multiple IPUs\n",
    "  calculate forward pass of model on micro_batch to compute loss \n",
    "  calculate backward pass of model to compute weight gradients\n",
    "  update weight using weight gradients\n",
    "```\n",
    "\n",
    "With data parallelism the training program has extra collectives operations:\n",
    "\n",
    "```shell\n",
    "repeat:\n",
    "  # fwd and bwd can happen on multiple IPUs\n",
    "  calculate forward pass of model on micro_batch to compute loss\n",
    "  calculate backward pass of model to compute weight gradients\n",
    "\n",
    "  # collectives: extra communication step, extra cost\n",
    "  obtain sum of gradients across all replicas\n",
    "\n",
    "  update weight using weight gradients\n",
    "```\n",
    "\n",
    "Each model replica will run this program on different `micro-batches`, achieving data-parallelism.\n",
    "\n",
    "![Figure 1: Replication during training ](images/data_parallel_training.png)\n",
    "<figcaption> <b>Fig 1: </b> Replication during training \n",
    " </figcaption>\n",
    "\n",
    "Sometimes the communication cost can be significant, in which case we may want to amortize it with larger batches. [Gradient accumulation](https://docs.graphcore.ai/projects/ipu-programmers-guide/en/3.1.0/algorithmic_techniques.html?highlight=micro%20batch#gradient-accumulation) can be help in this regard.\n",
    "\n",
    "## Gradient Accumulation\n",
    "\n",
    "Up to now we have been updating the weights of the network after each micro batch.\n",
    "With [Gradient accumulation](https://docs.graphcore.ai/projects/ipu-programmers-guide/en/3.1.0/algorithmic_techniques.html#gradient-accumulation) gradients are instead accumulated for ```N = gradient_accumulation``` micro batches before updating the weights. Accumulation may be summation, mean or running mean.\n",
    "\n",
    "Without gradient accumulation, a training step has the form\n",
    "```shell\n",
    "repeat: #train steps\n",
    "    load a micro batch \n",
    "    # process a micro batch\n",
    "    calculate forward pass of model on micro batch to get loss \n",
    "    calculate backward pass of model to get current weight gradients \n",
    "    # if replication\n",
    "    (obtain sum of gradients across all replicas)\n",
    "    \n",
    "    update weight using accumulated weight gradients\n",
    "```\n",
    "Each model replica processes ```micro_batch_size ``` samples per weight update.\n",
    "\n",
    "With gradient accumulation you instead have:\n",
    "\n",
    "```shell\n",
    "repeat: #train steps\n",
    "  zero accumulated weight gradients\n",
    "  repeat: # gradient accumulation\n",
    "    load a micro batch \n",
    "    # process a micro batch\n",
    "    calculate forward pass of model on micro batch to get loss \n",
    "    calculate backward pass of model to get current weight gradients\n",
    "    # accumulation step\n",
    "    add current weight gradients to accumulated weight gradients\n",
    "  #if replication\n",
    "  (obtain sum of gradients across all replicas)\n",
    "  update weight using accumulated weight gradients\n",
    "```\n",
    "In this case, each model replica processes ```micro_batch_size * gradient_accumulation``` samples per weight update, but samples are not loaded all at once. Only a ```micro_batch``` is loaded for each gradient accumulation step. This way, we can have larger batches still fitting in the device memory.\n",
    "For a given number of processed samples, the time cost of the training with or without gradient accumulation is the same: ``` device_time = time_per_sample * micro_batch_size * gradient_accumulation``` (equal if we have smaller ```micro_batches``` with ```gradient_accumulation > 1``` or a single large ```micro_batch``` with ```gradient_accumulation = 1```)\n",
    "\n",
    "However, from a memory perspective we have an advantage: since we don't have to load the full batch on the device, we can use larger batches and have a better throughput ( large batches are not always a good strategy from a training perspective though).\n",
    "For this same reason gradient accumulation can be used with data parallelism to amortize the communication cost: if more samples are processed, the communication cost is less relevant.\n",
    "Another use case for gradient accumulation is pipelining, which will be investigated in another tutorial.\n",
    "\n",
    "Taking into account both data parallelism and gradient accumulation, the total number of samples that contribute to an optimizer step is\n",
    "\n",
    "```global_batch_size = micro_batch_size * gradient_accumulation * data_parallel```\n",
    "\n",
    "## Batch Terminology\n",
    "- ```micro_batch_size``` size of the micro batch. Determines the data parallelism on a single device and is the number of samples that contribute to a gradient accumulation step.\n",
    "- ```gradient_accumulation``` number of micro batches processed from a single replica before updating the weights\n",
    "- ```data_parallel``` number of replicas used to implement data parallelism\n",
    "- ```global_batch_size``` total number of samples that contributes to a weight update ```global_batch_size = micro_batch_size * gradient_accumulation * data_parallel ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dedd115",
   "metadata": {},
   "source": [
    "## Mnist with Gradient Accumulation & Data Parallelism\n",
    "\n",
    "To add data parallelism to the program we first need to specify a ```ir.replication_factor ``` > 1. Before updating the weights we also have to add communication between replicas via **collective** ops. We can use ``` ops.collectives.replicated_all_reduce_```. Note that if we perform the operation in place, we need to use ```in_sequence(True)``` context.\n",
    "\n",
    "```python\n",
    "def train_program(opts):\n",
    "    # total number of replicas used in the program, regardless of their use\n",
    "    # here, we are using them to implement data parallelism, and no other \n",
    "    # use of replication is involved.\n",
    "    ir = popxl.Ir(replication_factor = opts.train.data_parallel)\n",
    "\n",
    "    with ir.main_graph:\n",
    "        ...\n",
    "            with popxl.in_sequence(True):\n",
    "            # fwd\n",
    "            # bwd\n",
    "            # reduce gradients across replicas\n",
    "                for g in grads:\n",
    "                    g = ops.collectives.replicated_all_reduce_(g, op = 'mean')\n",
    "\n",
    "            # optimizer step\n",
    "    ...\n",
    "```\n",
    "\n",
    "To add also gradient accumulation the first thing to do is using the ```addons.transforms.autodiff_with_accumulation``` transform instead of ```autodiff``` to generate the backward graph.\n",
    "```python\n",
    "def autodiff_with_accumulation(\n",
    "        graph: GraphWithNamedArgs,\n",
    "        tensors_to_accumulate_grads: Iterable[popxl.Tensor],\n",
    "        grads_required: Optional[Iterable[popxl.Tensor]] = None) -> Tuple[NamedVariableFactories, GraphWithNamedArgs]\n",
    "```\n",
    "While the standard ```autodiff``` transform produces a graph without state, i.e, without variables, the ```autodiff_with_accumulation``` transform generates a graph with state, hence returning both the graph and the variable factories for the ```NamedArgs```, which are the accumulators for the ```tensors_to_accumulate_grads``` and a ```mean_accum_counter``` which is incremented with each call of the gradient graph.  \n",
    "Each tensor in ```tensors_to_accumulate_grads``` is automatically added as a required grad. You can provide another list of tensors in ```grads_required``` for non-accumulated gradients. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345d852b",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"images/autodiff.png\" />\n",
    "    <img src=\"images/backward_auto_accum.png\"/>\n",
    "    <figcaption> <b>Fig 2: </b> Differences between <code>autodiff</code> and <code>autodiff_with_accumulation</code>. On the left, <code>autodiff</code> logic. The backward takes as inputs forward activations y, the derivative y', x and w and produces the derivatives x' and w'. On the right,  <code>autodiff_with_accumulation</code>  logic. The backward takes as inputs forward activations y, the derivative y', x, w and the accumulators X' and W'. No output is produced since the result is accumulated in X' and W'. I are intermediate tensors.\n",
    "</figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fea6ea",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"images/autodiff_transf.png\"/>\n",
    "    <figcaption> <b>Fig 3: </b> <code>autodiff_with_accumulation</code> calls <code>autodiff</code> and then for each tensor in <code>tensors_to_accumulate_grads</code> adds an operation to the output gradient\n",
    "    graph which takes a running mean of the tensor and the result stored in an accumulator tensor. The accumulators are added as NamedArgs TensorByRef inputs to the grad graph and the corresponding output of the original tensor removed.\n",
    " </figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6f640f",
   "metadata": {},
   "source": [
    "After each weight update the running mean needs to be reset. To this aim it's enough to reset the counter with  `ops.var_updates.accumulator_zero_(grad_args.mean_accum_counter)` after each weight update:\n",
    "\n",
    "```python\n",
    "def optimizer_step(...):\n",
    "    # update variables\n",
    "        ...\n",
    "    # Reset accumulators.\n",
    "    ops.var_updates.accumulator_scale_(grads.mean_accum_counter, 0.0)\n",
    "```\n",
    "\n",
    "Finally, note that in a gradient accumulation loop we typically have ```host loads``` in the program:\n",
    "\n",
    "```shell\n",
    "for _ in range(gradient_accumulation):\n",
    "      ...\n",
    "      input = ops.host_load(input_stream)\n",
    "      ...\n",
    "```\n",
    "Remember from [session user guide]() that\n",
    "\n",
    "> For each host_load (per tensor) operation in your model to run, you will need to increment the num_host_transfers by one. \n",
    "\n",
    "Hence, we will need to set\n",
    "```\n",
    "session.ir.num_host_transfers = gradient_accumulation\n",
    "```\n",
    "We will also need to provide training data in the appropriate data format when running the session: it should be of shape \n",
    "```\n",
    "(num_host_transfers, replication_factor, *device_shape)\n",
    "```\n",
    "\n",
    "In this example we present both techniques. When gradient accumulation is 1, standard autodiff is used to avoid the extra memory from the accumulator variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13255df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdc53cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from functools import partial\n",
    "from typing import Mapping, Optional\n",
    "import torch\n",
    "import numpy as np\n",
    "from time import time\n",
    "from dataclasses import dataclass\n",
    "import popxl\n",
    "import popxl_addons as addons\n",
    "import popxl.ops as ops\n",
    "from typing import Union, Dict\n",
    "from popxl_addons.graph import GraphWithNamedArgs\n",
    "from popxl_addons.named_tensors import NamedTensors\n",
    "from popxl_addons.variable_factory import NamedVariableFactories\n",
    "import torch\n",
    "import torchvision\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1372280",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704dc9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist_data(test_batch_size: int, batch_size: int):\n",
    "    training_data = torch.utils.data.DataLoader(\n",
    "        torchvision.datasets.MNIST(\n",
    "            \"~/.torch/datasets\",\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=torchvision.transforms.Compose(\n",
    "                [\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                    torchvision.transforms.Normalize(\n",
    "                        (0.1307,), (0.3081,)\n",
    "                    ),  # mean and std computed on the training set.\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    validation_data = torch.utils.data.DataLoader(\n",
    "        torchvision.datasets.MNIST(\n",
    "            \"~/.torch/datasets\",\n",
    "            train=False,\n",
    "            download=True,\n",
    "            transform=torchvision.transforms.Compose(\n",
    "                [\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                    torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "        batch_size=test_batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    return training_data, validation_data\n",
    "\n",
    "\n",
    "def accuracy(predictions: np.ndarray, labels: np.ndarray):\n",
    "    ind = np.argmax(predictions, axis=-1).flatten()\n",
    "    labels = labels.detach().numpy().flatten()\n",
    "    return np.mean(ind == labels) * 100.0\n",
    "\n",
    "\n",
    "class Linear(addons.Module):\n",
    "    def __init__(self, out_features: int, bias: bool = True):\n",
    "        super().__init__()\n",
    "        self.out_features = out_features\n",
    "        self.bias = bias\n",
    "\n",
    "    def build(self, x: popxl.Tensor) -> popxl.Tensor:\n",
    "        # add a state variable to the module\n",
    "        w = self.add_variable_input(\n",
    "            \"weight\",\n",
    "            partial(np.random.normal, 0, 0.02, (x.shape[-1], self.out_features)),\n",
    "            x.dtype,\n",
    "        )\n",
    "        y = x @ w\n",
    "        if self.bias:\n",
    "            # add a state variable to the module\n",
    "            b = self.add_variable_input(\"bias\", partial(np.zeros, y.shape[-1]), x.dtype)\n",
    "            y = y + b\n",
    "        return y\n",
    "\n",
    "\n",
    "class Net(addons.Module):\n",
    "    def __init__(self, cache: Optional[addons.GraphCache] = None):\n",
    "        super().__init__(cache=cache)\n",
    "        self.fc1 = Linear(512)\n",
    "        self.fc2 = Linear(512)\n",
    "        self.fc3 = Linear(512)\n",
    "        self.fc4 = Linear(10)\n",
    "\n",
    "    def build(self, x: popxl.Tensor):\n",
    "        x = x.reshape((-1, 28 * 28))\n",
    "        x = ops.gelu(self.fc1(x))\n",
    "        x = ops.gelu(self.fc2(x))\n",
    "        x = ops.gelu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def evaluate_throughput(session, samples_per_step, epochs: int = 5):\n",
    "    inputs = {\n",
    "        stream: np.ones(\n",
    "            session._full_input_shape(stream.shape), stream.dtype.as_numpy()\n",
    "        )\n",
    "        for stream in session.expected_inputs()\n",
    "    }\n",
    "\n",
    "    durations = []\n",
    "    with session:\n",
    "        for i in range(epochs):\n",
    "            start = time()\n",
    "            session.run(inputs)\n",
    "            dur = time() - start\n",
    "            durations.append(dur)\n",
    "\n",
    "    duration = np.mean(durations)\n",
    "\n",
    "    result_str = (\n",
    "        f\"Mean duration: {duration} s \"\n",
    "        f\"Throughput: {samples_per_step/duration:6.1f} samples/s \"\n",
    "    )\n",
    "    print(result_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260ca73e",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147eb0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Train:\n",
    "    micro_batch_size: int = 8\n",
    "    lr: Union[float, popxl.Tensor] = 1e-3\n",
    "    epochs: int = 1\n",
    "    gradient_accumulation: int = 1\n",
    "    data_parallel: int = 1\n",
    "    device = \"ipu_hw\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Test:\n",
    "    micro_batch_size: int = 80\n",
    "    device = \"ipu_hw\"\n",
    "\n",
    "\n",
    "class Options:\n",
    "    train = Train()\n",
    "    test = Test()\n",
    "\n",
    "\n",
    "opts = Options()\n",
    "opts.train.micro_batch_size = 8\n",
    "opts.train.lr = 1e-3\n",
    "opts.train.epochs = 1\n",
    "opts.train.gradient_accumulation = 4\n",
    "opts.train.data_parallel = 2\n",
    "\n",
    "opts.test.micro_batch_size = 80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a5cdb9",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aecdfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adam optimizer.\n",
    "Defines adam update step for a single variable\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Adam(addons.Module):\n",
    "    # we need to specify in_sequence because a lot of operations are in place and their order\n",
    "    # shouldn't be rearranged\n",
    "    @popxl.in_sequence()\n",
    "    def build(\n",
    "        self,\n",
    "        var: popxl.TensorByRef,\n",
    "        grad: popxl.Tensor,\n",
    "        *,\n",
    "        lr: Union[float, popxl.Tensor],\n",
    "        beta1: Union[float, popxl.Tensor] = 0.9,\n",
    "        beta2: Union[float, popxl.Tensor] = 0.999,\n",
    "        eps: Union[float, popxl.Tensor] = 1e-5,\n",
    "        weight_decay: Union[float, popxl.Tensor] = 1e-2,\n",
    "        first_order_dtype: popxl.dtype = popxl.float16,\n",
    "        bias_correction: bool = True\n",
    "    ):\n",
    "\n",
    "        # gradient estimators for the variable var - same shape as the variable\n",
    "        first_order = self.add_variable_input(\n",
    "            \"first_order\", partial(np.zeros, var.shape), first_order_dtype, by_ref=True\n",
    "        )\n",
    "        ops.var_updates.accumulate_moving_average_(first_order, grad, f=beta1)\n",
    "\n",
    "        # variance estimators for the variable var - same shape as the variable\n",
    "        second_order = self.add_variable_input(\n",
    "            \"second_order\", partial(np.zeros, var.shape), popxl.float32, by_ref=True\n",
    "        )\n",
    "        ops.var_updates.accumulate_moving_average_square_(second_order, grad, f=beta2)\n",
    "\n",
    "        # adam is a biased estimator: provide the step to correct bias\n",
    "        step = None\n",
    "        if bias_correction:\n",
    "            step = self.add_variable_input(\n",
    "                \"step\", partial(np.zeros, ()), popxl.float32, by_ref=True\n",
    "            )\n",
    "\n",
    "        # calculate the weight increment with adam heuristic\n",
    "        updater = ops.var_updates.adam_updater(\n",
    "            first_order,\n",
    "            second_order,\n",
    "            weight=var,\n",
    "            weight_decay=weight_decay,\n",
    "            time_step=step,\n",
    "            beta1=beta1,\n",
    "            beta2=beta2,\n",
    "            epsilon=eps,\n",
    "        )\n",
    "\n",
    "        # in place weight update: w += (-lr)*dw\n",
    "        ops.scaled_add_(var, updater, b=-lr)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Update all variables creating per-variable optimizers. \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def optimizer_step(\n",
    "    variables: NamedTensors,\n",
    "    grads: Dict[popxl.Tensor, popxl.Tensor],\n",
    "    optimizer: addons.Module,\n",
    "    accum_counter: popxl.Tensor,\n",
    "    lr: popxl.float32 = 1e-3,\n",
    "):\n",
    "    for name, var in variables.named_tensors.items():\n",
    "        # create optimizer and state factories for the variable\n",
    "        opt_facts, opt_graph = optimizer.create_graph(\n",
    "            var, var.spec, lr=lr, weight_decay=0.0, bias_correction=False\n",
    "        )\n",
    "        state = opt_facts.init()\n",
    "        # bind the graph to its state and call it.\n",
    "        opt_graph.bind(state).call(var, grads[var])\n",
    "\n",
    "    if accum_counter is not None:\n",
    "        # Reset accumulators.\n",
    "        # Resetting the counter for mean gradient accumulation is sufficient to zero the accumulators\n",
    "        # in the next call to ops.accumulate_mean_\n",
    "        ops.var_updates.accumulator_scale_(accum_counter, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5d24fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_program(opts):\n",
    "    ir = popxl.Ir(replication=opts.train.data_parallel)\n",
    "    # total number of replicas used in the program, regardeless their use\n",
    "    # here, we are using them to implement data parallelism, and no other\n",
    "    # use of replication is involved.\n",
    "\n",
    "    with ir.main_graph:\n",
    "        # Create input streams from host to device\n",
    "        img_spec = popxl.TensorSpec(\n",
    "            (opts.train.micro_batch_size, 28, 28), popxl.float32\n",
    "        )\n",
    "        img_stream = popxl.h2d_stream(img_spec.shape, popxl.float32, \"image\")\n",
    "        label_stream = popxl.h2d_stream(\n",
    "            (opts.train.micro_batch_size,), popxl.int32, \"labels\"\n",
    "        )\n",
    "        loss_stream = popxl.d2h_stream((), popxl.float32, \"loss\")\n",
    "\n",
    "        # Create forward graph\n",
    "        facts, fwd_graph = Net().create_graph(img_spec)\n",
    "        variables = facts.init()\n",
    "        bound_fwd = fwd_graph.bind(variables)\n",
    "\n",
    "        counter = None\n",
    "        required_grads = fwd_graph.args.tensors\n",
    "\n",
    "        if opts.train.gradient_accumulation > 1:\n",
    "            bwd_facts, bwd_graph = addons.autodiff_with_accumulation(\n",
    "                fwd_graph, required_grads\n",
    "            )\n",
    "            accumulated_grads = bwd_facts.init()\n",
    "            counter = accumulated_grads.mean_accum_counter\n",
    "            bound_bwd = bwd_graph.bind(accumulated_grads)\n",
    "        else:\n",
    "            # standard autodiff, avoid extra memory\n",
    "            bwd_graph = addons.autodiff(fwd_graph, grads_required=required_grads)\n",
    "\n",
    "        # in sequence needed for in place ops\n",
    "        with popxl.in_sequence(True):\n",
    "            # gradient accumulation loop\n",
    "            for ga_step in range(opts.train.gradient_accumulation):\n",
    "                # load data\n",
    "                img_t = ops.host_load(img_stream)\n",
    "                labels = ops.host_load(label_stream, \"labels\")\n",
    "\n",
    "                # fwd\n",
    "                fwd_info = bound_fwd.call_with_info(img_t)\n",
    "                x = fwd_info.outputs[0]\n",
    "\n",
    "                # loss\n",
    "                loss, dx = addons.ops.cross_entropy_with_grad(x, labels)\n",
    "                ops.host_store(loss_stream, loss)\n",
    "\n",
    "                # bwd\n",
    "                activations = bwd_graph.grad_graph_info.inputs_dict(fwd_info)\n",
    "\n",
    "                if opts.train.gradient_accumulation > 1:\n",
    "                    bound_bwd.call(dx, args=activations)\n",
    "                    grads = accumulated_grads.tensors[:-1]  # exclude the counter\n",
    "\n",
    "                else:\n",
    "                    grads = bwd_graph.call(dx, args=activations)\n",
    "\n",
    "            # reduce gradients across replicas with add and divide by the number of replicas\n",
    "            if opts.train.data_parallel > 1:\n",
    "                for g in grads:\n",
    "                    g = ops.collectives.replicated_all_reduce_(g, op=\"mean\")\n",
    "\n",
    "            # optimizer step: the optimizer resets the accumulators\n",
    "            grads_dict = dict(zip(variables.tensors, grads))\n",
    "            optimizer = Adam(cache=True)\n",
    "            optimizer_step(variables, grads_dict, optimizer, counter, opts.train.lr)\n",
    "\n",
    "    # we have a for loop, the number of host loads is equal to gradient_accumulation\n",
    "    ir.num_host_transfers = opts.train.gradient_accumulation\n",
    "\n",
    "    return (\n",
    "        popxl.Session(ir, \"ipu_hw\"),\n",
    "        [img_stream, label_stream],\n",
    "        variables,\n",
    "        loss_stream,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b0cb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_batch_size = (\n",
    "    opts.train.micro_batch_size\n",
    "    * opts.train.gradient_accumulation\n",
    "    * opts.train.data_parallel\n",
    ")\n",
    "training_data, test_data = get_mnist_data(opts.test.micro_batch_size, global_batch_size)\n",
    "train_session, train_input_streams, train_variables, loss_stream = train_program(opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b7178a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_batches = len(training_data)\n",
    "with train_session:\n",
    "    for epoch in range(1, opts.train.epochs + 1):\n",
    "        nr_batches = len(training_data)\n",
    "        for epoch in range(1, opts.train.epochs + 1):\n",
    "            print(\"Epoch {0}/{1}\".format(opts.train.epochs, opts.train.epochs))\n",
    "            bar = tqdm(training_data, total=nr_batches)\n",
    "            for data, labels in bar:\n",
    "                # reshape data accounting for replication and num hosts transfers\n",
    "                data = data.reshape(\n",
    "                    train_session.ir.num_host_transfers,\n",
    "                    train_session.ir.replication_factor,\n",
    "                    opts.train.micro_batch_size,\n",
    "                    28,\n",
    "                    28,\n",
    "                ).squeeze()\n",
    "                labels = labels.reshape(\n",
    "                    train_session.ir.num_host_transfers,\n",
    "                    train_session.ir.replication_factor,\n",
    "                    opts.train.micro_batch_size,\n",
    "                ).squeeze()\n",
    "\n",
    "                inputs: Mapping[popxl.HostToDeviceStream, np.ndarray] = dict(\n",
    "                    zip(train_input_streams, [data.squeeze().float(), labels.int()])\n",
    "                )\n",
    "                loss = train_session.run(inputs)\n",
    "                losses_np = loss[\n",
    "                    loss_stream\n",
    "                ]  # shape(ir.num_host_transfers, ir.replication_factor, )\n",
    "                avg_loss = np.mean(losses_np)\n",
    "                bar.set_description(\"Loss:{:0.4f}\".format(avg_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870e1602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get weights data : dictionary { train_session variables : tensor data (numpy) }\n",
    "train_vars_to_data = train_session.get_tensors_data(train_variables.tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c4b7db",
   "metadata": {},
   "source": [
    "### Throughput and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ce6a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_program(test_batch_size, device):\n",
    "    ir = popxl.Ir(replication=1)\n",
    "\n",
    "    with ir.main_graph:\n",
    "        # Inputs\n",
    "        in_stream = popxl.h2d_stream((test_batch_size, 28, 28), popxl.float32, \"image\")\n",
    "        in_t = ops.host_load(in_stream)\n",
    "\n",
    "        # Create graphs\n",
    "        facts, graph = Net().create_graph(in_t)\n",
    "\n",
    "        # Initialise variables\n",
    "        variables = facts.init()\n",
    "\n",
    "        # Forward\n",
    "        (outputs,) = graph.bind(variables).call(in_t)\n",
    "        out_stream = popxl.d2h_stream(outputs.shape, outputs.dtype, \"outputs\")\n",
    "        ops.host_store(out_stream, outputs)\n",
    "\n",
    "    ir.num_host_transfers = 1\n",
    "    return popxl.Session(ir, device), [in_stream], variables, out_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3828e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_session, test_input_streams, test_variables, out_stream = test_program(\n",
    "    opts.test.micro_batch_size, opts.test.device\n",
    ")\n",
    "\n",
    "# dictionary { train_session variables : test_session variables }\n",
    "train_vars_to_test_vars = train_variables.to_mapping(test_variables)\n",
    "\n",
    "# Create a dictionary { test_session variables : tensor data (numpy) }\n",
    "# We want to copy the values before evaluating throughput on synthetic data, otherwise weights are changed\n",
    "test_vars_to_data = {\n",
    "    test_var: train_vars_to_data[train_var].copy()\n",
    "    for train_var, test_var in train_vars_to_test_vars.items()\n",
    "}\n",
    "\n",
    "# Copy trained weights to the program, with a single host to device transfer at the end\n",
    "test_session.write_variables_data(test_vars_to_data)\n",
    "\n",
    "# evaluate the ratio samples per step / time for train session\n",
    "print(\"train session\")\n",
    "evaluate_throughput(train_session, global_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66304208",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_batches = len(test_data)\n",
    "sum_acc = 0.0\n",
    "with test_session:\n",
    "    for data, labels in tqdm(test_data, total=nr_batches):\n",
    "        inputs: Mapping[popxl.HostToDeviceStream, np.ndarray] = dict(\n",
    "            zip(test_input_streams, [data.squeeze().float(), labels.int()])\n",
    "        )\n",
    "        output = test_session.run(inputs)\n",
    "        sum_acc += accuracy(output[out_stream], labels)\n",
    "print(\"Accuracy on test set: {:0.2f}%\".format(sum_acc / len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3e9a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_per_step = (\n",
    "    opts.test.micro_batch_size\n",
    ")  # no data parallelism or gradient accumulation for inference in this program\n",
    "# evaluate the ratio samples per step / time for test session\n",
    "print(\"test session\")\n",
    "evaluate_throughput(test_session, samples_per_step)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
