{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0180a639",
   "metadata": {},
   "source": [
    "Copyright (c) 2022 Graphcore Ltd. All rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc6c5dd",
   "metadata": {},
   "source": [
    "*Notebook autogenerated from demo.py on 28-Sep-2022*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc9d343",
   "metadata": {},
   "source": [
    "# Using TensorBoard in TensorFlow 2 on the IPU\n",
    "\n",
    "TensorBoard is a visualization tool provided with TensorFlow that provides a\n",
    "platform to view many aspects of model training and inference at near real-time\n",
    "intervals (if so configured). Data that may be visualized includes, but is not\n",
    "limited to:\n",
    "\n",
    "  - Training losses and metrics\n",
    "  - Images\n",
    "  - Model graphs\n",
    "  - Histograms/distributions of variables (such as model parameters,\n",
    "  hyperparameters, etc.)\n",
    "\n",
    "In fact, the data that may be displayed in TensorBoard is entirely customizable\n",
    "by the use of custom callbacks (as we will see later in this tutorial). IPU\n",
    "TensorFlow works with TensorBoard out of the box!\n",
    "\n",
    "![Alt Text](figures/ExampleScreen.png \"TensorBoard Example\")\n",
    "\n",
    "In this tutorial, we shall develop a very simple convolutional network to train\n",
    "on the MNIST dataset for character recognition. This problem is purposely\n",
    "selected for demonstration as it is rather straightforward and allows the focus\n",
    "of this tutorial to be on the use of TensorBoard.\n",
    "\n",
    "## Preliminary Setup\n",
    "To run the code in this tutorial there are two basic requirements.\n",
    "\n",
    "- A Poplar SDK environment enabled, with the Graphcore port of TensorFlow 2\n",
    "  installed (see the [Getting Started\n",
    "  guide](https://docs.graphcore.ai/en/latest/getting-started.html) for your IPU\n",
    "  system)\n",
    "- Python packages installed with `python -m pip install -r requirements.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06f1c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650773a8",
   "metadata": {},
   "source": [
    "If you wish to follow along with the Jupyter notebook provided for this\n",
    "tutorial, then some additional setup steps are required.\n",
    "\n",
    "First, ensuring that you have a Poplar SDK environment enabled, install the\n",
    "Jupyter Notebook server in that environment, as follows.\n",
    "\n",
    "```\n",
    "python -m pip install jupyter\n",
    "```\n",
    "\n",
    "Once the Jupyter Notebook server is installed in your environment, start\n",
    "Jupyter as follows.\n",
    "\n",
    "```\n",
    "jupyter-notebook --no-browser --port 8888\n",
    "```\n",
    "\n",
    "On your local machine, you can now forward the port 8888 (chosen here) to\n",
    "the remote machine running your Poplar environment. Note that the choice\n",
    "of port is entirely up to you. Here, 8888 is simply an example.\n",
    "\n",
    "```\n",
    "ssh -NL 8888:localhost:8888 <your-username>@<your-machine>.<your-domain>\n",
    "```\n",
    "\n",
    "You can now navigate in your web browser to the Jupyter instance running on\n",
    "your remote machine via the address `localhost:8888`.\n",
    "\n",
    "For more details about this process, or if you need troubleshooting, see our\n",
    "[guide on using IPUs from Jupyter\n",
    "notebooks](../../standard_tools/using_jupyter/README.md).\n",
    "\n",
    "## Introduction to TensorBoard and Data Logging\n",
    "### How does TensorBoard work?\n",
    "TensorBoard itself runs independently of any TensorFlow processes, so starting\n",
    "and stopping TensorBoard does not affect any TensorFlow processes that may be\n",
    "running. The reason that TensorBoard and TensorFlow are independent in this way\n",
    "is due to the method of data transfer between them. Rather than a client/server\n",
    "model as one might first expect, there is no direct communication between the\n",
    "two packages.\n",
    "\n",
    "Instead, TensorBoard monitors a **log directory** to which your TensorFlow\n",
    "program writes data in a format that is parsable to TensorBoard. So, getting\n",
    "TensorBoard running and ready to start displaying data from your TensorFlow\n",
    "program is as simple as just starting the Daemon.\n",
    "\n",
    "## How do I launch TensorBoard?\n",
    "If we go ahead and enter `tensorboard` into a terminal (on a machine that has a\n",
    "TensorFlow installation and `tensorboard` on your path), you will notice the\n",
    "following error.\n",
    "\n",
    "```\n",
    "Error: A logdir or db must be specified. For example `tensorboard --logdir mylogdir` or `tensorboard --db sqlite:~/.tensorboard.db`. Run `tensorboard --helpful` for details and examples.\n",
    "```\n",
    "\n",
    "As we can see, TensorBoard will not start without a log directory (or,\n",
    "alternatively a SQLite DB) being provided.\n",
    "\n",
    "For the purpose of this tutorial, we shall be using the\n",
    "`/tmp/tensorboard_example_logs` directory as our log directory. Let's go ahead\n",
    "and launch TensorBoard.\n",
    "\n",
    "```\n",
    "tensorboard --logdir /tmp/tensorboard_example_logs\n",
    "```\n",
    "\n",
    "If all is well, then you should see output akin to the following on your terminal.\n",
    "\n",
    "```\n",
    "TensorBoard 2.7.0 at http://<your-machine>.<your-domain>:6006/ (Press CTRL+C to quit)\n",
    "```\n",
    "\n",
    "If you navigate to the address given in your terminal (in a browser of your\n",
    "choice), you should see the blank (no logged data) TensorBoard interface.\n",
    "\n",
    "![Alt Text](figures/BlankSession.png \"Blank TensorBoard Session\")\n",
    "\n",
    "### TensorBoard on a Remote Machine\n",
    "If you use a remote machine for your TensorFlow development, then there are a\n",
    "couple of options for accessing TensorBoard remotely.\n",
    "\n",
    "#### SSH Tunnelling\n",
    "The first (and more secure) approach is to setup an SSH tunnel to your remote\n",
    "machine, binding the remote TensorBoard port to a local port on your machine.\n",
    "For example, to setup a tunnel that binds the TensorBoard port `6006` to port\n",
    "`6006` on your local machine, you would execute the following (on your local\n",
    "machine).\n",
    "\n",
    "```\n",
    "ssh -L 6006:localhost:6006 <your-username>@<your-server>\n",
    "```\n",
    "\n",
    "In this example, navigating with your web browser to `localhost:6006` should\n",
    "yield the screen shown above.\n",
    "\n",
    "#### Exposing TensorBoard to the Network\n",
    "Alternatively, if you are on a secure network, you may start TensorBoard with\n",
    "the `--bind_all` argument, which will allow access on the port used (`6006` in\n",
    "this example) from within the machines subnet.\n",
    "\n",
    "### Automatically Handling Log Directory Cleansing\n",
    "When developing a TensorFlow program it may sometimes be desirable to clear the\n",
    "current data displayed by TensorBoard and start fresh. Below is a simple\n",
    "function to handle this clean-up for us. The function\n",
    "`create_tensorboard_log_dir` first checks that we have access to the `/tmp`\n",
    "directory (recall, for this tutorial we are using the log directory\n",
    "`/tmp/tensorboard_example_logs`), then it deletes the old log directory (if it\n",
    "exists) and (re)creates the log directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b374f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "def create_tensorboard_log_dir():\n",
    "    \"\"\"Handles deletion and creation of the TensorBoard log directory.\"\"\"\n",
    "    # Ensure we have the top level /tmp directory first.\n",
    "    path = \"/tmp\"\n",
    "    if not os.path.isdir(path):\n",
    "        raise RuntimeError(\n",
    "            f\"Unable to locate {path} directory. Are you running on a Unix-like OS?\"\n",
    "        )\n",
    "\n",
    "    # Delete the log directory, if it exists.\n",
    "    path += \"/tensorboard_example_logs\"\n",
    "    if os.path.isdir(path):\n",
    "        try:\n",
    "            shutil.rmtree(path)\n",
    "        except OSError as e:\n",
    "            print(f\"Unable to remove {e.filename} due to: {e.strerror}\")\n",
    "\n",
    "    # Create the log directory.\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError as e:\n",
    "        print(f\"Unable to create {e.filename} due to: {e.strerror}\")\n",
    "\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800ed295",
   "metadata": {},
   "source": [
    "## Logging Data with `tf.keras.callbacks.Callback`\n",
    "When using a `tf.keras.Model` or `tf.keras.Sequential` model (as we have above)\n",
    "we have access to three main APIs: `fit`, `evaluate` and `predict`. These APIs\n",
    "take an optional `callbacks` parameter, which is a list of\n",
    "`tf.keras.callbacks.Callback` instances to be executed on the occurrence of\n",
    "certain events, such as the end of a training epoch.\n",
    "\n",
    "Continuing on with the development of our example, we shall define a simple\n",
    "`tf.keras.callbacks.Callback` derived class to provide simple staggered\n",
    "execution functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77b7ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class StaggeredCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"A simple `tf.keras.callbacks.Callback` derived class that\n",
    "    provides a check to allow staggered execution every `period` batches.\n",
    "\n",
    "    Args:\n",
    "      period (int): The number of iterations that should elapse before\n",
    "      `StaggeredCallback.should_run` returns `True`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, period):\n",
    "        self._period = period\n",
    "        self._n = 1\n",
    "\n",
    "    def should_run(self):\n",
    "        self._n += 1\n",
    "        return (self._n - 1) % self._period == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5237dd5a",
   "metadata": {},
   "source": [
    "Note that the above class (`StaggeredCallback`) will not actually perform any\n",
    "actions on any events as we have not overridden any event handlers. The\n",
    "`tf.keras.callbacks.Callback` class provides the following event handlers to be\n",
    "overridden.\n",
    "\n",
    "  - `on_train_begin(self, logs=None)`\n",
    "  - `on_train_end(self, logs=None)`\n",
    "  - `on_test_begin(self, logs=None)`\n",
    "  - `on_test_end(self, logs=None)`\n",
    "  - `on_train_batch_begin(self, batch, logs=None)`\n",
    "  - `on_train_batch_end(self, batch, logs=None)`\n",
    "  - `on_epoch_begin(self, epoch, logs=None)`\n",
    "  - `on_epoch_end(self, epoch, logs=None)`\n",
    "\n",
    "### Running Evaluation at the end of an Epoch\n",
    "We shall now write a simple callback to perform a full evaluation every $n$\n",
    "training epochs. As we wish to perform an action at the end of an epoch, we\n",
    "shall override the `on_epoch_end` handler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572a8555",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluateCallback(StaggeredCallback):\n",
    "    \"\"\"Runs an evaluation pass on the provided test dataset.\n",
    "\n",
    "    Args:\n",
    "      path (str): The directory to which evaluation logs will be written.\n",
    "      model (tf.keras.Model): The Model on which to run evaluation.\n",
    "      pred_dataset (tf.Dataset): A Dataset to use for prediction.\n",
    "      period (int): The number of epochs to elapse before evaluating.\n",
    "      steps (int): The number of steps to perform evaluation in.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path, model, pred_dataset, period, steps=128):\n",
    "        super().__init__(period)\n",
    "        self._model = model\n",
    "        self._ds = pred_dataset\n",
    "        self._writer = tf.summary.create_file_writer(path)\n",
    "        self._steps = steps\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if not self.should_run():\n",
    "            return\n",
    "\n",
    "        res = self._model.evaluate(self._ds, steps=self._steps, return_dict=True)\n",
    "\n",
    "        with self._writer.as_default():\n",
    "            for k, v in res.items():\n",
    "                tf.summary.scalar(f\"evaluation_{k}\", v, step=self._n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d0585d",
   "metadata": {},
   "source": [
    "If we first turn our attention to `__init__`, you will see that\n",
    "`PredictionCallback` takes the following initialization arguments.\n",
    "\n",
    "  - `path`\n",
    "    - The TensorBoard log path.\n",
    "  - `model`\n",
    "    - The `tf.keras.Model` instance on which we will run prediction.\n",
    "  - `pred_dataset`\n",
    "    - The `tf.Dataset` instance on which we will run prediction.\n",
    "  - `period`\n",
    "    - The period in epochs to run a prediction pass.\n",
    "  - `steps`\n",
    "    - The number of steps in which to evaluate. Simply passed to `Model.evaluate`.\n",
    "\n",
    "The first thing to note is that `EvaluateCallback` derives from\n",
    "`StaggeredCallback` (defined above), so we initialize the superclass with the\n",
    "period at which the callback should perform an action. Thus, upon entry to\n",
    "`on_epoch_end`, a check is made to determine if further execution should be\n",
    "skipped (until reaching a number of epochs that is multiple of the `period`\n",
    "parameter\n",
    "\n",
    "The second important thing to note is the initialization of `self._writer`.\n",
    "Here we are initializing a TensorFlow summary (`tf.summary`) file writer on the\n",
    "TensorBoard log path. At this point, it is pertinent to introduce TensorFlow\n",
    "summary ops (`tf.summary`) as the functionality provided is used extensively\n",
    "when writing custom code to output data to TensorBoard. Within `tf.summary` are\n",
    "many ops that are used to log out various types of data when invoked within the\n",
    "context of a `tf.summary.SummaryWriter` (`self._writer` in our example).\n",
    "\n",
    "With this in mind, we can turn our attention to the usage of `res`, the return\n",
    "value of our call to `self._model.evaluate` (note the keyword argument\n",
    "`return_dict=True`). Within the context of `self._writer.as_default()` we\n",
    "iterate over each of the specified losses and metrics and write them out as\n",
    "scalar data (note the call to `tf.summary.scalar`). This logged data is then\n",
    "available within the TensorBoard log directory for the running TensorBoard\n",
    "instance to display.\n",
    "\n",
    "### Supported Data Types in `tf.summary`\n",
    "Within `tf.summary` there are many ops supporting a variety of data types that\n",
    "can be logged, including the following.\n",
    "\n",
    "  - `generic` (tensor summary)\n",
    "  - `scalar`\n",
    "  - `histogram`\n",
    "  - `image`\n",
    "  - `audio`\n",
    "  - `graph`\n",
    "\n",
    "### Logging Custom Image Data at the end of an Epoch\n",
    "Now that we have seen how to build a basic `tf.keras.callbacks.Callback`\n",
    "derived class to run an evaluation pass at the end of every $n^{th}$ epoch, we\n",
    "may move on to build an example of logging out image data. As we will be\n",
    "training and analyzing a Convolutional Neural Network (as defined in\n",
    "`create_model`), we will in this section be building a callback to render out\n",
    "the filters of our convolution layers. The end result for a given convolution\n",
    "layer (`conv_1` in the example image that follows) should look something like this.\n",
    "\n",
    "<div>\n",
    "<img src=\"figures/ConvFilters.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "As with our implementation of `EvaluateCallback`, `FilterRenderCallback` shall\n",
    "also derive from our `StaggeredCallback` class to allow for the limiting of\n",
    "rendering to be every $n^{th}$ epoch. The implementation of\n",
    "`FilterRenderCallback` is as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b88fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "\n",
    "\n",
    "class FilterRenderCallback(StaggeredCallback):\n",
    "    \"\"\"A `StaggeredCallback` derived class that renders the kernels of a\n",
    "    given list of convolution layers.\n",
    "\n",
    "    Args:\n",
    "      path (string): The directory to which rendered filters will be written.\n",
    "      model (tf.keras.Model): The Model from which to extract filters.\n",
    "      layer_names (list(str)): A list of layer names from which filters should\n",
    "      be extracted and rendered.\n",
    "      period (int): The number of epochs to elapse before rendering filters.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path, model, layer_names, period):\n",
    "        super().__init__(period)\n",
    "        self._model = model\n",
    "        self._layer_names = layer_names\n",
    "        self._writer = tf.summary.create_file_writer(path)\n",
    "\n",
    "    def tile_filters(self, filters, name, step):\n",
    "        # First normalize the filters.\n",
    "        fmin = tf.reduce_min(filters)\n",
    "        fmax = tf.reduce_max(filters)\n",
    "        filters_norm = (filters - fmin) / (fmax - fmin)\n",
    "\n",
    "        # Instead of providing each filter as a separate image to summary_ops.image\n",
    "        # we here collapse the leading dimensions to yield a (D, D, N) tensor,\n",
    "        # where D is the filter size and N the total number of filters.\n",
    "        shape = (\n",
    "            filters_norm.shape[0],\n",
    "            filters_norm.shape[1],\n",
    "            filters_norm.shape[2] * filters_norm.shape[3],\n",
    "        )\n",
    "        filters_collapsed = tf.reshape(filters_norm, shape)\n",
    "\n",
    "        # Split into a list of N DxD filters.\n",
    "        filters_split = tf.experimental.numpy.dsplit(\n",
    "            filters_collapsed, filters_collapsed.shape[-1]\n",
    "        )\n",
    "\n",
    "        # Find the width and height of the resulting image, not in pixels but\n",
    "        # in the number of filters to display in each dimension.\n",
    "        WH = int(math.sqrt(len(filters_split)))\n",
    "\n",
    "        # Generate each row of the output image.\n",
    "        filter_num = 0\n",
    "        rows = []\n",
    "        for _ in range(WH):\n",
    "            # Collect the filters for this row.\n",
    "            row_images = [\n",
    "                tf.squeeze(t) for t in filters_split[filter_num : filter_num + WH]\n",
    "            ]\n",
    "\n",
    "            # Generate vertical padding to be used between filters.\n",
    "            col_padding = [tf.zeros((filters.shape[0], 1))] * WH\n",
    "\n",
    "            # Interleave filters and padding.\n",
    "            row = list(itertools.chain(*zip(row_images, col_padding)))\n",
    "\n",
    "            # Stack, excluding the last padding tensor as there is no filter\n",
    "            # to the right of it.\n",
    "            rows.append(tf.experimental.numpy.hstack(row[:-1]))\n",
    "\n",
    "            filter_num += WH\n",
    "\n",
    "        # Find the width of the image.\n",
    "        W = rows[-1].shape[1]\n",
    "\n",
    "        # Generate horizontal padding.\n",
    "        row_padding = [tf.zeros((1, W))] * WH\n",
    "        padded_rows = list(itertools.chain(*zip(rows, row_padding)))\n",
    "\n",
    "        # Stack and expand dims.\n",
    "        img = tf.experimental.numpy.vstack(padded_rows[:-1])\n",
    "        img = tf.expand_dims(tf.expand_dims(img, -1), 0)\n",
    "\n",
    "        # Write out the resultant image.\n",
    "        with self._writer.as_default():\n",
    "            tf.summary.image(name, img, step=step)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if not self.should_run():\n",
    "            return\n",
    "\n",
    "        for layer_name in self._layer_names:\n",
    "            filters = self._model.get_layer(layer_name).get_weights()[0]\n",
    "            self.tile_filters(filters, layer_name, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bda4b46",
   "metadata": {},
   "source": [
    "It can be seen that the `__init__` implementation of `FilterRenderCallback` is\n",
    "rather similar to that of `EvaluateCallback`; we take a path to write data to,\n",
    "a model to extract filter weights from, a list of layer names from which these\n",
    "weights will be extracted and finally a period at which to produce a new\n",
    "rendering.\n",
    "\n",
    "Notice that the implementation of the event handling method `on_epoch_end` is\n",
    "rather lightweight. It first checks if the computation should be skipped for\n",
    "the current epoch (as per the specified period), akin to `EvaluateCallback`.\n",
    "Then, for each specified layer (the list of names passed to `__init__`) we\n",
    "extract the kernel weights for that convolutional layer and pass them with the\n",
    "layer name to the `tile_filters` method which, as the name suggests tiles the\n",
    "convolution filters to be rendered as the image you see above.\n",
    "\n",
    "Take a moment to familiarise yourself with the logic in `tile_filters` if you\n",
    "are interested, however the majority of the method is not essential to\n",
    "demonstrate the use of TensorBoard and `tf.summary.image`. Most of it is just\n",
    "to generate a pretty picture, so that we have something to log for TensorBoard\n",
    "to display.\n",
    "\n",
    "The crucial part to note is the following two lines.\n",
    "```\n",
    "with self._writer.as_default():\n",
    "  tf.summary.image(name, img, step=step)\n",
    "```\n",
    "\n",
    "Notice that the process for logging image data is near identical to the\n",
    "`tf.summary.scalar` case in `EvaluateCallback.on_epoch_end`.\n",
    "\n",
    "### Using `tf.keras.callbacks.TensorBoard`\n",
    "Now that we have seen how `tf.keras.callbacks.Callback` works and how we can\n",
    "derive from it to define our own custom callbacks, we shall have a brief look\n",
    "at `tf.keras.callbacks.TensorBoard`, which is a built in callback to allow\n",
    "monitoring of basic information such as losses, metrics, parameter\n",
    "distributions and our model graph with minimal setup required.\n",
    "\n",
    "An impressive amount of information is available to us from simply creating an\n",
    "instance of `tf.keras.callbacks.TensorBoard` and passing it in the callback\n",
    "list to a Keras model. We shall see in a later section, once we have started\n",
    "training our model the information generated by this callback.\n",
    "\n",
    "## Model Setup & Data Preparation\n",
    "For the example we shall develop in this tutorial, we wish to obtain three\n",
    "MNIST datasets; training, validation and test datasets are extracted from\n",
    "`tf.keras.datasets.mnist`. Below we define a function `create_datasets` that\n",
    "loads the Keras MNIST dataset and processes it for our purpose here. The first\n",
    "step is to split the MNIST training set into both training and validation sets,\n",
    "as only two sets are provided (train and test) whereas here we require\n",
    "training, validation and test datasets.\n",
    "\n",
    "Following this split, the data is normalized, shuffled and placed into a\n",
    "`tf.data.Dataset`. The last preprocessing step is to cast the data to\n",
    "`tf.float32` and convert the targets (class labels) to `tf.int32` one-hot\n",
    "encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe24dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def create_datasets(train_validate_split=0.8, num_to_prefetch=16):\n",
    "    \"\"\"Create datasets for training, validation and evaluation.\n",
    "\n",
    "    Args:\n",
    "      train_validate_split (float): The proportion of data to use for training\n",
    "      versus evaluation.\n",
    "      num_to_prefetch (int): The number of dataset elements that\n",
    "      should be prefetched.\n",
    "\n",
    "    Returns:\n",
    "      tuple(tf.Dataset): Training, validation and testing datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    def prepare_ds(x, y):\n",
    "        # Normalize.\n",
    "        x = np.expand_dims(x, -1) / 255.0\n",
    "\n",
    "        # Shuffle and batch.\n",
    "        ds = (\n",
    "            tf.data.Dataset.from_tensor_slices((x, y))\n",
    "            .shuffle(10000)\n",
    "            .batch(32, drop_remainder=True)\n",
    "        )\n",
    "\n",
    "        # Cast and convert targets to one-hot representation.\n",
    "        ds = ds.map(\n",
    "            lambda d, l: (tf.cast(d, tf.float32), tf.cast(tf.one_hot(l, 10), tf.int32))\n",
    "        )\n",
    "\n",
    "        return ds\n",
    "\n",
    "    # Load MNIST dataset. The training set is split into training and\n",
    "    # validation sets.\n",
    "    mnist = tf.keras.datasets.mnist\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    N = int(train_validate_split * x_train.shape[0])\n",
    "\n",
    "    # Get the datasets.\n",
    "    training_ds = prepare_ds(x_train[:N], y_train[:N])\n",
    "    validation_ds = prepare_ds(x_train[N:], y_train[N:])\n",
    "    test_ds = prepare_ds(x_test, y_test)\n",
    "\n",
    "    # Repeat (for training), prefetch and return.\n",
    "    return (\n",
    "        training_ds.repeat().prefetch(num_to_prefetch),\n",
    "        validation_ds.prefetch(num_to_prefetch),\n",
    "        test_ds.prefetch(num_to_prefetch),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dbef80",
   "metadata": {},
   "source": [
    "The datasets generated by `create_datasets` consist of $(28 \\times 28)$\n",
    "greyscale images of handwritten numerical digits and their associated value\n",
    "as a target, $[0, 1 \\dots 9]$\n",
    "\n",
    "![Alt Text](figures/MnistExamples.png \"MNIST Example Digits\")\n",
    "\n",
    "So, we see that we have a very straightforward classification problem.\n",
    "\n",
    "## Model Definition\n",
    "As MNIST classification is a reasonably trivial task, only a modest complexity\n",
    "model is required. The function that follows (`create_model`) will generate a\n",
    "`tf.keras.Sequential` instance with the following layers. Note that the IPU\n",
    "pipeline stage assignment does not occur until a later stage, it is merely\n",
    "documented here for reference.\n",
    "\n",
    "| Layer Type                 | Pipeline Stage |\n",
    "|----------------------------|:--------------:|\n",
    "| 28x28x1 Input              | N/A            |\n",
    "| 32 Filter, 3x3 Convolution | 0              |\n",
    "| 2x2 Max Pooling            | 0              |\n",
    "| 64 Filter, 3x3 Convolution | 0              |\n",
    "| 2x2 Max Pooling            | 0              |\n",
    "| 32 Filter, 3x3 Convolution | 1              |\n",
    "| 2x2 Max Pooling            | 1              |\n",
    "| Flatten                    | 1              |\n",
    "| Dropout (0.5 probability)  | 1              |\n",
    "| 10 Unit Dense with Softmax | 1              |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e493d064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\"Create a simple CNN to be split into two pipeline stages.\"\"\"\n",
    "    return tf.keras.Sequential(\n",
    "        [\n",
    "            # Input layers do not get assigned an IPU pipeline stage.\n",
    "            tf.keras.layers.Input((28, 28, 1)),\n",
    "            # Pipeline stage 0.\n",
    "            tf.keras.layers.Conv2D(\n",
    "                32, kernel_size=(3, 3), activation=\"relu\", name=\"conv_0\"\n",
    "            ),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=(2, 2), name=\"pool_0\"),\n",
    "            tf.keras.layers.Conv2D(\n",
    "                64, kernel_size=(3, 3), activation=\"relu\", name=\"conv_1\"\n",
    "            ),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=(2, 2), name=\"pool_1\"),\n",
    "            # Pipeline stage 1.\n",
    "            tf.keras.layers.Conv2D(\n",
    "                32, kernel_size=(3, 3), activation=\"relu\", name=\"conv_2\"\n",
    "            ),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=(2, 2), name=\"pool_2\"),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df0b38f",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "We shall now proceed to build and train our model so that we have some data to\n",
    "view in TensorBoard. As a recap, we have so far implemented the following\n",
    "\"ingredients\" of our example TensorFlow program.\n",
    "\n",
    "  - `create_datasets`\n",
    "    - A function to load the MNIST dataset and provide us with three splits of\n",
    "    data for training, validation and testing.\n",
    "  - `create_model`\n",
    "    - A function to provide us with our Convolutional Neural Network with a 10\n",
    "    class classifier (an instance of `tf.keras.Sequential`.\n",
    "  - `create_tensorboard_log_dir`\n",
    "    - A utility function to delete any existing data directory and create a\n",
    "    clean directory to which TensorFlow will log data for TensorBoard to parse\n",
    "    and display.\n",
    "  - `StaggeredCallback`\n",
    "    - A simple child class of `tf.keras.callbacks.Callback` with state and\n",
    "    `should_run` method to allow child classes to perform staggered execution\n",
    "    of their event handlers.\n",
    "  - `EvaluateCallback`\n",
    "    - A child class of `StaggeredCallback` to invoke\n",
    "    `tf.keras.Sequential.evaluate` on our model instance every n completed\n",
    "    epochs, writing out losses and metrics to the data log directory.\n",
    "  - `FilterRenderCallback`\n",
    "    - A child class of `StaggeredCallback` to render out the convolutional\n",
    "    filters of a specified list of layer names every n completed epochs,\n",
    "    logging the images to the data log directory.\n",
    "\n",
    "The following code completes our example and performs the following tasks.\n",
    "  - Create a TensorBoard log directory.\n",
    "  - Configure IPU devices.\n",
    "  - Create a model.\n",
    "  - Create the datasets.\n",
    "  - Define a list of metrics.\n",
    "  - Compile the model.\n",
    "  - Setup pipelining options.\n",
    "  - Setup callbacks (note the use of our `FilterRenderCallback` and\n",
    "  `EvaluateCallback` classes).\n",
    "  - Start the training session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b424b0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python import ipu\n",
    "\n",
    "# Setup log path.\n",
    "log_path = create_tensorboard_log_dir()\n",
    "\n",
    "# Configure the IPU device.\n",
    "config = ipu.config.IPUConfig()\n",
    "config.auto_select_ipus = 2\n",
    "config.configure_ipu_system()\n",
    "\n",
    "# Create a strategy for execution on the IPU.\n",
    "strategy = ipu.ipu_strategy.IPUStrategy()\n",
    "with strategy.scope():\n",
    "    # Create a Keras model inside the strategy.\n",
    "    m = create_model()\n",
    "    training_dataset, validation_dataset, test_dataset = create_datasets()\n",
    "\n",
    "    # Metrics.\n",
    "    metrics = [\n",
    "        \"accuracy\",\n",
    "        tf.keras.metrics.AUC(),\n",
    "        tf.keras.metrics.TopKCategoricalAccuracy(3),\n",
    "    ]\n",
    "\n",
    "    # Compile the model for training.\n",
    "    m.compile(\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "        optimizer=tf.keras.optimizers.RMSprop(),\n",
    "        metrics=metrics,\n",
    "        steps_per_execution=128,\n",
    "    )\n",
    "\n",
    "    # Setup pipelining.\n",
    "    m.set_pipelining_options(gradient_accumulation_steps_per_replica=16)\n",
    "    m.set_pipeline_stage_assignment([0] * 4 + [1] * 5)\n",
    "\n",
    "    # Trigger callbacks every step (default behaviour is every steps_per_epoch steps).\n",
    "    m.set_asynchronous_callbacks(True)\n",
    "\n",
    "    # Create callbacks for the training session.\n",
    "    callbacks = [\n",
    "        FilterRenderCallback(log_path, m, [f\"conv_{n}\" for n in [0, 1, 2]], 2),\n",
    "        EvaluateCallback(log_path, m, test_dataset, 5),\n",
    "        tf.keras.callbacks.TensorBoard(log_path, histogram_freq=1),\n",
    "    ]\n",
    "\n",
    "    # Train.\n",
    "    m.fit(\n",
    "        training_dataset,\n",
    "        epochs=50,\n",
    "        steps_per_epoch=128,\n",
    "        validation_data=validation_dataset,\n",
    "        validation_freq=2,\n",
    "        validation_steps=128,\n",
    "        callbacks=callbacks,\n",
    "        verbose=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50829845",
   "metadata": {},
   "source": [
    "All of the above setup and training code is standard IPU TensorFlow operation\n",
    "and should be familiar to any user of\n",
    "[IPU TensorFlow](https://docs.graphcore.ai/projects/tensorflow-user-guide/en/3.1.0/tensorflow/intro.html).\n",
    "However, special attention should be paid to the\n",
    "`set_asynchronous_callbacks(True)` call on our model `m`. This call allows the\n",
    "model to asynchronously trigger callbacks at the end of an epoch, versus\n",
    "queueing up callbacks to be executed at the end of the models execution.\n",
    "\n",
    "## Exploring TensorBoard\n",
    "At this point, we should have a wealth of information available to explore in\n",
    "TensorBoard. So, go back to the previously empty TensorBoard dashboard we saw\n",
    "at the beginning of the tutorial and refresh the page. You will notice that the\n",
    "top navigation bar has changed and appears as follows.\n",
    "\n",
    "![Alt Text](figures/NavBar.png \"TensorBoard Navigation Bar\")\n",
    "\n",
    "We shall now look at the information available to us on each of the categories\n",
    "visible on the left of the navigation bar.\n",
    "\n",
    "### Scalars\n",
    "The first and most immediately pertinent to training page we shall look at is\n",
    "that for scalar data. Scalar data when written with a `step` provided is\n",
    "plotted as a 2D graph (with `step` on the x axis). You should see a plot\n",
    "similar to the following at the top of the page.\n",
    "\n",
    "![Alt Text](figures/epoch_accuracy.png \"Training Accuracy Plot\")\n",
    "\n",
    "This is the training accuracy plot. Recall that we specified accuracy, AUC and\n",
    "top-k ($k=3$) metrics for our model. There are two traces visible on the plot.\n",
    "One is for training and the other for validation, recalling again that we\n",
    "specified validation data in our call to `fit`. If you hover the cursor over\n",
    "this plot, you will notice that the plot is interactive. Take a moment to play\n",
    "with this.\n",
    "\n",
    "You should also see the following training plots on this page.\n",
    "\n",
    "  - epoch_auc\n",
    "  - epoch_loss\n",
    "  - epoch_top_k_categorical_accuracy\n",
    "\n",
    "The training plots that you see on this page are the result of providing an\n",
    "instance of `tf.keras.callbacks.TensorBoard` in our list of callbacks to `fit`.\n",
    "The data logged for TensorBoard to generate these plots has been done so\n",
    "automatically by this rather handy callback.\n",
    "\n",
    "If we turn our attention to the remaining plots on this page, you should see\n",
    "something like the following.\n",
    "\n",
    "![Alt Text](figures/eval_accuracy.png \"Evaluation Accuracy Plot\")\n",
    "\n",
    "In addition to evaluation_accuracy, you should see the following additional\n",
    "plots.\n",
    "\n",
    "  - evaluation_auc\n",
    "  - evaluation_loss\n",
    "  - evaluation_top_k_categorical_accuracy\n",
    "\n",
    "Unlike the training plots that were generated on data logged by our\n",
    "`tf.keras.callbacks.TensorBoard` instance, these evaluation plots have been\n",
    "generated by our custom `EvaluationCallback` class that we implemented earlier.\n",
    "The first thing to notice about these plots is that they are far less smooth\n",
    "than their training counterparts. This is not a bug! Recall that our custom\n",
    "callbacks use staggered execution, so we are running an evaluation pass every\n",
    "$n$ epochs, where in our case $n=5$ for our `EvaluationCallback` instance.\n",
    "\n",
    "*Perhaps as an exercise, you could play with different frequencies to find an optimum information quality/performance trade-off?*\n",
    "\n",
    "*If performance isn't a concern, perhaps the callback could be altered to compute a running mean over $n$ epochs? If not, take a moment to play with the smoothing coefficient in TensorBoard (slider to the left of the screen).*\n",
    "\n",
    "### Images\n",
    "We shall now turn our attention to the Images page in TensorBoard. When using\n",
    "only the `tf.keras.callbacks.TensorBoard` callback, as will suffice in many\n",
    "scenarios, there will be no images written out, so this page will not be\n",
    "visible. However, recall that we have written our own callback\n",
    "(`FilterRenderCallback`) to output images of our convolution layers filters.\n",
    "There are many situations where one might wish to output image data to\n",
    "TensorBoard during training, particularly in Computer Vision applications.\n",
    "Semantic Segmentation for example would be one such application; one could\n",
    "write out the original image with it's segmentation masks applied.\n",
    "\n",
    "If we look at the Images page now, you should see an image similar to the\n",
    "following.\n",
    "\n",
    "![Alt Text](figures/conv_0.png \"Tiled Convolution Filters for conv_0 Layer\")\n",
    "\n",
    "Recall that every $n$ epochs (where $n=2$ in this case), the filters of our\n",
    "three convolutional layers (`conv_0`, `conv_1` and `conv_2`) are tiled and\n",
    "rendered out by our `FilterRenderCallback` instance. As such, you should see\n",
    "the following additional images on this page.\n",
    "\n",
    "  - conv_1\n",
    "  - conv_2\n",
    "\n",
    "*Take a minute to play with the brightness and contrast settings on the left of the screen. Does it help you to identify any kind of structure in the weights?*\n",
    "\n",
    "### Graphs\n",
    "The `tf.keras.callbacks.TensorBoard` callback provides a very useful data\n",
    "output; the model graph. On this page, you will see some computation modules.\n",
    "The Graphs page should look something like the following.\n",
    "\n",
    "![Alt Text](figures/model_graph_page.png \"Graphs Page\")\n",
    "\n",
    "The Graphs page in TensorBoard provides an interactive graph explorer, in which\n",
    "modules and nodes can be expanded and collapsed, so it is very convenient to\n",
    "\"drill down\" into our model. For example, if we expand one of the pipeline\n",
    "stage modules (stage 1 in this case), you should see something like the\n",
    "following. Note that you can pan across the graph. Also note the view frustum\n",
    "indicator in the lower right hand corner of the screen.\n",
    "\n",
    "![Alt Text](figures/model_graph_pipeline_expanded.png \"Pipeline Stage 1\")\n",
    "\n",
    "*As an exercise, can you identify the control dependencies that `conv_2` has in the graph, using the graph exploration interface?*\n",
    "\n",
    "*Perhaps it would be useful to take a few minutes to familiarise yourself with the Graphs page and it's controls.*\n",
    "\n",
    "### Distributions and Histograms\n",
    "#### Distributions\n",
    "The Distributions page in TensorBoard is another useful source of information\n",
    "for inspecting your model. As with the model graph, the data displayed on the\n",
    "Distributions page is generated by the `tf.keras.callbacks.TensorBoard`\n",
    "callback we provided to our model. You should see two plots that look something\n",
    "like the following.\n",
    "\n",
    "![Alt Text](figures/conv_0_dists.png \"conv_0 Distributions\")\n",
    "\n",
    "The plots provided on the Distributions page provide statistical insight into\n",
    "your models parameters. In the above example, we see how the distribution of\n",
    "the weights and biases for our `conv_0` layer changes over time. Each line in\n",
    "the plot represents a percentile. The\n",
    "[TensorBoard Documentation](https://github.com/tensorflow/tensorboard#distribution-dashboard)\n",
    "defines the percentiles as follows.\n",
    "\n",
    "| As Percentage | As Standard Deviation          |\n",
    "|---------------|--------------------------------|\n",
    "| Maximum       | Maximum                        |\n",
    "| $$ 93 \\% $$   | $$ \\mu + \\frac{3 \\sigma}{2} $$ |\n",
    "| $$ 84 \\% $$   | $$ \\mu + \\sigma $$             |\n",
    "| $$ 69 \\% $$   | $$ \\mu + \\frac{\\sigma}{2} $$   |\n",
    "| $$ 50 \\% $$   | $$ \\mu $$                      |\n",
    "| $$ 31 \\% $$   | $$ \\mu - \\frac{\\sigma}{2} $$   |\n",
    "| $$ 16 \\% $$   | $$ \\mu - \\sigma $$             |\n",
    "| $$ 7 \\% $$    | $$ \\mu - \\frac{3 \\sigma}{2} $$ |\n",
    "| Minimum       | Minimum                        |\n",
    "\n",
    "*Take a moment to see how the plots displayed correspond to the above table.*\n",
    "\n",
    "This may be easier to do if you enlarge the plots (there is a button below each\n",
    "plot). It may be worthwhile doing this for each remaining parameter in our\n",
    "model, there should be plots for each remaining layer in our model.\n",
    "\n",
    "  - conv_1\n",
    "  - conv_2\n",
    "  - dense\n",
    "\n",
    "#### Histograms\n",
    "The Histograms page provides an alternative view of the data presented in the\n",
    "Distributions page. It is the same data, just presented differently. For\n",
    "example, you should see two plots akin to the following, for our `conv_0`\n",
    "layer.\n",
    "\n",
    "![Alt Text](figures/conv_0_histograms.png \"conv_0 Histograms\")\n",
    "\n",
    "As with the plots on the Distributions page, the plots on the Histograms page\n",
    "allow us to inspect the statistical properties of our models parameters. You\n",
    "will see that there are multiple histograms stacked for each parameter,\n",
    "indexed by time-step. Similarly to the Distributions page, we can also here see\n",
    "how our model parameters have evolved over time. Hover the cursor over one of\n",
    "the plots to see the outline for a given time-step.\n",
    "\n",
    "*Perhaps it would be useful to compare the data presented in these plots with those in the Distributions page? Do they correspond as one might expect?*\n",
    "\n",
    "## Time Series\n",
    "The final page in TensorBoard that we will cover in this tutorial is the Time\n",
    "Series page. On this page you will see no new information or plots, however you\n",
    "will find all of the plots we have seen already in one place with controls to\n",
    "filter and move through time-steps. For example.\n",
    "\n",
    "![Alt Text](figures/TimeSeriesPage.png \"Time Series Page\")\n",
    "\n",
    "## Using TensorBoard Without Keras\n",
    "Although in this tutorial we have focused on the use of TensorBoard with Keras,\n",
    "it is entirely possible to log data out to TensorBoard without using Keras and\n",
    "`tf.keras.callbacks.Callback` instances. It just may involve some extra\n",
    "implementation as we won't have useful tools like\n",
    "`tf.keras.callbacks.TensorBoard`. However, as we have seen, the actual logging\n",
    "of data is achieved by the use of ops within `tf.summary`.\n",
    "\n",
    "*As an exercise, try to de-Keras-ify this model whilst keeping the same level of TensorBoard insight.*\n",
    "\n",
    "## To Conclude\n",
    "In conclusion, we have looked at TensorBoard and how it may be used with IPU\n",
    "TensorFlow. We have covered primarily the case in which we are using Keras,\n",
    "including how to write our own custom data logging callbacks via TensorFlow's\n",
    "summary ops (`tf.summary`). We have also seen the wealth of information\n",
    "available to us by simply logging data using a built in callback\n",
    "(`tf.keras.callbacks.TensorBoard`). Finally, we took a brief tour of the\n",
    "TensorBoard interface and how to interpret some of the data presented to us\n",
    "from the MNIST classifier we built for our experimentation."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
