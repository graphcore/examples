{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92383abb",
   "metadata": {},
   "source": [
    "Copyright (c) 2021 Graphcore Ltd. All rights reserved."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ef05180",
   "metadata": {},
   "source": [
    "*Notebook autogenerated from walkthrough.py on 14-Nov-2022*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a113c9a",
   "metadata": {},
   "source": [
    "# Efficient data loading with PopTorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a770585d",
   "metadata": {},
   "source": [
    "This tutorial will present how PopTorch can help to efficiently load data to\n",
    "your model and how to avoid common performance bottlenecks when passing data\n",
    "from the host to the IPU. It also covers the more general notion of data\n",
    "batching on IPUs which is also relevant to other frameworks.\n",
    "\n",
    "Before starting this tutorial, we recommend that you read through our\n",
    "tutorial on the basics of PyTorch on the IPU `../basics` and our\n",
    "[MNIST starting tutorial](https://github.com/graphcore/examples/tree/master/tutorials/simple_applications/pytorch/mnist)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d261a5c",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "\n",
    "The best way to run this demo is on Paperspace Gradientâ€™s cloud IPUs because everything is already set up for you. To improve your experience we preload datasets and pre-install packages. This can take a few minutes. If you experience errors immediately after starting a session please try restarting the kernel before contacting support. If a problem persists or you want to give us feedback on the content of this notebook, please reach out to through our community of developers using our [slack channel](https://www.graphcore.ai/join-community) or raise a [GitHub issue](https://github.com/graphcore/Gradient-PyTorch/issues).\n",
    "\n",
    "\n",
    "To run the demo using other IPU hardware, you need to have the Poplar SDK enabled. Refer to the [Getting Started guide](https://docs.graphcore.ai/en/latest/getting-started.html#getting-started) for your system for details on how to enable the Poplar SDK. Also refer to the [Jupyter Quick Start guide](https://docs.graphcore.ai/projects/jupyter-notebook-quick-start/en/latest/index.html) for how to set up Jupyter to be able to run this notebook on a remote IPU machine."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f90f6549",
   "metadata": {},
   "source": [
    "## PyTorch and PopTorch DataLoader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ddf34f82",
   "metadata": {},
   "source": [
    "If you are familiar with PyTorch you may have used\n",
    "[torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader).\n",
    "\n",
    "PopTorch provides [its own\n",
    "DataLoader](https://docs.graphcore.ai/projects/poptorch-user-guide/en/latest/batching.html#poptorch-dataloader)\n",
    "which is a wrapper around `torch.utils.data.DataLoader`. It accepts the same\n",
    "arguments as PyTorch's DataLoader with some extra features specific to the IPU:\n",
    "\n",
    "- It takes a `poptorch.Options` instance to use IPU-specific features;\n",
    "- It automatically computes the number of elements consumed by\n",
    "  [one step](#how-many-samples-will-then-be-loaded-in-one-step);\n",
    "- It enables asynchronous data loading.\n",
    "\n",
    "See the\n",
    "[documentation](https://docs.graphcore.ai/projects/poptorch-user-guide/en/latest/batching.html#poptorch-asynchronousdataaccessor)\n",
    "for more information about asynchronous mode."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "589a9dee",
   "metadata": {},
   "source": [
    "> **Note**: When executing code from this tutorial in a python script, it\n",
    "> requires this conditional block:\n",
    ">\n",
    "> ```python\n",
    "> if __name__ == '__main__':\n",
    "> ```\n",
    ">\n",
    "> This is necessary to avoid [issues with asynchronous\n",
    "> DataLoader](https://docs.graphcore.ai/projects/poptorch-user-guide/en/latest/batching.html#poptorch-asynchronousdataaccessor).\n",
    "> The asynchronous dataloader calls the spawn method, which creates a new\n",
    "> python interpreter. This interpreter will import the main module of the\n",
    "> application. Therefore, we need protection against infinite spawning of new\n",
    "> processes and repeated, undesirable code invocations and so the entire\n",
    "> executable part of the script should be in an `if` block. Function and class\n",
    "> definitions do not have to be in this block. This change does not apply to\n",
    "> interactive python Interpreters (for example Jupyter notebooks) which support\n",
    "> multiprocessing in a different way.\n",
    "\n",
    "<!-- separate blocks to prevent them being rendered together -->\n",
    "\n",
    "> **Note**: The dataset must be serializable by pickle."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8cb6ec9e",
   "metadata": {},
   "source": [
    "Let's reuse the model from [the introductory tutorial on\n",
    "PopTorch](../basics)\n",
    "and make a random dataset to experiment with the different IPU parameters.\n",
    "\n",
    "We will start by importing the necessary libraries:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47cba9ac",
   "metadata": {},
   "source": [
    "\n",
    "In order to improve usability and support for future users, Graphcore would like to collect information about the\n",
    "applications and code being run in this notebook. The following information will be anonymised before being sent to Graphcore:\n",
    "\n",
    "- User progression through the notebook\n",
    "- Notebook details: number of cells, code being run and the output of the cells\n",
    "- Environment details\n",
    "\n",
    "You can disable logging at any time by running `%unload_ext graphcore_cloud_tools.notebook_logging.gc_logger` from any cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407a5862",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"graphcore-cloud-tools[logger] @ git+https://github.com/graphcore/graphcore-cloud-tools\"\n",
    "%load_ext graphcore_cloud_tools.notebook_logging.gc_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20786f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sys import exit\n",
    "\n",
    "import poptorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "\n",
    "executable_cache_dir = os.getenv(\"POPLAR_EXECUTABLE_CACHE_DIR\", \"/tmp/exe_cache/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1ea9c6c5",
   "metadata": {},
   "source": [
    "Now we will define some global variables that are used later. If you change\n",
    "any of these values then you should re-run all the cells below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b80ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_iterations = 50\n",
    "batch_size = 16\n",
    "replicas = 1\n",
    "num_workers = 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56e6ad56",
   "metadata": {},
   "source": [
    "Let's create the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffd9393",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 5, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(5, 12, 5)\n",
    "        self.norm = nn.GroupNorm(3, 12)\n",
    "        self.fc1 = nn.Linear(41772, 100)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=0)\n",
    "        self.loss = nn.NLLLoss()\n",
    "\n",
    "    def forward(self, x, labels=None):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.norm(self.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.log_softmax(self.fc2(x))\n",
    "        if self.training:\n",
    "            return x, self.loss(x, labels)\n",
    "        return x\n",
    "\n",
    "\n",
    "opts = poptorch.Options()\n",
    "opts.deviceIterations(device_iterations)\n",
    "opts.replicationFactor(replicas)\n",
    "\n",
    "model = ClassificationModel()\n",
    "model.train()  # Switch the model to training mode\n",
    "# Models are initialised in training mode by default, so the line above will\n",
    "# have no effect. Its purpose is to show how the mode can be set explicitly.\n",
    "\n",
    "training_model = poptorch.trainingModel(\n",
    "    model, opts, torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ec96942",
   "metadata": {},
   "source": [
    "Now we will create a sample random dataset, which we will later use to\n",
    "calculate processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea69a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = torch.randn([10000, 1, 128, 128])\n",
    "labels = torch.empty([10000], dtype=torch.long).random_(10)\n",
    "dataset = torch.utils.data.TensorDataset(features, labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78e99767",
   "metadata": {},
   "source": [
    "In the [tutorial on PyTorch basics](https://github.com/graphcore/examples/tree/master/tutorials/tutorials/pytorch/basics) we used images from the MNIST dataset\n",
    "with a size of 28x28, now we will use larger images (128x128) to simulate a\n",
    "heavier data load. This change increases the input size of the layer `fc1` from\n",
    "`self.fc1 = nn.Linear(972, 100)` to `self.fc1 = nn.Linear(41772, 100)`.\n",
    "\n",
    "Let's set up a PopTorch DataLoader in asynchronous mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a64e1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = poptorch.DataLoader(\n",
    "    opts,\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers,\n",
    "    mode=poptorch.DataLoaderMode.Async,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2fd4dad5",
   "metadata": {},
   "source": [
    "The asynchronous mode of `poptorch.DataLoader` performs the data loading\n",
    "on separate processes. This allows the data to be preprocessed asynchronously\n",
    "on the CPU to minimize CPU/IPU transfer time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "08f016aa",
   "metadata": {},
   "source": [
    "## Understanding batching with IPU"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10c11d5f",
   "metadata": {},
   "source": [
    "When developing a model for the IPU, you will encounter different notions of\n",
    "batching including mini-batches, replica batches and global batches. This\n",
    "section will explain how these hyperparameters are related to the IPU and how\n",
    "to compute the number of samples the DataLoader is going to fetch for one step."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e27dc2f",
   "metadata": {},
   "source": [
    "### Device iterations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b73dac42",
   "metadata": {},
   "source": [
    "This diagram represents a basic execution on 1 IPU with\n",
    "***n*** device iterations and 1 mini-batch used per iteration.\n",
    "\n",
    "![Device Iterations](static/iterations.png)\n",
    "\n",
    "A device iteration corresponds to one iteration of the training loop executed\n",
    "on the IPU, starting with data-loading and ending with a weight update.\n",
    "In this simple case, when we set ***n*** `deviceIterations`, the host will\n",
    "prepare ***n*** mini-batches in an infeed queue so the IPU can perform\n",
    "efficiently ***n*** iterations.\n",
    "\n",
    "From the host point of view, this will correspond to a single call to the\n",
    "model (1 step):\n",
    "\n",
    "> ```python\n",
    "> training_model(data, labels)\n",
    "> ```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8f0eece",
   "metadata": {},
   "source": [
    "#### A note on returned data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c70b149",
   "metadata": {},
   "source": [
    "The number of batches of data returned to the host depends on the option\n",
    "`poptorch.Options.outputMode`. It defaults to `Final` for `trainingModel` and\n",
    "`All` for `inferenceModel`. This is because you will usually want to receive\n",
    "all the output tensors when you use a `inferenceModel()` while you will often\n",
    "not need to receive all or any of the output tensors when you use a\n",
    "`trainingModel`. See the\n",
    "[documentation](https://docs.graphcore.ai/projects/poptorch-user-guide/en/latest/reference.html#poptorch.Options.outputMode)\n",
    "for more information about `poptorch.Options.outputMode`.\n",
    "\n",
    "In this case presented above, we are using a `trainingModel` and\n",
    "`poptorch.Options.outputMode` is therefore the default value `Final`. Since\n",
    "`poptorch.Options.replicationFactor` defaults to 1, the number of data elements\n",
    "returned to the host will just be 16, which is the batch size. The tensor shape\n",
    "will then be (16, 1, 128, 128)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d5d1f79",
   "metadata": {},
   "source": [
    "### Gradient accumulation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "76d9e030",
   "metadata": {},
   "source": [
    "This parameter must be used with pipelining. A pipelined model consists in\n",
    "splitting the graph into different successive computation stages. Every stage\n",
    "of a pipelined model can be placed on a different IPU, they all compute\n",
    "specific parts of the graph with their own weights. Each stage will compute the\n",
    "forward and backward pass of a mini-batch.\n",
    "\n",
    "In the image below, we can see a 4 stage pipeline where 8 mini-batches\n",
    "(`B1`-`B8`) are being processed:\n",
    "\n",
    "- An `f` suffix indicates a forward pass;\n",
    "- A `b` suffix indicates a backward pass.\n",
    "\n",
    "![Gradient accumulation](static/pipeline.png)\n",
    "\n",
    "When we set up a pipelined execution, we overcome the cost of having multiple\n",
    "stages by computing several batches in parallel when the pipeline is full."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82524ebf",
   "metadata": {},
   "source": [
    "Every pipeline stage needs to update its weights when the gradients are ready.\n",
    "However, it would be very inefficient to update them after each mini-batch\n",
    "completion. The solution is the following:\n",
    "After each backward pass the gradients are accumulated together for ***K***\n",
    "mini-batches. Then, the accumulators are used to update the stage weights.\n",
    "This is gradient accumulation. This value can be set in PopTorch via the\n",
    "option `opts.Training.gradientAccumulation(K)`\n",
    "\n",
    "In the previous part, we only had 1 mini-batch per weight update. This time we\n",
    "have ***K*** mini-batches per weight update.\n",
    "\n",
    "Then, for one device iteration with pipeline we have multiplied the number of\n",
    "samples processed by ***K***.\n",
    "\n",
    "More information about gradient accumulation can be found [in the PopTorch User\n",
    "Guide](https://docs.graphcore.ai/projects/poptorch-user-guide/en/latest/batching.html#poptorch-options-training-gradientaccumulation)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23db9870",
   "metadata": {},
   "source": [
    "### Replication"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04fb63ba",
   "metadata": {},
   "source": [
    "![Replication](static/replica.png)\n",
    "\n",
    "Replication describes the process of running multiple instances of the same\n",
    "model simultaneously on different IPUs to achieve data parallelism. This can\n",
    "give large improvements in throughput, as the training data is processed in\n",
    "parallel.\n",
    "\n",
    "In a single device iteration, many mini-batches may be processed and the\n",
    "resulting gradients accumulated. We call this total number of samples processed\n",
    "for one optimiser step the **global batch size**.\n",
    "\n",
    "If the model requires ***N*** IPUs and the replication factor is ***M***,\n",
    "***N*** x ***M*** IPUs will be necessary, but for one device iteration we have\n",
    "increased by ***M*** the number of mini-batches processed.\n",
    "\n",
    "The PopTorch Dataloader will ensure that the host is sending each replica a\n",
    "different subset of the dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69b1e56d",
   "metadata": {},
   "source": [
    "### Global batch size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3e83d85",
   "metadata": {},
   "source": [
    "Because several mini-batches can be processed by one device iteration (that is,\n",
    "for one weight update), we call **global batch size** this total number of\n",
    "samples:\n",
    "\n",
    "- Mini-batch size (sometimes called micro-batch)\n",
    "\n",
    "  - The number of samples processed by one simple fwd/bwd pass.\n",
    "\n",
    "- Replica batch size\n",
    "\n",
    "  - The number of samples on a single replica process before weight update\n",
    "    Replica batch size = *Mini-batch size* x Gradient accumulation factor\n",
    "\n",
    "- Global batch size\n",
    "\n",
    "  - The number of samples used for the weight update\n",
    "    Global batch size  = *Replica batch size* x Number of replicas\n",
    "    Global batch size  = (*Mini-batch size* x Gradient accumulation factor) x Number of replicas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61c539d8",
   "metadata": {},
   "source": [
    "#### How many samples will then be loaded in one step?\n",
    "\n",
    "Considering you are iterating through the PopTorch DataLoader:\n",
    "\n",
    "```python\n",
    "for data, labels in training_data:\n",
    "    training_model(data, labels)\n",
    "```\n",
    "\n",
    "For each step, the number of samples contained in `data` and `labels` will be:\n",
    "***N = Global batch size x Device iterations***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "32278d49",
   "metadata": {},
   "source": [
    "## Tuning hyperparameters\n",
    "\n",
    "### Evaluating the asynchronous DataLoader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0563d023",
   "metadata": {},
   "source": [
    "How can we make sure the DataLoader is not a bottleneck for our model\n",
    "throughput? In this tutorial we made an example benchmark to answer this\n",
    "question:\n",
    "\n",
    "1. As we will often measure time we will prepare a context manager which will\n",
    "   capture the elapsed time of a code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267550e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class catchtime:\n",
    "    def __enter__(self):\n",
    "        self.seconds = time.time()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        self.seconds = time.time() - self.seconds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4da786e",
   "metadata": {},
   "source": [
    "2. Evaluate the asynchronous DataLoader throughput without the IPU.\n",
    "\n",
    "   We just loop through the DataLoader without running the model so we can\n",
    "   estimate its maximum throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba921f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = len(training_data)\n",
    "with catchtime() as t:\n",
    "    for i, (data, labels) in enumerate(training_data):\n",
    "        a, b = data, labels\n",
    "\n",
    "print(f\"Total execution time: {t.seconds:.2f} s\")\n",
    "items_per_second = (steps * device_iterations * batch_size * replicas) / t.seconds\n",
    "print(f\"DataLoader throughput: {items_per_second:.2f} items/s\")\n",
    "\n",
    "training_data.terminate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "562dbccd",
   "metadata": {},
   "source": [
    "   > ***Note about releasing resources***:\n",
    "   >\n",
    "   > In the Jupyter environment, we have to manually detach from IPU devices\n",
    "   > and terminate worker threads of the asynchronous data loader. Workers\n",
    "   > must be manually terminated because we use the asynchronous data loader\n",
    "   > (`DataLoaderMode.Async`) with a number of data samples not exactly\n",
    "   > divisible by the total of batch size multiplied by device count. This\n",
    "   > mismatch leaves some workers waiting to provide data which might not be\n",
    "   > needed with the training epoch ending before all samples are exhausted.\n",
    "   > In a production model, running from Python, this should not be necessary.\n",
    "\n",
    "3. Evaluate the IPU throughput with synthetic data. To do so we will evaluate\n",
    "   the model with synthetic data generated by the IPU. Create a new\n",
    "   `poptorch.Options()` structure, this time specifying\n",
    "   `enableSyntheticData(True)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0b1ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = poptorch.Options()\n",
    "opts.deviceIterations(device_iterations)\n",
    "opts.replicationFactor(replicas)\n",
    "opts.enableSyntheticData(True)\n",
    "opts.enableExecutableCaching(executable_cache_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5f7b2e1",
   "metadata": {},
   "source": [
    "When using synthetic data, no data is copied onto the device. Hence, the\n",
    "throughput measured will be the upper bound of IPU/model performance.\n",
    "\n",
    "Provided that the asynchronous DataLoader throughput is greater or equal than\n",
    "this upper bound, the host-side data loading will not be a bottleneck. Else,\n",
    "there is a risk that the DataLoader throughput is limiting performance of the\n",
    "model.\n",
    "\n",
    "Note that this is only true if you're using an asynchronous DataLoader, the\n",
    "synchronous one can still slow down the overall execution as it will be run\n",
    "serially.\n",
    "\n",
    "> ***Note for IPU benchmarking***:\n",
    ">\n",
    "> The warmup time can be avoided by calling `training_model.compile(data,\n",
    "> labels)` before any other call to the model. If not, the first call will\n",
    "> include the compilation time, which can take few minutes.\n",
    ">\n",
    "> ```python\n",
    "> # Warmup\n",
    "> print(\"Compiling + Warmup ...\")\n",
    "> training_model.compile(data, labels)\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e01fe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassificationModel()\n",
    "training_model = poptorch.trainingModel(\n",
    "    model,\n",
    "    opts,\n",
    "    poptorch.optim.SGD(\n",
    "        model.parameters(), lr=0.001, momentum=0.9, use_combined_accum=False\n",
    "    ),\n",
    ")\n",
    "training_data = poptorch.DataLoader(\n",
    "    opts,\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers,\n",
    "    mode=poptorch.DataLoaderMode.Async,\n",
    "    async_options={\"early_preload\": True},\n",
    ")\n",
    "steps = len(training_data)\n",
    "data_batch, labels_batch = next(iter(training_data))\n",
    "training_model.compile(data_batch, labels_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b357da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Evaluating: {steps} steps of {device_iterations * batch_size * replicas} items\")\n",
    "\n",
    "# With synthetic data enabled, no data is copied from the host to the IPU,\n",
    "# so we don't use the dataloader, to prevent influencing the execution\n",
    "# time and therefore the IPU throughput calculation\n",
    "with catchtime() as t:\n",
    "    for _ in range(steps):\n",
    "        training_model(data_batch, labels_batch)\n",
    "\n",
    "items_per_second = (steps * device_iterations * batch_size * replicas) / t.seconds\n",
    "print(f\"Total execution time: {t.seconds:.2f} s\")\n",
    "print(f\"IPU throughput: {items_per_second:.2f} items/s\")\n",
    "\n",
    "training_data.terminate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "32508a8f",
   "metadata": {},
   "source": [
    "### What if the DataLoader throughput is too low?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "345c7ee8",
   "metadata": {},
   "source": [
    "You can:\n",
    "\n",
    "- Try using the asynchronous mode of `poptorch.DataLoader`;\n",
    "- Try to increase the global batch size or the number of device iterations;\n",
    "- Increase the number of workers;\n",
    "- If you are using the asynchronous mode to load a small number of elements\n",
    "  per step, you can try setting `miss_sleep_time_in_ms = 0` (See below)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "16f61c1c",
   "metadata": {},
   "source": [
    "Suggestions if the performance drops at the beginning of an epoch:\n",
    "\n",
    "- Re-use workers by setting the DataLoader option `persistent_workers=True`;\n",
    "- Make sure `load_indefinitely` is set to `True` (the default value);\n",
    "- If the first iteration includes a very expensive operation (like opening or\n",
    "  loading a large file) then increasing the `buffer_size` (size of the ring\n",
    "  buffer) combined with the options above might help hide it at the cost of\n",
    "  using more memory;\n",
    "- Set the option `early_preload=True`. This means the data accessor starts\n",
    "  loading tensors immediately once it's being built (whereas usually it will\n",
    "  wait for an iterator to be created: for instance, when you enter the main\n",
    "  loop).\n",
    "\n",
    "  If the DataLoader is created before the model compilation is called, the data\n",
    "  will be ready to be used when the compilation is over. The main drawback is\n",
    "  that more RAM will be used on host as the data accessor and the compilation\n",
    "  will work at the same time.\n",
    "\n",
    "  > The options `miss_sleep_time_in_ms`, `early_preload`, `load_indefinitely`\n",
    "  > and `buffer_size` are specific to the AsynchronousDataAccessor. They will\n",
    "  > need to be passed to the DataLoader via the dictionary `async_options`:\n",
    "  >\n",
    "  > ```python\n",
    "  > training_data = poptorch.DataLoader(opts, dataset=dataset, batch_size=16,\n",
    "  >                                     shuffle=True, drop_last=True,\n",
    "  >                                     num_workers=4, mode=poptorch.DataLoaderMode.Async,\n",
    "  >                                     async_options={\"early_preload\": True, \"miss_sleep_time_in_ms\": 0})\n",
    "  > ```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f0b0ea76",
   "metadata": {},
   "source": [
    "### Device iterations vs global batch size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf6193d4",
   "metadata": {},
   "source": [
    "Even if we made sure the DataLoader is not a bottleneck any more, the strategy\n",
    "we used for batching can be suboptimal. We must keep in mind that increasing\n",
    "the global batch size will improve the IPU utilisation while increasing device\n",
    "iterations will not."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07ef152c",
   "metadata": {},
   "source": [
    "#### Case of a training session"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7741648b",
   "metadata": {},
   "source": [
    "We have seen that the device can efficiently iterate while taking data prepared\n",
    "by the CPU in a queue. However, one iteration implies gradient computation and\n",
    "weight update on the device. The backward pass is computationally expensive.\n",
    "Then, for training it is recommended to prefer bigger global batch size over\n",
    "many device iterations in order to maximise parallelism."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c531ad29",
   "metadata": {},
   "source": [
    "#### Case of an inference session"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "424f2696",
   "metadata": {},
   "source": [
    "For inference only, there is no gradient computation and weights are frozen.\n",
    "In that case increasing the number of device iterations and using a smaller\n",
    "global batch-size should not harm."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ccec52bc",
   "metadata": {},
   "source": [
    "#### Conclusion: Training and inference sessions\n",
    "\n",
    "Finally, as a general recommendation these two parameters have to be tuned so\n",
    "your DataLoader can consume the whole dataset in the smallest number of steps\n",
    "without throttling.\n",
    "We can get this number of steps just by getting the length of the DataLoader\n",
    "object:\n",
    "\n",
    "```python\n",
    "steps = len(training_data)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6928b40b",
   "metadata": {},
   "source": [
    "For an IterableDataset, the whole dataset is not necessarily consumed. With the\n",
    "`drop_last` argument, elements of the dataset may be discarded. If the batch\n",
    "size does not properly divide the number of elements per worker, the last\n",
    "incomplete batches will be discarded."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca458736",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "36b6b1aa",
   "metadata": {},
   "source": [
    "We invite you to try these different sets of parameters to assess their effect.\n",
    "We included the throughput we obtained for illustration but it may vary\n",
    "depending on your configuration.\n",
    "\n",
    "We will create a function that uses the previous code and validates the\n",
    "performance of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9c0ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model_performance(\n",
    "    dataset,\n",
    "    device_iterations=50,\n",
    "    batch_size=16,\n",
    "    replicas=2,\n",
    "    num_workers=4,\n",
    "    synthetic_data=False,\n",
    "):\n",
    "    opts = poptorch.Options()\n",
    "    opts.deviceIterations(device_iterations)\n",
    "    opts.replicationFactor(replicas)\n",
    "    opts.enableExecutableCaching(executable_cache_dir)\n",
    "\n",
    "    if synthetic_data:\n",
    "        opts.enableSyntheticData(True)\n",
    "\n",
    "    model = ClassificationModel()\n",
    "    training_model = poptorch.trainingModel(\n",
    "        model,\n",
    "        opts,\n",
    "        poptorch.optim.SGD(\n",
    "            model.parameters(), lr=0.001, momentum=0.9, use_combined_accum=False\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    training_data = poptorch.DataLoader(\n",
    "        opts,\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        num_workers=num_workers,\n",
    "        mode=poptorch.DataLoaderMode.Async,\n",
    "        async_options={\"early_preload\": True},\n",
    "    )\n",
    "    steps = len(training_data)\n",
    "    data_batch, labels_batch = next(iter(training_data))\n",
    "    training_model.compile(data_batch, labels_batch)\n",
    "    with catchtime() as t:\n",
    "        for data_batch, labels_batch in training_data:\n",
    "            pass\n",
    "\n",
    "    items_per_second = (steps * device_iterations * batch_size * replicas) / t.seconds\n",
    "    print(f\"DataLoader: {items_per_second:.2f} items/s\")\n",
    "    print(f\"Dataloader execution time: {t.seconds:.2f} s\")\n",
    "\n",
    "    if synthetic_data:\n",
    "        # With synthetic data enabled, no data is copied from the host to the\n",
    "        # IPU, so we don't use the dataloader, to prevent influencing the\n",
    "        # execution time and therefore the IPU throughput calculation\n",
    "        with catchtime() as t:\n",
    "            for _ in range(steps):\n",
    "                training_model(data_batch, labels_batch)\n",
    "    else:\n",
    "        with catchtime() as t:\n",
    "            for data, labels in training_data:\n",
    "                training_model(data, labels)\n",
    "            training_model.detachFromDevice()\n",
    "\n",
    "    items_per_second = (steps * device_iterations * batch_size * replicas) / t.seconds\n",
    "    print(f\"IPU throughput: {items_per_second:.2f} items/s\")\n",
    "    print(f\"Dataloader with IPU training execution time: {t.seconds:.2f} s\")\n",
    "\n",
    "    training_data.terminate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef3315af",
   "metadata": {},
   "source": [
    "Now we are ready to conduct experiments:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52deb15e",
   "metadata": {},
   "source": [
    "### Case 1: No bottleneck\n",
    "\n",
    "- mini-batch size: 16\n",
    "- replica: 1 (no replication)\n",
    "- device iterations: 50\n",
    "- workers: 4\n",
    "\n",
    "=> Global batch size 16 with real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1912d36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_model_performance(\n",
    "    dataset,\n",
    "    batch_size=16,\n",
    "    replicas=1,\n",
    "    device_iterations=50,\n",
    "    num_workers=4,\n",
    "    synthetic_data=False,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c53d6e6",
   "metadata": {},
   "source": [
    "=> Global batch size 16 with synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374983b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_model_performance(\n",
    "    dataset,\n",
    "    batch_size=16,\n",
    "    replicas=1,\n",
    "    device_iterations=50,\n",
    "    num_workers=4,\n",
    "    synthetic_data=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3280324",
   "metadata": {},
   "source": [
    "From the tests you should be able to see that the throughput with processing\n",
    "the model is less than the capabilities of the Dataloader. This means that\n",
    "dataloader is not a bottleneck as it is able to process more data than our\n",
    "model can consume.\n",
    "\n",
    "#### Why is the throughput lower with real data?\n",
    "\n",
    "As mentioned previously, using synthetic data does not include the stream\n",
    "copies on the IPU. It also excludes the synchronisation time with the host."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd66ce0d",
   "metadata": {},
   "source": [
    "### Case 2: Larger global batch size with replication\n",
    "\n",
    "Let's try to get better training performances by increasing the global batch\n",
    "size. We can choose to increase the replication factor so it avoids loading\n",
    "more data at a time on a single IPU.\n",
    "\n",
    "- mini-batch size: 16\n",
    "- replica: 2\n",
    "- device iterations: 50\n",
    "- workers: 4\n",
    "\n",
    "=> Global batch size 32 with real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bf6dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_model_performance(\n",
    "    dataset,\n",
    "    batch_size=16,\n",
    "    replicas=2,\n",
    "    device_iterations=50,\n",
    "    num_workers=4,\n",
    "    synthetic_data=False,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59a24b85",
   "metadata": {},
   "source": [
    "=> Global batch size 32 with synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d11834a",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_model_performance(\n",
    "    dataset,\n",
    "    batch_size=16,\n",
    "    replicas=2,\n",
    "    device_iterations=50,\n",
    "    num_workers=4,\n",
    "    synthetic_data=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b9516d42",
   "metadata": {},
   "source": [
    "Throughput of dataloader for synthetic and real data should be roughly the\n",
    "same. However, given the small number of steps (3 steps) and the very short\n",
    "execution time of the application (of the order of thousandths of a second) the\n",
    "results may diverge slightly more.\n",
    "\n",
    "This example gave an idea of how increasing the global batch size can improve\n",
    "the throughput.\n",
    "\n",
    "The runtime script where you can play with the parameters can be found in the\n",
    "file: `tuto_data_loading.py`. Helpful arguments:\n",
    "\n",
    "```bash\n",
    "--synthetic-data  # Run with IPU-generated synthetic data\n",
    "--replicas # Takes an integer parameter to set the number of replicas\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "457065b1",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- To efficiently load your dataset to the IPU, the best practice is to use the\n",
    "  dedicated PopTorch DataLoader;\n",
    "- During one step, ***N = Global batch size x Device iterations*** samples will\n",
    "  be loaded;\n",
    "- A good way to know if the DataLoader is not a bottleneck is to compare its\n",
    "  throughput with the model throughput on synthetic data;\n",
    "- Asynchronous mode can provide better throughput performance.\n",
    "\n",
    "Further information on Host-IPU IO optimisation can be found in our [memory and\n",
    "performance optimisation\n",
    "guide](https://docs.graphcore.ai/projects/memory-performance-optimisation/en/latest/optimising-performance.html#host-ipu-io-optimisation)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
