{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Copyright (c) 2022 Graphcore Ltd. All rights reserved."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Notebook autogenerated from run_cluster_gcn_notebook.py on 21-Jul-2022*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Copyright 2022 The Google Research Authors.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Running Cluster GCN Step by Step"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this tutorial we show how to train the Cluster-GCN algorithm,\n",
    "presented in [Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional\n",
    "Networks](https://arxiv.org/pdf/1905.07953.pdf), on the Graphcore IPU.\n",
    "\n",
    "In this tutorial, you will learn how to train and test the model step by step, including:\n",
    "\n",
    "- Load the data.\n",
    "- Configure the IPU.\n",
    "- Create a strategy.\n",
    "- Create the model.\n",
    "- Train and test the model.\n",
    "\n",
    "[1] Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, Cho-Jui Hsieh,\n",
    "\"Cluster-GCN An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks,\"\n",
    "KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &\n",
    "Data Mining, July 2019, Pages 257â€“266, https://doi.org/10.1145/3292500.3330925"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction\n",
    "\n",
    "Cluster GCN can be considered a sampling method that allows to train on large scale graph machine learning datasets. Cluster GCN works on two steps. First, it clusters the data, such that each cluster is a subgraph. Second, the algorithm trains the model using a stochastic gradient estimated with a batch of sampled subgraphs.\n",
    "\n",
    "By default, we will train with the [arXiv dataset](https://ogb.stanford.edu/docs/nodeprop/#ogbn-arxiv), which is a directed homogeneous graph encoding a citation network between all Computer Science (CS) papers hosted in arXiv and indexed by the Microsoft academic graph (MAG). Each paper has a 128-dimmensional node feature vector, that encodes the title and abstract, processed with a skip-gram model. Each directed link in the graph indicates that one paper cites another. The task is to predict the correct topic label for the paper from the 40 main categories. The train portion of the dataset is all papers published until 2017, the papers published in 2018 are the validation set, and papers published in 2019 are the test set. We wukk use the arXiv dataset, simply use the train_arxiv.json config, the dataset will be downloaded automatically.\n",
    "\n",
    "We will also show how to train with other common datasets."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preliminaries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!apt-get update\n",
    "!apt-get install -y libmetis-dev=5.1.0.dfsg-5\n",
    "%pip install -r requirements.txt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import logging\n",
    "from pprint import pformat\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import popdist.tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "from data_utils.batch_config import BatchConfig\n",
    "from data_utils.clustering_utils import ClusterGraph\n",
    "from data_utils.clustering_statistics import ClusteringStatistics\n",
    "from data_utils.dataset_batch_generator import tf_dataset_generator\n",
    "from data_utils.dataset_loader import load_dataset\n",
    "from keras_extensions.callbacks.callback_factory import CallbackFactory\n",
    "from keras_extensions.optimization import get_optimizer\n",
    "from model.loss_accuracy import get_loss_and_metrics\n",
    "from model.model import create_model\n",
    "from model.pipeline_stage_names import PIPELINE_ALLOCATE_PREVIOUS, PIPELINE_NAMES\n",
    "from model.precision import Precision\n",
    "from utilities.constants import GraphType\n",
    "from utilities.ipu_utils import create_ipu_strategy, set_random_seeds\n",
    "from utilities.options import Options\n",
    "from utilities.pipeline_stage_assignment import pipeline_model\n",
    "from utilities.utils import get_adjacency_dtype, get_adjacency_form, get_method_max"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configuration"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Setup logging"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "logging.getLogger().setLevel(\"INFO\")\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(levelname)-8s %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "# Prevent doubling of TF logs.\n",
    "tf.get_logger().propagate = False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define config file and use default values for those not defined in the config file.\n",
    "By default, we will use a config file for the arXiv dataset.\n",
    "\n",
    "But we encourage you to experiment with the other example files we provide too:\n",
    "\n",
    "- configs/train_arxiv.json\n",
    "- configs/train_mag.json\n",
    "- configs/train_mag240.json\n",
    "- configs/train_ppi.json\n",
    "- configs/train_products.json\n",
    "- configs/train_reddit.json\n",
    "\n",
    "Note that the PPI and Reddit datasets have to be downloaded manually from [Stanford GraphSAGE](https://snap.stanford.edu/graphsage/). The rest of datasets will download automatically. Note also some datasets, like Products and MAG, can take long to download, preprocess and clustering. The MAG240 dataset can take several hours for this preprocessing.  Moreover, note that the clustering method used in the original paper [1] -and also the one implemented here- has been designed for homogeneous graphs. However, two of the config files described above, namely `configs/train_mag.json` and `configs/train_mag240.json` that correspond with MAG and MAG240 datasets, respectively, include heterogeneous graphs. Hence, clustering these datasets as if they were homogeneous graphs can reduce the accuracy."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "config_file = \"configs/train_arxiv.json\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open(config_file, \"r\") as read_file:\n",
    "    config = json.load(read_file)\n",
    "config = Options(**config)\n",
    "\n",
    "logging.info(f\"Config: {pformat(config.dict())}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We disable logging results in Weights and Biases by overwriting the `config.wandb` parameter from the config file and set it to false."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "config.wandb = False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training configuration\n",
    "\n",
    "Set training precision and the way we represent the adjacency matrix from the config file. For the model precision we can choose FP32 or FP16; while for the representation of the adjacency matrix, we can choose either dense or the COO sparse representation.\n",
    "\n",
    "You can also override the precision configuration in the code. For example, in order to set FP16 and COO representation just do the following:\n",
    "```\n",
    "config.training.precision = \"fp16\"\n",
    "config.training.use_sparse_representation = True\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Set precision policy for training\n",
    "precision = Precision(config.training.precision)\n",
    "tf.keras.mixed_precision.set_global_policy(precision.policy)\n",
    "\n",
    "# Set how the adjacency matrix is expressed,\n",
    "# namely dense (tf.Tensor), dynamic COO representation (tf.sparse.SparseTensor),\n",
    "# or static COO representation with padding (tuple).\n",
    "adjacency_form_training = get_adjacency_form(\n",
    "    config.training.device, config.training.use_sparse_representation\n",
    ")\n",
    "\n",
    "# Decide on the dtype of the adjacency matrix\n",
    "adjacency_dtype_training = get_adjacency_dtype(\n",
    "    config.training.device, config.training.use_sparse_representation\n",
    ")\n",
    "\n",
    "method_max_edges = get_method_max(config.method_max_edges)\n",
    "method_max_nodes = get_method_max(config.method_max_nodes)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Set a unique name for this training run. This is useful when logging data, especially when logging the results on Weights and Biases."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "time_now = datetime.now().timestamp()\n",
    "universal_run_name = (\n",
    "    f\"{config.name}-\"\n",
    "    f\"{config.dataset_name}-\"\n",
    "    f\"{config.training.precision}-\"\n",
    "    f\"{config.training.device}-\"\n",
    "    f\"{adjacency_form_training}-\"\n",
    "    f\"{datetime.fromtimestamp(time_now).strftime('%Y%m%d_%H%M%S')}\"\n",
    ")\n",
    "logging.info(f\"Universal name for run: {universal_run_name}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the dataset\n",
    "\n",
    "We are now ready to load the dataset. We only have to introduce the path to the dataset. This path will be used to look for available preprocessed data and cached clustering results."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "config.data_path = os.getenv(\"DATASET_DIR\", \"/localdata/paperspace/graph_datasets/\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset = load_dataset(\n",
    "    dataset_path=config.data_path,\n",
    "    dataset_name=config.dataset_name,\n",
    "    precalculate_first_layer=config.model.first_layer_precalculation,\n",
    "    adjacency_dtype=adjacency_dtype_training,\n",
    "    features_dtype=precision.features_precision.as_numpy_dtype,\n",
    "    labels_dtype=precision.labels_precision.as_numpy_dtype,\n",
    "    regenerate_cache=config.regenerate_dataset_cache,\n",
    "    save_dataset_cache=config.save_dataset_cache,\n",
    "    pca_features_path=config.pca_features_path,\n",
    ")\n",
    "logging.info(f\"Graph dataset: {dataset}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training\n",
    "\n",
    "### Clustering the training dataset\n",
    "\n",
    "Once we have loaded the dataset, we can cluster the associated graph, using the parameters given in the config file, mainly `config.training.num_clusters` or `config.training.max_nodes_per_batch`. The former sets the total number of clusters in which we want to split the graph. The latter specifies the maximum number of nodes per cluster. Note these are alternative methods, so we cannot specify both parameters.\n",
    "\n",
    "Other important parameters are:\n",
    "- `config.training.clusters_per_batch`: Number of clusters to be processed together in a micro-batch.\n",
    "\n",
    "- `method_max_nodes` and `method_max_edges`: In order to maximise bandwidth, we compile the model as a static graph. This requires having static datastructures. Since the number of nodes can change from one cluster to another, we need a method to set the maximum number of nodes per cluster. If a cluster has a number of nodes smaller than what we have defined, then it will pad with a dummy node so that it fits the allocated memory, wasting computations and reducing throughput. On the other hand, if they are larger, it will remove some of them so that they fit in the allocated space, removing data and potentially reducing accuracy. Though there is a tradeoff here, and we offer three methods to optimise this tradeoff\n",
    "namely `average`, `average_plus_std` and `upper_bound`, which work as follows:\n",
    "    - `average` means all clusters will pad ro prune nodes so that they adjust to\n",
    "the average number of nodes across all clusters;\n",
    "    - `average_plus_std` means all clusters will adjust to the average number of nodes across all clusters\n",
    "plus one standard deviation;\n",
    "    - `upper_bound` means all clusters will pad to the number of nodes in the largest cluster.\n",
    "In practice, the gains obtained by using a statically compiled graph are larger than the extra computations due to padding, even when using the `upper_bound` case.\n",
    "\n",
    "- `method_max_edges`: Same as `method_max_nodes` but for the number of edges per cluster.\n",
    "\n",
    "- `config.inter_cluster_ratio`: When using COO representation and sampling multiple cluster per batch, this parameter represents the extra number of edges that connect nodes that are in different that we will use. When this is zero, we can think the adjacency matrix of the batch as a pure block-diagonal matrix. If this value is greater than zero, then the adjacency matrix will have some positive values off the diagonal blocks.\n",
    "\n",
    "- `node_edge_imbalance_ratio`: By default the METIS clustering algorithm will find clusters that have similar number of nodes, though they can have very different number of edges. Setting this parameter we can tell METIS to find a trade-off between balancing number of nodes and edges. In theory this could reduce the amount of padding when using `upper_bound` method to set number of nodes and edges, but in practice we have observed but it doesn't have much effect on throughput for the considered datasets.\n",
    "\n",
    "We encourage you playing with different number of clusters and clusters per batch. The idea is to keep the number of nodes per batch large enough to minimise the number of lost edges, which affects accuracy, while fitting in memory, so we can benefit from the high bandwidth offered by a compiled computational graph."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "training_clusters = ClusterGraph(\n",
    "    adjacency=dataset.adjacency_train,\n",
    "    num_clusters=config.training.num_clusters,\n",
    "    visible_nodes=dataset.dataset_splits[\"train\"],\n",
    "    max_nodes_per_batch=config.training.max_nodes_per_batch,\n",
    "    clusters_per_batch=config.training.clusters_per_batch,\n",
    "    dataset_name=config.dataset_name + \"-training\",\n",
    "    cache_dir=config.data_path,\n",
    "    regenerate_cluster_cache=config.regenerate_clustering_cache,\n",
    "    save_clustering_cache=config.save_clustering_cache,\n",
    "    directed_graph=(dataset.graph_type == GraphType.DIRECTED),\n",
    "    adjacency_form=adjacency_form_training,\n",
    "    inter_cluster_ratio=config.inter_cluster_ratio,\n",
    "    method_max_nodes=method_max_nodes,\n",
    "    method_max_edges=method_max_edges,\n",
    "    node_edge_imbalance_ratio=config.cluster_node_edge_imbalance_ratio,\n",
    ")\n",
    "training_clusters.cluster_graph()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can compute some statistics on the impact of clustering on the graph"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "clustering_statistics = ClusteringStatistics(\n",
    "    training_clusters.adjacency,\n",
    "    training_clusters.clusters,\n",
    "    num_clusters_per_batch=config.training.clusters_per_batch,\n",
    ")\n",
    "clustering_statistics.get_statistics(wandb=config.wandb)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Count the number of nodes that will be processed per epoch."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "num_real_nodes_per_epoch = len(dataset.dataset_splits[\"train\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Build training dataset generator\n",
    "\n",
    "Create a efficient dataset generator for training using the TensorFlow `tf.data.Dataset` API. This allows preprocessing the data with multiple threads, increasing the speed at which the host can feed data to the Graphcore IPU. This is important, as the Graphcore IPU is so fast for GNN, that the host is usually the bottleneck!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_generator_training = tf_dataset_generator(\n",
    "    adjacency=dataset.adjacency_train,\n",
    "    clusters=training_clusters.clusters,\n",
    "    features=dataset.features_train,\n",
    "    labels=dataset.labels,\n",
    "    mask=dataset.mask_train,\n",
    "    num_clusters=training_clusters.num_clusters,\n",
    "    clusters_per_batch=training_clusters.clusters_per_batch,\n",
    "    max_nodes_per_batch=training_clusters.max_nodes_per_batch,\n",
    "    max_edges_per_batch=training_clusters.max_edges_per_batch,\n",
    "    adjacency_dtype=adjacency_dtype_training,\n",
    "    adjacency_form=adjacency_form_training,\n",
    "    micro_batch_size=config.training.micro_batch_size,\n",
    "    seed=config.seed,\n",
    "    prefetch_depth=config.training.dataset_prefetch_depth,\n",
    "    distributed_worker_count=popdist.getNumInstances(),\n",
    "    distributed_worker_index=popdist.getInstanceIndex(),\n",
    ")\n",
    "logging.info(f\"Created batch generator for training: {data_generator_training}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As mentioned above, the Graphcore Bow-IPU is so fast that the dataset generator is usually the bottleneck, especially when we use distributed training with data parallelism, in which multiple replicas of the model run in different Graphcore IPU devices of a Graphcore Bow-POD.\n",
    "\n",
    "In order to be able to use the high throughput provided by a Graphcore Bow-POD-4, we offer the `poprun` tool, that allows to launch multiple instances in the host, each one feeding data to one of the replicas. This is really easy to do from the command line, as explained in the README.md of this repo.\n",
    "\n",
    "The next cell gives some feedback about this."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if config.training.replicas > 1:\n",
    "    logging.warning(\n",
    "        f\"Increasing the number of model replicas will scale up the throughput if \"\n",
    "        f\"the data generator in the host side is fast enough to feed data to all \"\n",
    "        f\"the replicas. This is easily achieved with `poprun`. See README.md from \"\n",
    "        f\"this repo for more information on how to scale the host throughput by \"\n",
    "        f\"running multiple instances in the host, feeding one replica each.\"\n",
    "    )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a batch config object that calculates the number of steps, micro-batches, etc. This is useful for distributed training, so we don't have to keep the numbers in mind when allocating data to the different replicas and/or pipeline stages."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "num_real_nodes_per_epoch = len(dataset.dataset_splits[\"train\"])\n",
    "batch_config_training = BatchConfig(\n",
    "    micro_batch_size=config.training.micro_batch_size,\n",
    "    num_clusters=training_clusters.num_clusters,\n",
    "    clusters_per_batch=training_clusters.clusters_per_batch,\n",
    "    max_nodes_per_batch=training_clusters.max_nodes_per_batch,\n",
    "    executions_per_epoch=config.training.executions_per_epoch,\n",
    "    gradient_accumulation_steps_per_replica=config.training.gradient_accumulation_steps_per_replica,\n",
    "    num_replicas=config.training.replicas,\n",
    "    epochs_per_execution=config.training.epochs_per_execution,\n",
    "    num_real_nodes_per_epoch=num_real_nodes_per_epoch,\n",
    "    num_epochs=config.training.epochs,\n",
    ")\n",
    "logging.info(f\"Training batch config:\\n{batch_config_training}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training strategy\n",
    "\n",
    "We are ready to create the model. In order to compile the model for different hardware, we leverage TensorFlow's strategy scope, so that we can define the same Keras model to run on a POD of IPUs or on a CPU, and everything is handled under the hood, transparently for the user."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Calculate the number of pipeline stages and the number of required IPUs per replica.\n",
    "num_pipeline_stages_training = len(config.training.ipu_config.pipeline_device_mapping)\n",
    "num_ipus_per_replica_training = (\n",
    "    max(config.training.ipu_config.pipeline_device_mapping) + 1\n",
    ")\n",
    "\n",
    "# Create a strategy scope for training\n",
    "strategy_training_scope = (\n",
    "    create_ipu_strategy(\n",
    "        num_ipus_per_replica=num_pipeline_stages_training,\n",
    "        num_replicas=config.training.replicas,\n",
    "        matmul_available_memory_proportion=config.training.ipu_config.matmul_available_memory_proportion_per_pipeline_stage[\n",
    "            0\n",
    "        ],\n",
    "        matmul_partials_type=precision.matmul_partials_type,\n",
    "        compile_only=config.compile_only,\n",
    "        enable_recomputation=config.training.ipu_config.enable_recomputation,\n",
    "        fp_exceptions=config.fp_exceptions,\n",
    "        num_io_tiles=config.training.ipu_config.num_io_tiles,\n",
    "    ).scope()\n",
    "    if config.training.device == \"ipu\"\n",
    "    else tf.device(\"/cpu:0\")\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating and compile model, and training loop\n",
    "\n",
    "Create, compile and train the model within the desired strategy scope."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Seed the random generators for reproducibility\n",
    "set_random_seeds(config.seed)\n",
    "\n",
    "with strategy_training_scope:\n",
    "    # Create the model for training\n",
    "    model_training = create_model(\n",
    "        micro_batch_size=config.training.micro_batch_size,\n",
    "        num_labels=dataset.num_labels,\n",
    "        num_features=dataset.num_features,\n",
    "        max_nodes_per_batch=training_clusters.max_nodes_per_batch,\n",
    "        max_edges_per_batch=training_clusters.max_edges_per_batch,\n",
    "        hidden_size=config.model.hidden_size,\n",
    "        num_layers=config.model.num_layers,\n",
    "        dropout_rate=config.model.dropout,\n",
    "        adjacency_params=config.model.adjacency.dict(),\n",
    "        cast_model_inputs_to_dtype=precision.cast_model_inputs_to_dtype,\n",
    "        first_layer_precalculation=config.model.first_layer_precalculation,\n",
    "        use_ipu_layers=(config.training.device == \"ipu\"),\n",
    "        adjacency_form=adjacency_form_training,\n",
    "    )\n",
    "    model_training.summary(print_fn=logging.info)\n",
    "\n",
    "    # Set options for the infeed and outfeed buffers that connect IPU and host.\n",
    "    model_training.set_infeed_queue_options(prefetch_depth=10)\n",
    "    model_training.set_outfeed_queue_options(buffer_depth=10)\n",
    "\n",
    "    if num_pipeline_stages_training > 1 and config.training.device == \"ipu\":\n",
    "        # Pipeline the model if required\n",
    "        pipeline_model(\n",
    "            model=model_training,\n",
    "            config=config.training,\n",
    "            pipeline_names=PIPELINE_NAMES,\n",
    "            pipeline_allocate_previous=PIPELINE_ALLOCATE_PREVIOUS,\n",
    "            num_ipus_per_replica=num_ipus_per_replica_training,\n",
    "            matmul_partials_type=precision.matmul_partials_type,\n",
    "        )\n",
    "    elif config.training.gradient_accumulation_steps_per_replica > 1:\n",
    "        # Set gradient accumulation if requested. If the model is pipelined\n",
    "        # this is done through the pipeline API above.\n",
    "        model_training.set_gradient_accumulation_options(\n",
    "            gradient_accumulation_steps_per_replica=config.training.gradient_accumulation_steps_per_replica\n",
    "        )\n",
    "\n",
    "    # Build the loss function and other metrics\n",
    "    loss, accuracy, f1_score_macro, f1_score_micro = get_loss_and_metrics(\n",
    "        task=dataset.task,\n",
    "        num_labels=dataset.num_labels,\n",
    "        adjacency_form=adjacency_form_training,\n",
    "        metrics_precision=precision.metrics_precision,\n",
    "        enable_loss_outfeed=(config.training.device == \"ipu\"),\n",
    "    )\n",
    "\n",
    "    # Build the optimizer\n",
    "    optimizer = get_optimizer(\n",
    "        gradient_accumulation_steps_per_replica=config.training.gradient_accumulation_steps_per_replica,\n",
    "        num_replicas=config.training.replicas,\n",
    "        learning_rate=tf.cast(config.training.lr, dtype=tf.float32),\n",
    "        loss_scaling=config.training.loss_scaling,\n",
    "        optimizer_compute_precision=precision.optimizer_compute_precision,\n",
    "    )\n",
    "\n",
    "    # Compile the model\n",
    "    model_training.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=[accuracy, f1_score_macro, f1_score_micro],\n",
    "        steps_per_execution=batch_config_training.steps_per_execution,\n",
    "    )\n",
    "\n",
    "    # Create any callbacks required for training\n",
    "    callbacks_training = CallbackFactory.get_callbacks(\n",
    "        universal_run_name=universal_run_name,\n",
    "        num_nodes_processed_per_execution=batch_config_training.num_nodes_processed_per_execution,\n",
    "        real_over_padded_ratio=batch_config_training.real_over_padded_ratio,\n",
    "        total_num_epochs=batch_config_training.scaled_num_epochs,\n",
    "        loss=loss,\n",
    "        checkpoint_path=config.save_ckpt_path.joinpath(universal_run_name),\n",
    "        config=config.dict(),\n",
    "        executions_per_log=config.executions_per_log,\n",
    "        executions_per_ckpt=config.executions_per_ckpt,\n",
    "        outfeed_queues=[loss.outfeed_queue],\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    model_training.fit(\n",
    "        data_generator_training,\n",
    "        epochs=batch_config_training.scaled_num_epochs,\n",
    "        steps_per_epoch=batch_config_training.steps_per_epoch,\n",
    "        callbacks=callbacks_training,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "trained_weights = model_training.get_weights()\n",
    "logging.info(\"Training complete\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test the trained model\n",
    "\n",
    "Once we have trained the model, let's evaluate how it performs on the test dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test configuration\n",
    "\n",
    "Note we usually have different requirements for testing than training. For example, while we may want to train the model with FP16 arithmetic for reduced memory and increased throughput, we typically want to test the model with FP32 to avoid losing accuracy.\n",
    "\n",
    "Here, again, we use the values from the config file."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Set the precision policy for testing\n",
    "precision = Precision(config.test.precision)\n",
    "tf.keras.mixed_precision.set_global_policy(precision.policy)\n",
    "\n",
    "# Set how the adjacency matrix is expressed,\n",
    "# namely dense tensor, sparse tensor, or tuple.\n",
    "adjacency_form_test = get_adjacency_form(\n",
    "    config.test.device, config.test.use_sparse_representation\n",
    ")\n",
    "# Decide on the dtype of the adjacency matrix\n",
    "adjacency_dtype_test = get_adjacency_dtype(\n",
    "    config.test.device, config.test.use_sparse_representation\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cluster test dataset\n",
    "\n",
    "Since the test dataset is different from the training dataset, it has also to be clustered independently.\n",
    "\n",
    "In practice, we want to evaluate the performance of the trained model on the full test graph, so no artefacts are introduced while measuring the accuracy. Note that evaluation only requires a forward pass per batch, which is computationally much less demanding that the multiple forward and backward passes per batch required during training. Hence, we usually can afford testing the model on the CPU, which though slower, usually has lots of memory available.\n",
    "\n",
    "In the case the graph dataset is too large so that it doesn't fit even on CPU memory, we just have to cluster the test dataset.\n",
    "\n",
    "In any case, we use the same `ClusterGraph` class that we used for the training set, since it is convenient to compute and store the maximum number of nodes and edges per batch."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Cluster the test graph\n",
    "test_clusters = ClusterGraph(\n",
    "    adjacency=dataset.adjacency_full,\n",
    "    num_clusters=config.test.num_clusters,\n",
    "    visible_nodes=np.arange(dataset.total_num_nodes),\n",
    "    max_nodes_per_batch=config.test.max_nodes_per_batch,\n",
    "    clusters_per_batch=config.test.clusters_per_batch,\n",
    "    dataset_name=config.dataset_name + \"-test\",\n",
    "    cache_dir=config.data_path,\n",
    "    regenerate_cluster_cache=config.regenerate_clustering_cache,\n",
    "    save_clustering_cache=config.save_clustering_cache,\n",
    "    directed_graph=(dataset.graph_type == GraphType.DIRECTED),\n",
    "    adjacency_form=adjacency_form_test,\n",
    "    inter_cluster_ratio=config.inter_cluster_ratio,\n",
    "    method_max_nodes=method_max_nodes,\n",
    "    method_max_edges=method_max_edges,\n",
    "    node_edge_imbalance_ratio=config.cluster_node_edge_imbalance_ratio,\n",
    ")\n",
    "test_clusters.cluster_graph()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test dataset generator\n",
    "\n",
    "Create an efficient dataset generator that can feed the Keras Model.evaluate method."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_generator_test = tf_dataset_generator(\n",
    "    adjacency=dataset.adjacency_full,\n",
    "    clusters=test_clusters.clusters,\n",
    "    features=dataset.features,\n",
    "    labels=dataset.labels,\n",
    "    mask=dataset.mask_test,\n",
    "    num_clusters=config.test.num_clusters,\n",
    "    clusters_per_batch=config.test.clusters_per_batch,\n",
    "    max_nodes_per_batch=test_clusters.max_nodes_per_batch,\n",
    "    max_edges_per_batch=test_clusters.max_edges_per_batch,\n",
    "    adjacency_dtype=adjacency_dtype_test,\n",
    "    adjacency_form=adjacency_form_test,\n",
    "    micro_batch_size=config.test.micro_batch_size,\n",
    "    seed=config.seed,\n",
    ")\n",
    "logging.info(f\"Created batch generator for test: {data_generator_test}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since there is only one cluster or subgraph (the whole graph) and there is no distributed training, we do not need to create a batch config object when testing on the CPU.\n",
    "\n",
    "We do not have to create a strategy when testing on the CPU either, since this is the default device for TensorFlow."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model\n",
    "\n",
    "Create the model for test that will run on the CPU."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "set_random_seeds(config.seed + 1)\n",
    "\n",
    "model_test = create_model(\n",
    "    micro_batch_size=config.test.micro_batch_size,\n",
    "    num_labels=dataset.num_labels,\n",
    "    num_features=dataset.num_features,\n",
    "    max_nodes_per_batch=test_clusters.max_nodes_per_batch,\n",
    "    max_edges_per_batch=test_clusters.max_edges_per_batch,\n",
    "    hidden_size=config.model.hidden_size,\n",
    "    num_layers=config.model.num_layers,\n",
    "    dropout_rate=config.model.dropout,\n",
    "    adjacency_params=config.model.adjacency.dict(),\n",
    "    cast_model_inputs_to_dtype=precision.cast_model_inputs_to_dtype,\n",
    "    first_layer_precalculation=config.model.first_layer_precalculation,\n",
    "    use_ipu_layers=(config.test.device == \"ipu\"),\n",
    "    adjacency_form=adjacency_form_test,\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Copy the weights from training."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_test.set_weights(trained_weights)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get the loss function and other metrics."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "_, accuracy, f1_score_macro, f1_score_micro = get_loss_and_metrics(\n",
    "    task=dataset.task,\n",
    "    num_labels=dataset.num_labels,\n",
    "    adjacency_form=adjacency_form_test,\n",
    "    metrics_precision=precision.metrics_precision,\n",
    "    enable_loss_outfeed=False,\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Compile the model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_test.compile(\n",
    "    metrics=[accuracy, f1_score_macro, f1_score_micro], steps_per_execution=1\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run test on the test dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "results = model_test.evaluate(data_generator_test, steps=1)\n",
    "\n",
    "logging.info(\n",
    "    f\"Test Accuracy: {results[1]},\"\n",
    "    f\" Test F1 macro: {results[2]},\"\n",
    "    f\" Test F1 micro: {results[3]}\"\n",
    ")\n",
    "logging.info(\"Test complete\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "traceability": {
   "sdk_version": "2.6.0+1069",
   "source_file": "run_cluster_gcn_notebook.py",
   "sst_version": "0.0.7",
   "timestamp": "2022-07-21T19:14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
