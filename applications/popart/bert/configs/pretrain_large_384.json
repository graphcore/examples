{
    "task": "PRETRAINING",
    "num_layers": 24,
    "layers_per_ipu": [2],
    "hidden_size": 1024,
    "attention_heads": 16,
    "sequence_length": 384,
    "mask_tokens": 56,
    "popart_dtype": "FLOAT16",
    "no_dropout": false,
    "no_attn_dropout": true,
    "no_cls_layer": true,
    "learning_rate": 0.0001,
    "lr_schedule_by_step": {
        "0": 0.0001,
        "512": 0.000095,
        "1024": 0.00009025,
        "1536": 0.0000857375,
        "2048": 0.0000814506,
        "2560": 0.0000773781,
        "3072": 0.0000735092,
        "3584": 0.0000698337,
        "4096": 0.000066342,
        "4608": 0.0000630249,
        "5120": 0.0000598737,
        "5632": 0.00005688,
        "6144": 0.000054036,
        "6656": 0.0000513342,
        "7168": 0.0000487675,
        "7680": 0.0000463291,
        "8192": 0.0000440127,
        "8704": 0.000041812,
        "9216": 0.0000397214,
        "9728": 0.0000377354,
        "10240": 0.0000358486,
        "10752": 0.0000340562,
        "11264": 0.0000323534,
        "11776": 0.0000307357,
        "12288": 0.0000291989,
        "12800": 0.000027739,
        "13312": 0.000026352,
        "13824": 0.0000250344,
        "14336": 0.0000237827,
        "14848": 0.0000225936,
        "15360": 0.0000214639,
        "15872": 0.0000203907,
        "16384": 0.0000193711,
        "16896": 0.0000184026,
        "17408": 0.0000174825,
        "17920": 0.0000166083,
        "18432": 0.0000157779,
        "18944": 0.000014989,
        "19456": 0.0000142396,
        "19968": 0.0000135276,
        "20480": 0.0000128512,
        "20992": 0.0000122087
    },
    "loss_scaling": 20.0,
    "velocity_scaling": 20.0,
    "momentum": 0.984375,
    "pipeline_lr_scaling": false,
    "pipeline_momentum_scaling": false,
    "stochastic_rounding": true,
    "batches_per_step": 128,
    "epochs": 2,
    "epochs_per_save": 1,
    "steps_per_save": 512,
    "steps_per_log": 100,
    "aggregate_metrics_over_steps": 1,
    "gradient_accumulation_factor": 64,
    "input_files": [
        "data/wikipedia/AA/sequence_384/wiki_00_tokenised",
        "data/wikipedia/AA/sequence_384/wiki_01_tokenised",
        "data/wikipedia/AA/sequence_384/wiki_02_tokenised",
        "data/wikipedia/AA/sequence_384/wiki_03_tokenised",
        "data/wikipedia/AA/sequence_384/wiki_04_tokenised",
        "data/wikipedia/AA/sequence_384/wiki_05_tokenised",
        "data/wikipedia/AA/sequence_384/wiki_06_tokenised",
        "data/wikipedia/AA/sequence_384/wiki_07_tokenised",
        "data/wikipedia/AA/sequence_384/wiki_08_tokenised",
        "data/wikipedia/AA/sequence_384/wiki_09_tokenised",
        "data/wikipedia/AA/sequence_384/wiki_10_tokenised",
        "data/wikipedia/AA/sequence_384/wiki_11_tokenised",
        "data/wikipedia/AA/sequence_384/wiki_12_tokenised",
        "data/wikipedia/AA/sequence_384/wiki_13_tokenised"
    ],
    "duplication_factor": 6,
    "epochs_to_cache": 1,
    "vocab_length": 30400,
    "embedding_serialization_vocab_steps": 8,
    "split_linear_layers": true,
    "recompute_checkpoint_every_layer": false,
    "engine_cache": "__pretrain_large_384",
    "shuffle": true,
    "pipeline": true,
    "checkpoint_dir": "checkpoints/pretrain_large_384",
    "no_validation": true
}
