{
    "task": "PRETRAINING",
    "use_packed_sequence_format": true,
    "max_sequences_per_pack": 3,
    "num_layers": 2,
    "encoder_start_ipu": 0,
    "layers_per_ipu": [1, 1],
    "hidden_size": 128,
    "attention_heads": 2,
    "sequence_length": 512,
    "mask_tokens": 76,
    "vocab_length": 30400,
    "popart_dtype": "FLOAT16",
    "no_dropout": false,
    "no_attn_dropout": false,
    "dropout_prob": 0.1,
    "stochastic_rounding": true,
    "enable_half_partials": true,
    "batches_per_step":  1,
    "epochs": 1,
    "epochs_per_save": 1,
    "floating_point_exceptions": true,
    "projection_bias" : true,
    "steps_per_save": 200,
    "steps_per_log": 1,
    "aggregate_metrics_over_steps": 1,
    "loss_scaling": 2,
    "micro_batch_size": 4,
    "gradient_accumulation_factor": 512,
    "replication_factor": 1,
    "split_qkv": true,
    "shuffle": true,
    "duplication_factor": 1,
    "epochs_to_cache": 1,
    "embedding_serialization_vocab_steps": 4,
    "available_memory_proportion": [0.20],
    "checkpoint_dir": "checkpoints/mk2/packed_pretrain_tiny",
    "no_validation": true,
    "optimizer_state_offchip": false, 
    "gradient_reduction_type": "Mean",
    "optimizer": "LAMB_NO_BIAS",
    "beta1": 0.88,
    "beta2": 0.99,
    "learning_rate_function": "Linear",
    "lr_warmup_steps" : 4,
    "lr_warmup_start" : 4e-3,
    "learning_rate": 8e-3,
    "weight_decay": 1e-2,
    "input_files": ["data/packed_pretraining_data/*"],
    "engine_cache": "__cache_mlperf_bert_tiny"
}