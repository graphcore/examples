{
    "task": "GLUE",
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "intermediate_size": 3072,
    "max_position_embeddings": 512,
    "vocab_size": 30400,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "seq_length": 384,
    "max_predictions_per_seq": 56,
    "use_attention_projection_bias": false,
    "use_cls_layer": false,
    "pipeline_stages": [
     ["emb", "pos"],
     ["hid","hid", "hid", "hid"],
     ["hid","hid", "hid", "hid"],
     ["hid","hid", "hid", "hid"],
     ["glu"]],
    "device_mapping":[0,1,2,3,0],
    "batch_size": 1,
    "batches_per_step": 1,
    "epochs": 3,
    "epsilon": 0.000001,
    "lr_schedule": "polynomial_decay",
    "base_learning_rate": 2e-5,
    "warmup": 0.1,
    "loss_scaling": 1,
    "optimizer": "adamw",
    "beta1": 0.9,
    "beta2": 0.999,
    "gradient_accumulation_count": 30,
    "pipeline_schedule": "Grouped",
    "replicas": 1,
    "partials_type": "half",
    "reduction_type": "mean",
    "buffer_size": 104800,
    "precision": "16",
    "seed": 7801,
    "steps_per_ckpts": 99999,
    "steps_per_logs": 1,
    "weight_decay_rate": 0.01,
    "available_memory_proportion": 0.15,
    "parallel_io_threads": 16,
    "matmul_serialize_factor": 5,
    "duplicate_factor": 5,
    "no_outlining": false,
    "stochastic_rounding": true,
    "enable_recomputation": true,
    "fp_exceptions": false,
    "steps_per_tensorboard": -1,
    "init_checkpoint": "",
    "static_mask": false,
    "do_training": false,
    "do_eval": false,
    "do_predict": false,
    "do_lower_case": true,
    "verbose_logging": false,
    "version_2_with_negative": false,
    "variable_offloading": true,
    "output_dir": "tmp/glue/",
    "save_path": "checkpoint/glue",
    "vocab_file": "data/ckpts/uncased_L-24_H-1024_A-16/vocab.txt",
    "tfrecord_dir": "tmp/tf_record_cache/"
}
